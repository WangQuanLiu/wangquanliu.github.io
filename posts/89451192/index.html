<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wangquanliu.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="吴恩达机器学习与深度学习总结。">
<meta property="og:type" content="article">
<meta property="og:title" content="吴恩达深度学习">
<meta property="og:url" content="http://wangquanliu.com/posts/89451192/index.html">
<meta property="og:site_name" content="王权个人博客">
<meta property="og:description" content="吴恩达机器学习与深度学习总结。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/梯度下降算法代价函数的定义.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/梯度下降算法概要.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/梯度下降算法详解.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/学习率1.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/学习率2.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/神经网络的梯度下降.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/计算图1.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/计算图2.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/使用计算图求导1.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/使用计算图求导2.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/神经模型1.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/神经模型2.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/神经模型3.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/神经模型4.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/代价函数1.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/代价函数2.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/反向传播1.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/反向传播2.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/反向传播3.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/反向传播4.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/反向传播5.png">
<meta property="og:image" content="http://wangquanliu.com/posts/89451192/image-20220619151637481-16556230021951.png">
<meta property="article:published_time" content="2022-06-19T07:15:44.000Z">
<meta property="article:modified_time" content="2022-06-19T07:17:23.067Z">
<meta property="article:author" content="刘王权">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="吴恩达">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wangquanliu.com/posts/89451192/梯度下降算法代价函数的定义.png">

<link rel="canonical" href="http://wangquanliu.com/posts/89451192/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>吴恩达深度学习 | 王权个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">王权个人博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">14</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">5</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">11</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://wangquanliu.com/posts/89451192/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="刘王权">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="王权个人博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          吴恩达深度学习
        </h1>

        <div class="post-meta">

            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2022-06-19 15:15:44 / 修改时间：15:17:23" itemprop="dateCreated datePublished" datetime="2022-06-19T15:15:44+08:00">2022-06-19</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">计算机</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>4k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>4 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>吴恩达机器学习与深度学习总结。</p>
<span id="more"></span>
<h1 id="梯度下降原理"><a href="#梯度下降原理" class="headerlink" title="梯度下降原理"></a>梯度下降原理</h1><h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><h3 id="代价函数定义"><a href="#代价函数定义" class="headerlink" title="代价函数定义"></a>代价函数定义</h3><p>梯度下降算法—求最小代价函数的一种算法，通过不断变幻参数使得找到最小代价函数，代价函数的定义</p>
<p><img src="/posts/89451192/梯度下降算法代价函数的定义.png" alt="梯度下降算法代价函数的定义"></p>
<h3 id="算法概要"><a href="#算法概要" class="headerlink" title="算法概要"></a>算法概要</h3><p><img src="/posts/89451192/梯度下降算法概要.png" alt="梯度下降算法概要"></p>
<p><strong>注：a表示学习率下降得多快</strong></p>
<h3 id="算法详解"><a href="#算法详解" class="headerlink" title="算法详解"></a>算法详解</h3><p><img src="/posts/89451192/梯度下降算法详解.png" alt="梯度下降算法详解"></p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><p>当 <script type="math/tex">\theta_1</script>在右边时，由于导数为x轴的<script type="math/tex">\Delta x</script> 与y轴的 <script type="math/tex">\Delta y</script>的比值，又由于<script type="math/tex">\theta_1</script> 在右边，故 <script type="math/tex">\Delta x</script>与  <script type="math/tex">\Delta y</script>皆为正数所以其导数为正，通过公式<script type="math/tex">\theta_1=\theta_1-a\frac{d}{d\theta_1}J(\theta_1)</script>，参数<script type="math/tex">\theta_1</script>会向右趋近。<br>当  <script type="math/tex">\theta_1</script>在左边时，由于<script type="math/tex">\Delta x</script>  为正数与 <script type="math/tex">\Delta y</script>为负数，故其导数为负数，通过上面公式<script type="math/tex">\theta_1=\theta_1-a\frac{d}{d\theta_1}J(\theta_1)</script> ，参数<script type="math/tex">\theta_1</script>会向左进行趋近，无论<script type="math/tex">\theta_1</script> 在其极小值的左边或右边皆向极小值逼进，通过一系列的趋近，最终目的是使得参数<script type="math/tex">\theta_1</script>趋近于最小值。<br>此算法通过不断迭代直到极小值，当 <script type="math/tex">\theta_1</script> 为极小值点时其导数斜率趋向于某个值 ，此时斜率发生的变化极小  </p>
<p><strong>注：我们需要合理调整学习率</strong></p>
<p>当学习率a过大时，会导致<script type="math/tex">\frac{\Delta x}{\Delta y}</script>过大，就有</p>
<p><img src="/posts/89451192/学习率1.png" alt="学习率1" style="zoom:60%;"></p>
<p>当学习率过大时，会导致</p>
<p><img src="/posts/89451192/学习率2.png" alt="学习率2" style="zoom:60%;"></p>
<p>这样会导致从右边直接跨过极小点，直接到左边的位置，这是学习率过大导致的情况。</p>
<p>而若学习率过小会导致需要多次迭代。</p>
<h1 id="神经网络原理"><a href="#神经网络原理" class="headerlink" title="神经网络原理"></a>神经网络原理</h1><h2 id="神经网络梯度下降算法"><a href="#神经网络梯度下降算法" class="headerlink" title="神经网络梯度下降算法"></a>神经网络梯度下降算法</h2><p><img src="/posts/89451192/神经网络的梯度下降.png" alt="神经网络的梯度下降"></p>
<p>如图所示，我们有参数w,b，代价函数<script type="math/tex">J(w^{(1)},b^{1},w^{2},b^{2})=\frac{1}{m}\sum_{i=1}^{m}\iota(y^*,y)</script>，我们需要通过<script type="math/tex">dw^{(l)}</script>和<script type="math/tex">db^{(l)}</script>来更新梯度下降，使得参数<script type="math/tex">w^{(l)}</script>和<script type="math/tex">b^{(l)}</script>来更新。</p>
<h2 id="神经网络反向传播详解"><a href="#神经网络反向传播详解" class="headerlink" title="神经网络反向传播详解"></a>神经网络反向传播详解</h2><p>神经网络的计算是按照正向传播与反向传播过程来实现的，首先通过正向传播计算出神经网络的输出，紧接着进行一个反向传播，反向传播可以计算出对应的梯度或导数，即正向传播服务于反向传播，反向传播服务于导数的计算。依赖关系导数-&gt;反向传播-&gt;正向传播。</p>
<p>我们来通过计算图进行分析。</p>
<h3 id="计算图求导数"><a href="#计算图求导数" class="headerlink" title="计算图求导数"></a>计算图求导数</h3><p><img src="/posts/89451192/计算图1.png" alt="计算图1"></p>
<p>如图我们想要计算函数J(a,b,c)的值，其中</p>
<script type="math/tex; mode=display">
J(a,b,c)=3(a+bc)=3(5+3*2)=33</script><p>我们计算函数J，可以进行如下步骤的拆分 </p>
<script type="math/tex; mode=display">
U=bc \\
V=a+u \\
J=3V</script><p>我们可以得到如下计算图</p>
<p><img src="/posts/89451192/计算图2.png" alt="计算图2"></p>
<p>当结果有不同或者一些特殊的输出变量时，比如J也是我们想要优化，在logistic回归中，J也是想要最小化的成本函数，可以看出，一个从左到右的过程，可以计算出函数J的值。接下来我们将会看到为了计算导数数，从右到左的过程。</p>
<p><img src="/posts/89451192/使用计算图求导1.png" alt="使用计算图求导1"></p>
<p>假设我们要计算<script type="math/tex">\frac{dJ}{dV}</script>，我们应该怎么计算呢？</p>
<p>我们若改变V的值，那么J将如何改变呢？</p>
<p>我们有<script type="math/tex">J=3V</script>，我们将<script type="math/tex">V=11</script>，变化为V=11.001，而J=33变化为33.003，<br>即我们可以得到我们在x轴变动1，而<script type="math/tex">\frac{dJ}{dV}=3</script> ，我们y轴将变化的动静为x轴的3倍，又当我们令a=5变化成a=5.001， v=11.001， J=33.003，即a的波动影响v， 而影响J，又我们有链式法则<script type="math/tex">\frac{dJ}{da}=\frac{dJ}{dv}\frac{dv}{da}=3</script> ，即若我们要计算<script type="math/tex">\frac{dJ}{da}</script>， 需要先计算出<script type="math/tex">\frac{dJ}{dv}</script> 和<script type="math/tex">\frac{dv}{da}</script>  。</p>
<p><img src="/posts/89451192/使用计算图求导2.png" alt="使用计算图求导2"></p>
<p>可以通过上面的方法求出各导数，反向传播原理与上面过程相同。</p>
<h3 id="神经模型"><a href="#神经模型" class="headerlink" title="神经模型"></a>神经模型</h3><p><img src="/posts/89451192/神经模型1.png" alt="神经模型1"></p>
<p>神经模型中的单个逻辑单元如上图所示，通过输入x0,x1,…,xn得到一个逻辑激活函数，这称之为逻辑激活函数 。<br>逻辑激活函数即  </p>
<script type="math/tex; mode=display">
g(z)=\frac{1}{1+e^{-z}}</script><p><img src="/posts/89451192/神经模型2.png" alt="神经模型2"></p>
<p>在上图中是由一组神经元组合而成的神经网络，其中最左边的为输入层，中间为隐藏层，最右边的为输出层。</p>
<p><img src="/posts/89451192/神经模型3.png" alt="神经模型3"></p>
<p>在上图中 示神经网络共有三层，第一层 示输入，中间层 示隐藏层，第三层示输出层。其中<script type="math/tex">a_i^{(j)}</script> 示第j层的第i个激活单元，<script type="math/tex">\theta^{j}</script>表示权重控制的矩阵函数从第j 层映射到第 j+1层。如果神经网络在第j层有<script type="math/tex">s_j</script>单元在第j+1层有<script type="math/tex">s_j+1</script>单元然后<script type="math/tex">\theta^{(j)}</script>有维度  </p>
<script type="math/tex; mode=display">
s_{j+1}\times(s_j+1)</script><p>向前传播是指从第一层传播到第三层，以这样的方式进行传播，即从输入层传播到输出层  </p>
<p><img src="/posts/89451192/神经模型4.png" alt="神经模型4"></p>
<p>我们对向前传播进行向量化，我们变化为</p>
<script type="math/tex; mode=display">
z_1^{(2)}=\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3 \\
z_2^{(2)}=\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3 \\
z_3^{(2)}=\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3</script><p>其中</p>
<script type="math/tex; mode=display">
z^{(2)}=\theta^{(1)}x=\theta^{(1)}a^{(1)}\\
a^{(2)}=g(z^{(2)})\\
z^{(3)}=\theta^{(2)}x=\theta^{(2)}a^{(2)}\\
h_{\theta}(x)=a^{(3)}=g(z^{(3)})</script><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p><img src="/posts/89451192/代价函数1.png" alt="代价函数1"></p>
<p>在神经网络中，我们若要实行多元分类，我们需要通过输入样本数据<script type="math/tex">x^{(i)}</script>，然后输出数据<script type="math/tex">y^{(i)}</script>分类的矩阵，我们需要使得函数<script type="math/tex">h_\theta(x)</script>来近似于<script type="math/tex">y^{(i)}</script>来对不同的数据进行分类。</p>
<p><img src="/posts/89451192/代价函数2.png" alt="代价函数2"></p>
<p>神经网络的代价函数如下：  </p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}[\sum_{i=1}^{m}\sum _{k=1}^{k} y^{(i)}_{k}log(h_{\theta}(x^{(i)})_k)+(1-y^{(i)}_k)log(1-h_{\theta}(x^{(i)})_k)]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\theta_{ji}^{(l)})^2</script><p>其中<script type="math/tex">(h_{\theta}(x))_i=i^{th} output</script> 表示第i个输出函数，因为在神经网络中有多个输入项，所以有多个代价函数，在这个式子中代价函数数为k,即k个输出函数(项),所以第一项为上式所 示,故有第 一项为  </p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}[\sum_{i=1}^{m}\sum _{k=1}^{k} y^{(i)}_{k}log(h_{\theta}(x^{(i)})_k)+(1-y^{(i)}_k)log(1-h_{\theta}(x^{(i)})_k)]</script><p>在第二项中， 示有L层，而因为 由上一层与连接下一层而形成的，所以个数也为两层的乘积，而后有从1到L-1，故有第二项为  </p>
<script type="math/tex; mode=display">
\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\theta_{ji}^{(l)})^2</script><h3 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h3><p>反向传播可以计算出在神经网络中各个节点的导数，我们再通过梯度下降算法来进行目标函数的最小化。</p>
<p><img src="/posts/89451192/反向传播1.png" alt="反向传播1"></p>
<p>在上图中我们的最终目的是寻找最小化 <script type="math/tex">J(\theta)</script></p>
<p><img src="/posts/89451192/反向传播2.png" alt="反向传播2"></p>
<p>反射传播从直观上来说就是计算每一层的每个节点的误差，在上图中就是计算第三层的各个节点的误差，而后计算第二层的各个节点的误差，最后计算第一层的各个节点的误差，其中</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta^{(l)}_{ij}}J(\theta)=a_j^{(l)}\delta_i^{(l+1)}</script><p>我们有梯度下降的正向传播过程以及涉及的参数如下  </p>
<p><img src="/posts/89451192/反向传播3.png" alt="反向传播3"></p>
<p>梯度下降算法中的反射传播算法过程如下  </p>
<p><img src="/posts/89451192/反向传播4.png" alt="反向传播4"></p>
<p>其中反向传播算法如下：</p>
<p><img src="/posts/89451192/反向传播5.png" alt="反向传播5"></p>
<p>其中符号<script type="math/tex">\Delta</script>，其中l表示神经网络的层数，i表示样本数，j表示神经网络中某层的第几个节点</p>
<p>算法步骤解释如下：</p>
<p>一、设置所有符号<script type="math/tex">\Delta_{ij}^{(l)}</script>均为0.</p>
<p>二、设置从样本1到m的循环，即遍历所有样本.</p>
<p>三、在循环中设置输入参数为<script type="math/tex">a^{(1)}</script>.</p>
<p>四、执行正向传播算法，求得各层的<script type="math/tex">a^{(l)}</script>.</p>
<p>五、使用结果<script type="math/tex">y^{(i)}</script>来计算输出层的差值<script type="math/tex">\delta^{(l)}</script>.</p>
<p>六、计算隐藏层的差值<script type="math/tex">\delta^{(l-1)},\delta^{(l-2)}...</script></p>
<p>七、计算每个样本的各层的各个节点的<script type="math/tex">\Delta_{ij}^{(l)}</script>,其中式子</p>
<script type="math/tex; mode=display">
\Delta_{ij}^{(l)}=\Delta_{ij}^{(l)}+a_{j}^{(l)}\delta_i^{(l+1)}</script><p>当<script type="math/tex">J(\theta)</script>的m为1，且无正则项时，</p>
<script type="math/tex; mode=display">
\frac {\partial}{\partial \theta_{ij}^{(l)}}J(\theta)=a_j^{(l)}\delta_i^{(l+1)}</script><p>八、 我们计算各样本各层节点的<script type="math/tex">D_{ij}^{(l)}</script>，<script type="math/tex">\frac{\partial}{\partial \theta_{ij}^{(l)}}=D_{ij}^{(l)}</script>，此处的函数<script type="math/tex">J(\theta)</script>m不等于1且有正则项的分两种情况，当</p>
<script type="math/tex; mode=display">
D_{ij}^{(l)}=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda \theta_{ij}^{(l)} \:\:\: if\:\: j\neq0(非偏置项) \\
D_{ij}^{(l)}=\frac{1}{m}\Delta_{ij}^{(l)}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ if\:\: j\neq0(偏置项)</script><p>为什么此时是<script type="math/tex">\lambda \theta_{ij}^{(l)}</script>不是<script type="math/tex">\frac{\lambda }{m}\theta_{ij}^{(l)}</script>暂时不清楚</p>
<h3 id="反向传播算法的理解与推导"><a href="#反向传播算法的理解与推导" class="headerlink" title="反向传播算法的理解与推导"></a>反向传播算法的理解与推导</h3><p><img src="/posts/89451192/image-20220619151637481-16556230021951.png" alt="image-20220619151637481"></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/%E5%90%B4%E6%81%A9%E8%BE%BE/" rel="tag"># 吴恩达</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/5b906d18/" rel="prev" title="高性能mysql">
      <i class="fa fa-chevron-left"></i> 高性能mysql
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/341ecd43/" rel="next" title="git命令">
      git命令 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%8E%9F%E7%90%86"><span class="nav-number">1.</span> <span class="nav-text">梯度下降原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="nav-number">1.1.</span> <span class="nav-text">梯度下降算法</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0%E5%AE%9A%E4%B9%89"><span class="nav-number">1.1.1.</span> <span class="nav-text">代价函数定义</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E6%A6%82%E8%A6%81"><span class="nav-number">1.1.2.</span> <span class="nav-text">算法概要</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3"><span class="nav-number">1.1.3.</span> <span class="nav-text">算法详解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link"><span class="nav-number">1.2.</span> <span class="nav-text"> </span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8E%9F%E7%90%86"><span class="nav-number">2.</span> <span class="nav-text">神经网络原理</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%AE%97%E6%B3%95"><span class="nav-number">2.1.</span> <span class="nav-text">神经网络梯度下降算法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%AF%A6%E8%A7%A3"><span class="nav-number">2.2.</span> <span class="nav-text">神经网络反向传播详解</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E6%B1%82%E5%AF%BC%E6%95%B0"><span class="nav-number">2.2.1.</span> <span class="nav-text">计算图求导数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E6%A8%A1%E5%9E%8B"><span class="nav-number">2.2.2.</span> <span class="nav-text">神经模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E4%BB%B7%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.3.</span> <span class="nav-text">代价函数</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E8%BF%87%E7%A8%8B"><span class="nav-number">2.2.4.</span> <span class="nav-text">反向传播过程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95%E7%9A%84%E7%90%86%E8%A7%A3%E4%B8%8E%E6%8E%A8%E5%AF%BC"><span class="nav-number">2.2.5.</span> <span class="nav-text">反向传播算法的理解与推导</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">刘王权</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">11</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">14</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wangquanliu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wangquanliu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/liuwangquan@163.com" title="E-Mail → liuwangquan@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘王权</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">68k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:02</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
