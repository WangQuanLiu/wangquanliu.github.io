<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"wangquanliu.com","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="本章节为&lt;&lt;动手学深度学习&gt;&gt;的知识点整理。">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学深度学习总结">
<meta property="og:url" content="http://wangquanliu.com/posts/b96b4738/index.html">
<meta property="og:site_name" content="王权个人博客">
<meta property="og:description" content="本章节为&lt;&lt;动手学深度学习&gt;&gt;的知识点整理。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220404205915668.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/小批量随机梯度下降.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220402214312255.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220402214736512.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220402215308326.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220404212518086.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220404212656630.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220404212748893.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409155515489.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409181323633.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409181537778.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409181856448.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409191415164.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409191601846.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409192303044.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409192405732.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409205054558.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409205422168.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409205503297.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409220107017.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409225842985.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220409232656580.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220410092456601.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220410092549529.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220410103609151.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/权重衰减1.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220415182710558.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220415182738771.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220415183135068.png">
<meta property="og:image" content="http://wangquanliu.com/posts/b96b4738/image-20220415183156489.png">
<meta property="article:published_time" content="2022-06-19T07:02:07.000Z">
<meta property="article:modified_time" content="2022-06-24T04:08:04.736Z">
<meta property="article:author" content="刘王权">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="MXNet">
<meta property="article:tag" content="动手学深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://wangquanliu.com/posts/b96b4738/image-20220404205915668.png">

<link rel="canonical" href="http://wangquanliu.com/posts/b96b4738/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>动手学深度学习总结 | 王权个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">王权个人博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签<span class="badge">15</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类<span class="badge">6</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档<span class="badge">12</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://wangquanliu.com/posts/b96b4738/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="刘王权">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="王权个人博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          动手学深度学习总结
        </h1>

        <div class="post-meta">

    <i class="fa fa-thumb-tack"></i>
    <font color=7D26CD>置顶</font>
    <span class="post-meta-divider">|</span>

            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2022-06-19 15:02:07" itemprop="dateCreated datePublished" datetime="2022-06-19T15:02:07+08:00">2022-06-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-06-24 12:08:04" itemprop="dateModified" datetime="2022-06-24T12:08:04+08:00">2022-06-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/" itemprop="url" rel="index"><span itemprop="name">计算机</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>本章节为&lt;&lt;动手学深度学习&gt;&gt;的知识点整理。</p>
<span id="more"></span>
<h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p><img src="/posts/b96b4738/image-20220404205915668.png" alt="image-20220404205915668"></p>
<p><strong>注</strong>：线性回归为一个单层的神经网络</p>
<h3 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h3><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>设房屋面积为x1，房龄为x2，售出价格为y，我们需要建立基于输入x1和x2来计算输出y的表达式，即模型，线性回归假设输出与各个输入之间是线性关系，则我们有公式</p>
<script type="math/tex; mode=display">
y^*=x_1w_1+x_2w_2+b</script><p>其中w1,w2是权重，b是偏差。模型输出 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="2.096ex" height="2.029ex" role="img" focusable="false" viewbox="0 -691.8 926.6 896.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(523,363) scale(0.707)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"/></g></g></g></g></svg></mjx-container> 是线性回归对于真实价格y的预测与估计。</p>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>我们需要寻找特定的模型的参数值，使得模型在数据上的误差尽可能的小。</p>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>我们收集一系列的数据，在这个数据上面寻找模型参数来使得模型的预测价格与真实价格的误差最小。这个数据集被称之为训练集。</p>
<p>假设我们采集的样本数为n，索引为i的样本的特征为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.669ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="2.548ex" role="img" focusable="false" viewbox="0 -830.4 1008.6 1126.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mi" transform="translate(605,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(605,-295.7) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.669ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="2.548ex" role="img" focusable="false" viewbox="0 -830.4 1008.6 1126.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mi" transform="translate(605,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(605,-295.7) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g></g></svg></mjx-container> ，标签为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.848ex" height="2.343ex" role="img" focusable="false" viewbox="0 -830.4 817 1035.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(523,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container> ，对于索引为i的房屋，线性回归模型的房屋价格预测表达式为</p>
<script type="math/tex; mode=display">
y^{*(i)}=x^{i}_1w_1+x^{i}_2w_2+b</script><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>在模型训练中，我们需要衡量价格预测与真实值之间的误差，通常会选取一个非负作为误差，且数值越小表示误差越小，一个常用的式子为</p>
<script type="math/tex; mode=display">
\iota^i(w_1,w_2,b)=\frac{1}{2}(y^{*(i)}-y^{(i)})^2</script><p>其中常数1/2使对平方项求导后的常数系数为1，这样在形式上稍微简单，其中这个式子的误差越小表示预测价格与真实价格越相近。</p>
<p>通常我们用训练数据集中所有样本误差的平均来衡量模型预测的质量，即</p>
<script type="math/tex; mode=display">
\iota(w_1,w_2,b)=\frac{1}{n}\sum_{i=1}^{n}\iota^i(w_1,w_2,b)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(x^i_1w_1+x^i_2w_2+b-y^i)^2</script><p>在模型的训练中，我们希望找出一组模型参数，记为$ w^<em>_1<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="0.629ex" height="0.713ex" role="img" focusable="false" viewbox="0 -121 278 315"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g></g></svg></mjx-container>w^</em>_2 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="0.629ex" height="0.713ex" role="img" focusable="false" viewbox="0 -121 278 315"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g></g></svg></mjx-container>b^*$，来使得训练样本平均损失最小：</p>
<script type="math/tex; mode=display">
w^*_1,w*_2,b^*=argmin_{(w_1,w_2,b)}\iota(w_1,w_2,b)</script><h4 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h4><p>我们通过小批量随机梯度下降法，来进行求出最优的$ w^<em>_1<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="0.629ex" height="0.713ex" role="img" focusable="false" viewbox="0 -121 278 315"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g></g></svg></mjx-container>w^</em>_2 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="0.629ex" height="0.713ex" role="img" focusable="false" viewbox="0 -121 278 315"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g></g></svg></mjx-container>b^*$,使得目标函数最小，小批量随机梯度下降法具有特征在每次迭代中使用b个样本。Mini-Batch梯度下降算法与批量梯度下降算法类似，只不过它会选取B个样本来进行迭代，介于批量梯度下降算法与随机梯度下降算法之间，对于Mini-Batch而言B在
</p><p>批量梯度下降算法：在每次迭代中使用所有样本</p>
<p>随机梯度下降算法：在每次迭代中使用一个样本</p>
<p>Mini-Batch梯度下降算法：在每次迭代中使用b个样本</p>
<p><img src="/posts/b96b4738/小批量随机梯度下降.png" alt="小批量随机梯度下降"></p>
<p>图中是B为10的一个例子，Mini-batch在数据存取和求导的过程中使用向量化，进行并行计算，这样可以加快计算速度，而Mini-batch的一个缺点是需要确定参数B的大小。</p>
<p>对于上例中我们有算法</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Repeat {</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>,i=b+<span class="number">1</span>,i=<span class="number">2b+1</span>,...n{</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
w_1\leftarrow w_1-\frac{n}{|\beta|}(\sum_{i\in\beta}{}\frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial w_1})=w_1-\frac{n}{|\beta|}[\sum_{i\in\beta}{}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})]</script><script type="math/tex; mode=display">
w_2\leftarrow w_2-\frac{n}{|\beta|}(\sum_{i\in\beta}{}\frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial w_2})=w_2-\frac{n}{|\beta|}[\sum_{i\in\beta}{}x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})]</script><script type="math/tex; mode=display">
b\leftarrow b-\frac{n}{|\beta|}(\sum_{i\in\beta}{}\frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial b})=b-\frac{n}{|\beta|}[\sum_{i\in\beta}{}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})]</script><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
<h2 id="线性回归的表示方法"><a href="#线性回归的表示方法" class="headerlink" title="线性回归的表示方法"></a>线性回归的表示方法</h2><p>我们对训练数据集里的3个房屋样本(索引分别为1、2和3)逐一预测价格，将得到</p>
<script type="math/tex; mode=display">
y^{*{(1)}}=x_1^{(1)}w_1+x_2^{(1)}w_2+b \\
y^{*{(2)}}=x_1^{(2)}w_1+x_2^{(2)}w_2+b \\
y^{*{(3)}}=x_1^{(3)}w_1+x_2^{(3)}w_2+b</script><p>将上面3个等式转化成矢量计算，则有</p>
<script type="math/tex; mode=display">
y^*=\left|\begin{matrix}
    y^{*(1)} \\
    y^{*(2)}\\
    y^{*(3)} \\
    \end{matrix} \right| ,
X=\left|\begin{matrix}
    x_1^{(1)}  \:\:x_2^{(1)}\\
    x_1^{(2)}  \:\:x_2^{(2)}\\
    x_1^{(3)}  \:\:x_2^{(3)} \\
    \end{matrix} \right| ,
W=\left|\begin{matrix}
    w_1 \\
    w_2\\
    \end{matrix} \right|</script><p>当数据样本数为n，特征数为d时，线性回归的矢量计算表达式为</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="13.148ex" height="2.034ex" role="img" focusable="false" viewbox="0 -694 5811.6 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(523,363) scale(0.707)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"/></g></g><g data-mml-node="mo" transform="translate(1204.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(2260.1,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"/></g><g data-mml-node="mi" transform="translate(3112.1,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mo" transform="translate(4382.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(5382.6,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g></g></g></svg></mjx-container></p>
<p>其中模型输出$y^<em>\in R^{n</em>1}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="20.362ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 9000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">批</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">量</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">据</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">样</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">本</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">特</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">征</text></g></g></g></svg></mjx-container>X \in R^{n<em>d}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="6.787ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 3000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">权</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">重</text></g></g></g></svg></mjx-container>W \in R^{d</em>1}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="6.787ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 3000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">偏</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">差</text></g></g></g></svg></mjx-container>b \in R<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="20.362ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 9000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">批</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">量</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">据</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">样</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">本</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">标</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">签</text></g></g></g></svg></mjx-container>y\in R^{n*1}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="9.05ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 4000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">设</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">模</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">型</text></g></g></g></svg></mjx-container>\theta=[w_1,w_2,b]^T$，我 们重写损失函数为</p>
<script type="math/tex; mode=display">
\iota(\theta)=\frac{1}{2n}(y^*-y)^T(y^*-y)</script><p>小批量随机梯度下降可以改写为</p>
<script type="math/tex; mode=display">
\theta\leftarrow \theta -\frac{n}{|\beta|}\sum_{i \in \beta}\bigtriangledown_{\theta}\iota^{(i)}(\theta)</script><script type="math/tex; mode=display">
\bigtriangledown_{\theta}\iota^{(i)}(\theta)=\left|\begin{matrix}
    \frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial w_1} \\
    \frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial w_2} \\
    \frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial b}  \\
    \end{matrix} \right|= \left|\begin{matrix}
    x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)} \\
    x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)} \\
    x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)}  \\
    \end{matrix} \right|= \left|\begin{matrix}
    x_1^{(i)} \\
    x_2^{(i)} \\
    1 \\
    \end{matrix} \right|(y^{*(i)}-y^{(i)})</script><h2 id="线性回归从零开始实现"><a href="#线性回归从零开始实现" class="headerlink" title="线性回归从零开始实现"></a>线性回归从零开始实现</h2><p>设我们训练数据集样本数为1000，输入个数(特征数)为2。给定随机生成的批量样本特征<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="11.398ex" height="1.977ex" role="img" focusable="false" viewbox="0 -833.9 5037.9 873.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"/></g><g data-mml-node="mo" transform="translate(1129.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"/></g><g data-mml-node="msup" transform="translate(2074.6,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"/></g><g data-mml-node="TeXAtom" transform="translate(792,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"/></g><g data-mml-node="mo" transform="translate(2000,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"/></g><g data-mml-node="mn" transform="translate(2500,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g></g></g></svg></mjx-container>,我们使用线性回归模型真实权重<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="13.998ex" height="2.47ex" role="img" focusable="false" viewbox="0 -841.7 6187 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mo" transform="translate(993.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mo" transform="translate(2049.6,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"/></g><g data-mml-node="mn" transform="translate(2327.6,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><g data-mml-node="mo" transform="translate(2827.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(3272.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(4050.2,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"/><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"/><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(778,0)"/></g><g data-mml-node="msup" transform="translate(5328.2,0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"/></g><g data-mml-node="mi" transform="translate(311,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g></g></g></g></svg></mjx-container>和偏差<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="6.879ex" height="1.756ex" role="img" focusable="false" viewbox="0 -694 3040.6 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mo" transform="translate(706.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1762.6,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"/><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"/><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(778,0)"/></g></g></g></svg></mjx-container>以及一个随机噪声项<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewbox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g></svg></mjx-container>来生成标签</p>
<script type="math/tex; mode=display">
y=Xw+b+\epsilon</script><p>其中噪声项<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewbox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g></svg></mjx-container>服从均值为0、标准差为0.01的正态分布。噪声代表了数据集中无意义的干扰。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">num_inputs</span>=2 #特征项数</span><br><span class="line"><span class="attribute">num_examples</span>=1000 #样本数</span><br><span class="line">true_w=[2,-3.4]</span><br><span class="line"><span class="attribute">true_b</span>=4.2</span><br><span class="line"><span class="attribute">features</span>=nd.random.normal(scale=1,shape(num_examples,num_inputs)) #生成1000<span class="number">*2</span>个为1的标准差</span><br><span class="line"><span class="attribute">labels</span>=true_w[0]<span class="number">*fea</span>tures[:,0]+true_w[1]<span class="number">*fea</span>tures[;，1]+true_b</span><br><span class="line">lables+=nd.random.normal(<span class="attribute">scale</span>=0.01,shape=labels.shape) #</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该函数返回batch_size(批量大小)个随机样本的特征和标签</span></span><br><span class="line"></span><br><span class="line">def data_iter(batch_size,features,labels):</span><br><span class="line">	<span class="attribute">num_examples</span>=len(features) </span><br><span class="line">	<span class="attribute">indices</span>=list(range(num_examples)) #生成样本数量个随机数</span><br><span class="line">	random.shuffle(indices) #样本的读取顺序是随机的</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(0,num_examples,batch_size):</span><br><span class="line">		<span class="attribute">j</span>=nd.array(indices[i:min(i+batch_size,num_examples)])</span><br><span class="line">		yield features.take(j),labels.take(j) #take函数根据索引返回对应元素</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line"><span class="comment">#我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0</span></span><br><span class="line"><span class="attribute">w</span>=nd.random.normal(scale=0.01,shape=(num_inputs,1))</span><br><span class="line"><span class="attribute">b</span>=nd.zeros(shape=(1,))</span><br><span class="line"></span><br><span class="line"><span class="comment">#申请梯度</span></span><br><span class="line">w.attach_grad()</span><br><span class="line">b.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义函数 使用dot函数做矩阵乘法</span></span><br><span class="line">def linreg(X,w,b):	</span><br><span class="line">	return nd.dot(X,w)+b</span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="comment">#在实现中，我们需要把真实值y变成预测值y_hat的形状</span></span><br><span class="line">def squared_loss(y_hat,y):</span><br><span class="line">	return (y_hat-y.reshape(y.hat.shape))*<span class="number">*2</span>/2</span><br><span class="line"></span><br><span class="line"><span class="comment">#sgd函数实现了小批量随机梯度下降算法，通过不断迭代模型参数来优化损失函数。使用自动求梯度模块计算得来的#梯度是一个批量样本的梯度和，我们将它除以批量大小来得到平均值 </span></span><br><span class="line"><span class="comment"># params参数，lr学习率，batch_size批量大小</span></span><br><span class="line">def sgd(params,lr,batch_size):</span><br><span class="line">	<span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">		param[:]=param-lr*param.grad/batch_size</span><br><span class="line">		</span><br><span class="line"><span class="attribute">lr</span>=0.03	#学习率</span><br><span class="line"><span class="attribute">num_epochs</span>=3 #迭代周期</span><br><span class="line"><span class="attribute">net</span>=linreg	</span><br><span class="line"><span class="attribute">loss</span>=squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):#训练模型一共需要num_epochs个迭代周期</span><br><span class="line">    #在每一个迭代周期中，会使用训练数据集中所有样本一次(假设样本数能够被批量大小整除)。</span><br><span class="line">    #x和y分别是小批量样本的特征和标签</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">        with autograd.record():</span><br><span class="line">            <span class="attribute">l</span>=loss(net(X,w,b),y) #l是有关小批量x和y的损失</span><br><span class="line">        l.backward() #小批量的损失对模型参数求梯度</span><br><span class="line">        sgd([w,b],lr,batch_size) #使用小批量随机梯度下降迭代模型参数</span><br><span class="line">    <span class="attribute">train_l</span>=loss(net(features,w,b),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'epoch %d,loss %f'</span> % (epoch+1,train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220402214312255.png" alt="image-20220402214312255"></p>
<p>通过上面的计算，我们得到结果(true_w为刚开始定义的值,w为我们通过计算得出的值)</p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">true</span>_w,w</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220402214736512.png" alt="image-20220402214736512"></p>
<figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="literal">true</span>_b,b</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220402215308326.png" alt="image-20220402215308326"></p>
<p>通过以上结果我们可以知道，以上算法是通过优化目标模型来反向求得参数，使得求得的参数最接近原参数。</p>
<h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet import autograd,nd</span><br><span class="line"></span><br><span class="line"><span class="attribute">num_inputs</span>=2</span><br><span class="line"><span class="attribute">num_examples</span>=1000</span><br><span class="line">true_w=[2,3.4]</span><br><span class="line"><span class="attribute">true_b</span>=4.2</span><br><span class="line"><span class="attribute">features</span>=nd.random.normal(scale=1,shape=(num_examples,num_inputs))</span><br><span class="line"><span class="attribute">labels</span>=true_w[0]<span class="number">*fea</span>tures[:,0]+true_w[1]<span class="number">*fea</span>tures[:,1]+true_b</span><br><span class="line">labels+=nd.random.normal(<span class="attribute">scale</span>=0.01,shape=labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import data as gdata</span><br><span class="line"></span><br><span class="line"><span class="attribute">batch_size</span>=10</span><br><span class="line"><span class="comment">#将训练数据的特征和标签组合</span></span><br><span class="line"><span class="attribute">dataset</span>=gdata.ArrayDataset(features,labels)</span><br><span class="line"><span class="comment">#随机读取小批量</span></span><br><span class="line"><span class="attribute">data_iter</span>=gdata.DataLoader(dataset,batch_size,shuffle=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line"><span class="comment">#Sequential为一个串联各个层的容器，可以看作一个画板</span></span><br><span class="line"><span class="comment">#Dense为全连接层，定义该层的输出为1</span></span><br><span class="line"><span class="comment">#Gluon中我们无须指定每一层输入的形状，如线性回归的输入个数，当模型得到数据时，如后面的执行net(X)时</span></span><br><span class="line"><span class="comment">#模型将自动推断出每一层的输入个数，如若是全相连的，通过层n推断n+1层的输入也是比较合理，且能实现的</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import nn</span><br><span class="line"><span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(1))</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line"><span class="comment">#从mxnet导入init模块，该模块提供了模型参数初始化的各种方法，通过init.Normal(sigma=0.01)指权重</span></span><br><span class="line"><span class="comment">#参数每个元素在初始化时随机采样于均值为0，标准差为0.01的正态分布，偏差参数默认会为0</span></span><br><span class="line"><span class="keyword">from</span> mxnet import init </span><br><span class="line">net.initialize(init.Normal(<span class="attribute">sigma</span>=0.01))</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import loss as gloss</span><br><span class="line"><span class="attribute">loss</span>=gloss.L2Loss() #平方损失又称L2范数损失</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义优化函数</span></span><br><span class="line"><span class="comment">#在导入Gluon之后，我们创建一个Trainer实例，并指定学习率为0.03的小批量随机梯度下降(sgd)为优化算法</span></span><br><span class="line"><span class="comment">#该优化算法用来迭代net实例所有通过add函数嵌套的层所包含的全部参数，这些参数可以通过collect_params</span></span><br><span class="line"><span class="comment">#函数获取</span></span><br><span class="line"><span class="keyword">from</span> mxnet import gluon</span><br><span class="line"><span class="attribute">trainer</span>=gluon.Trainer(net.collect_params(),'sgd',{<span class="string">'learning_rate'</span>:0.03})</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="comment">#在使用Gluon训练模型时，我们通过调用Trainer实例的step函数来迭代模型参数，由于变量l是长度为</span></span><br><span class="line"><span class="comment">#batch_size的一维NDArray，执行l.backward()等价于执行l.sum().backward()。按照小批量随机</span></span><br><span class="line"><span class="comment">#梯度下降的定义，我们在step函数中指明批量大小，从而对批量中样本梯度求平均</span></span><br><span class="line"><span class="attribute">num_epochs</span>=3;</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(1,num_epochs+1):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        with autograd.record():</span><br><span class="line">             <span class="attribute">l</span>=loss(net(X),y)</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.<span class="keyword">step</span>(batch_size)</span><br><span class="line">    <span class="attribute">l</span>=loss(net(features),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'epoch %d, loss:%f'</span> % (epoch,l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220404212518086.png" alt="image-20220404212518086"></p>
<p>下面我们分别比较学到的模型参数与真实的模型参数，从而从net获得需要的层，并访问其权重和偏差。</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">dense</span>=net[<span class="number">0</span>]</span><br><span class="line"><span class="title">true_w</span>,dense.weight.<span class="class"><span class="keyword">data</span>()</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220404212656630.png" alt="image-20220404212656630"></p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">true_b</span>,dense.bias.<span class="class"><span class="keyword">data</span>()</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220404212748893.png" alt="image-20220404212748893"></p>
<h2 id="图像分类数据集-Fashion-MNIST"><a href="#图像分类数据集-Fashion-MNIST" class="headerlink" title="图像分类数据集(Fashion-MNIST)"></a>图像分类数据集(Fashion-MNIST)</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导包</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取数据集，从网上获取数据集，通过train来指定获取训练数据集或测试数据集</span></span><br><span class="line">mnist_train=gdata.vision.FashionMNIST(train=<span class="literal">True</span>)</span><br><span class="line">mnist_test=gdata.vision.FashionMNIST(train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mnist_train[0]同时返回feature与label,变量feature对应⾼和宽均为28像素的图像。每个像素的数值为0</span></span><br><span class="line"><span class="comment">#到255之间8位⽆符号整数（uint8)</span></span><br><span class="line">feature,label=mnist_train[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出 ((28,28,1),numpy.uint8)</span></span><br><span class="line">feature.shape,feature.dtype</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出 (2,numpy.int32,dtype('int32')) label为2，后面通过函数把标签的数字转成文字标签</span></span><br><span class="line">label,<span class="built_in">type</span>(label),label.dtype</span><br><span class="line"></span><br><span class="line"><span class="comment">#将标签中的数字转换成文字标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span><br><span class="line">    text_labels=[<span class="string">'t-shirt'</span>,<span class="string">'trouser'</span>,<span class="string">'pullover'</span>,<span class="string">'dress'</span>,<span class="string">'coat'</span>,</span><br><span class="line">                 <span class="string">'sandal'</span>,<span class="string">'shirt'</span>,<span class="string">'sneaker'</span>,<span class="string">'bag'</span>,<span class="string">'ankle boot'</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line">    </span><br><span class="line"><span class="comment">#下⾯定义⼀个可以在⼀⾏⾥画出多张图像和对应标签的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_fashion_mnist</span>(<span class="params">images,labels</span>):</span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    _,figs=d2l.plt.subplots(<span class="number">1</span>,<span class="built_in">len</span>(images),figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> f,img,lbl <span class="keyword">in</span> <span class="built_in">zip</span>(figs,images,labels):</span><br><span class="line">        f.imshow(img.reshape((<span class="number">28</span>,<span class="number">28</span>)).asnumpy())</span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">True</span>)</span><br><span class="line">        f.axes.get_yaxis().set_visible(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#我们看⼀下训练数据集中前9个样本的图像内容和⽂本标签</span></span><br><span class="line">X,y=mnist_train[<span class="number">0</span>:<span class="number">9</span>]</span><br><span class="line">show_fashion_mnist(X,get_fashion_mnist_labels(y))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409155515489.png" alt="image-20220409155515489"></p>
<h2 id="softmax回归从零开始实现"><a href="#softmax回归从零开始实现" class="headerlink" title="softmax回归从零开始实现"></a>softmax回归从零开始实现</h2><figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib <span class="keyword">inline</span></span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd,nd</span><br><span class="line"></span><br><span class="line">#设置批处理大小，且导入数据到训练集与测试集中</span><br><span class="line">batch_size=<span class="number">256</span></span><br><span class="line">train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#因为softmax回归模型是一个单层神经网络</span></span><br><span class="line"><span class="comment">#又由于输入向量的长度为28*28=784，该向量的每个元素对应图像中的每个像素，由于图像有十个类别，所以单层</span></span><br><span class="line"><span class="comment">#神经网络的输出层的输出个数为10</span></span><br><span class="line"><span class="attribute">num_inputs</span>=784</span><br><span class="line"><span class="attribute">num_outputs</span>=10</span><br><span class="line"><span class="attribute">w</span>=nd.random.normal(scale=0.01,shape=(num_inputs,num_outputs))</span><br><span class="line"><span class="attribute">b</span>=nd.zeros(num_outputs)</span><br><span class="line">w.attach_grad()</span><br><span class="line">b.attach_grad()</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409181323633.png" alt="image-20220409181323633"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在下⾯的函数中，矩阵X的⾏数是样本数，</span></span><br><span class="line"><span class="comment">#列数是输出个数。为了表达样本预测各个输出的概率， softmax运算会先通过exp函数对每个元素</span></span><br><span class="line"><span class="comment">#做指数运算，再对exp矩阵同⾏元素求和，最后令矩阵每⾏各元素与该⾏元素之和相除。这样⼀</span></span><br><span class="line"><span class="comment">#来，最终得到的矩阵每⾏元素和为1且⾮负。因此，该矩阵每⾏都是合法的概率分布。 softmax运</span></span><br><span class="line"><span class="comment">#算的输出矩阵中的任意⼀⾏元素代表了⼀个样本在各个输出类别上的预测概率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp=X.exp()</span><br><span class="line">    partition=X_exp.<span class="built_in">sum</span>(axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp/partition</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409181537778.png" alt="image-20220409181537778"></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义了softmax的回归模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(nd.dot(X.reshape((-<span class="number">1</span>,num_inputs)),w)+b)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409181856448.png" alt="image-20220409181856448"></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line"><span class="comment">#对于样本i,使其他在正确标签的的时候值为1，其余的时候为0</span></span><br><span class="line"><span class="comment">#又因为每个样本只有一个标签，那么损失函数可能以进行简写</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat,y</span>)</span><br><span class="line">	<span class="keyword">return</span> -nd.pick(y_hat,y).log()</span><br></pre></td></tr></table></figure>
<p>上面的损失函数等价于<script type="math/tex">-log_{y^{(i)}}y^{*(i)}</script>，其余部分后面实现</p>
<p><img src="/posts/b96b4738/image-20220409191415164.png" alt="image-20220409191415164"></p>
<p><img src="/posts/b96b4738/image-20220409191601846.png" alt="image-20220409191601846"></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义两个数之间的准确率</span></span><br><span class="line"><span class="comment">#相等条件判别式(y_hat.argmax(axis=1) == y)是⼀个值为0（相等为假）或1（相等</span></span><br><span class="line"><span class="comment">#为真）的NDArray。由于标签类型为整数，我们先将变量y变换为浮点数再进⾏相等条件判断。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat,y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(axis=<span class="number">1</span>)==y.astype(<span class="string">'float32'</span>)).mean().asscalar()</span><br></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def evaluate_accuracy(data_iter,net):</span><br><span class="line">    acc_sum,n=<span class="number">0.0</span>,<span class="number">0</span></span><br><span class="line">    for X,y in data_iter:</span><br><span class="line">        y=y.<span class="built_in">astype</span>(<span class="string">'float32'</span>)</span><br><span class="line">        acc_sum+=(<span class="built_in">net</span>(X).<span class="built_in">argmax</span>(axis=<span class="number">1</span>)==y).<span class="built_in">sum</span>().<span class="built_in">asscalar</span>()</span><br><span class="line">        n+=y.size</span><br><span class="line">    return acc_sum/n</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">num_epochs,<span class="attribute">lr</span>=5,0.1</span><br><span class="line"></span><br><span class="line">def train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,<span class="attribute">params</span>=None,lr=None,trainer=None):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum,train_acc_sum,<span class="attribute">n</span>=0.0,0.0,0</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter: #train_iter 返回图像和标签</span><br><span class="line">            with autograd.record(): </span><br><span class="line">                <span class="attribute">y_hat</span>=net(X) #自己的y值</span><br><span class="line">                <span class="attribute">l</span>=loss(y_hat,y).sum()</span><br><span class="line">            l.backward() #求导</span><br><span class="line">            <span class="keyword">if</span> trainer is None:</span><br><span class="line">                d2l.sgd(params,lr,batch_size) #之前定义的小型梯度下降</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                trainer.<span class="keyword">step</span>(batch_size)</span><br><span class="line">            <span class="attribute">y</span>=y.astype('float32')</span><br><span class="line">            train_l_sum+=l.asscalar()</span><br><span class="line">            train_acc_sum+=(y_hat.argmax(<span class="attribute">axis</span>=1)==y).sum().asscalar()</span><br><span class="line">            n+=y.size</span><br><span class="line">        <span class="attribute">test_acc</span>=evaluate_accuracy(test_iter,net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'epoch %d,loss %.4f,train acc %.3f,test acc %.3f'</span>%(epoch+1,train_l_sum/n,train_acc_sum/n,test_acc))</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train<span class="constructor">_ch3(<span class="params">net</span>,<span class="params">train_iter</span>,<span class="params">test_iter</span>,<span class="params">cross_entropy</span>,<span class="params">num_epochs</span>,<span class="params">batch_size</span>,[<span class="params">w</span>,<span class="params">b</span>],<span class="params">lr</span>)</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409192303044.png" alt="image-20220409192303044"></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#给定⼀系列图像（第三⾏图像输出），我们⽐较⼀下它们的真实标签（第⼀⾏⽂本输出）和模型预测结果（第⼆⾏⽂</span><br><span class="line">#本输出）</span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> test_iter:</span><br><span class="line">    break</span><br><span class="line">true_labels=d2l<span class="selector-class">.get_fashion_mnist_labels</span>(y<span class="selector-class">.asnumpy</span>())</span><br><span class="line">pred_labels=d2l<span class="selector-class">.get_fashion_mnist_labels</span>(<span class="built_in">net</span>(X)<span class="selector-class">.argmax</span>(axis=<span class="number">1</span>)<span class="selector-class">.asnumpy</span>())</span><br><span class="line">titels=<span class="selector-attr">[true+<span class="string">'\n'</span>+pred for true,pred in zip(true_labels,pred_labels)]</span></span><br><span class="line">d2l<span class="selector-class">.show_fashion_mnist</span>(X<span class="selector-attr">[0:9]</span>,titels<span class="selector-attr">[0:9]</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409192405732.png" alt="image-20220409192405732"></p>
<h2 id="softmax回归简洁实现"><a href="#softmax回归简洁实现" class="headerlink" title="softmax回归简洁实现"></a>softmax回归简洁实现</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import d2lzh as d2l</span><br><span class="line"><span class="keyword">from</span> mxnet import autograd,nd</span><br><span class="line"><span class="keyword">from</span> mxnet import gluon,init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import loss as gloss,nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line"><span class="attribute">batch_size</span>=256</span><br><span class="line">train_iter,<span class="attribute">test_iter</span>=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义和初始化模型</span></span><br><span class="line"><span class="comment">#softmax回归的输出是一个全连接层，因此，我们添加一个输出个数为10的全连接层，我们使用均值0、标准差为#0.01的正态分布随机初始化模型的权重参数</span></span><br><span class="line"><span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(10))</span><br><span class="line">net.initialize(init.Normal(<span class="attribute">sigma</span>=0.01))</span><br><span class="line"></span><br><span class="line"><span class="comment">#softmax和交叉熵损失函数</span></span><br><span class="line"><span class="comment">#Gluon提供了⼀个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。</span></span><br><span class="line"><span class="attribute">loss</span>=gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义优化算法</span></span><br><span class="line"><span class="comment">#使用学习率为0.1的小批量随机梯度下降作为优化算法</span></span><br><span class="line"><span class="attribute">trainer</span>=gluon.Trainer(net.collect_params(),'sgd',{<span class="string">'learning_rate'</span>:0.1})</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="attribute">num_epochs</span>=5</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer)</span><br></pre></td></tr></table></figure>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p><img src="/posts/b96b4738/image-20220409205054558.png" alt="image-20220409205054558"></p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p><img src="/posts/b96b4738/image-20220409205422168.png" alt="image-20220409205422168"></p>
<p><img src="/posts/b96b4738/image-20220409205503297.png" alt="image-20220409205503297"></p>
<h2 id="多层感知机的从零开始实现"><a href="#多层感知机的从零开始实现" class="headerlink" title="多层感知机的从零开始实现"></a>多层感知机的从零开始实现</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import d2lzh as d2l</span><br><span class="line"><span class="keyword">from</span> mxnet import<span class="built_in"> nd</span></span><br><span class="line"><span class="built_in"></span><span class="keyword">from</span> mxnet.gluon import loss as gloss</span><br><span class="line"></span><br><span class="line"><span class="attribute">batch_size</span>=256</span><br><span class="line">train_iter,<span class="attribute">test_iter</span>=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fashion-MNIST数据集中图像形状</span></span><br><span class="line"><span class="comment">#为28 × 28，类别数为10。本节中我们依然使⽤⻓度为28 × 28 = 784的向量表⽰每⼀张图像。因此，</span></span><br><span class="line"><span class="comment">#输⼊个数为784，输出个数为10。实验中，我们设超参数隐藏单元个数为256</span></span><br><span class="line">num_inputs,num_outputs,<span class="attribute">num_hiddens</span>=784,10,256</span><br><span class="line"><span class="attribute">W1</span>=nd.random.normal(scale=0.01,shape=(num_inputs,num_hiddens))</span><br><span class="line"><span class="attribute">b1</span>=nd.zeros(num_hiddens)</span><br><span class="line"><span class="attribute">W2</span>=nd.random.normal(scale=0.01,shape=(num_hiddens,num_outputs))</span><br><span class="line"><span class="attribute">b2</span>=nd.zeros(num_outputs)</span><br><span class="line">params=[W1,b1,W2,b2]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="多层感知机的简洁实现"><a href="#多层感知机的简洁实现" class="headerlink" title="多层感知机的简洁实现"></a>多层感知机的简洁实现</h2><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">import d2lzh as d2l</span><br><span class="line"><span class="keyword">from</span> mxnet import gluon,init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import loss as gloss,nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#和softmax回归唯⼀的不同在于，我们多加了⼀个全连接层作为隐藏层。它的隐藏单元个数为256，</span></span><br><span class="line"><span class="comment">#并使⽤ReLU函数作为激活函数</span></span><br><span class="line"><span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(256,<span class="attribute">activation</span>=<span class="string">'relu'</span>),nn.Dense(10))</span><br><span class="line">net.initialize(init.Normal(<span class="attribute">sigma</span>=0.01))</span><br><span class="line"></span><br><span class="line"><span class="attribute">batch_size</span>=256</span><br><span class="line">train_iter,<span class="attribute">test_iter</span>=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="attribute">loss</span>=gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"><span class="attribute">trainer</span>=gluon.Trainer(net.collect_params(),'sgd',{<span class="string">'learning_rate'</span>:0.5})</span><br><span class="line"><span class="attribute">num_epochs</span>=5</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer)</span><br></pre></td></tr></table></figure>
<h2 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h2><p>给定一个由标量数据特征x和对应的标题标签y组成的训练数据集，多项式函数拟合的目标是找一个K项多项式的函数</p>
<script type="math/tex; mode=display">
y^*=b+\sum_{k=1}^{K}x^kw_k</script><p>来近似，在上式中，<script type="math/tex">w_k</script>的模型的权重参数,b是偏差参数。</p>
<p>因为⾼阶多项式函数模型参数更多，模型函数的选择空间更⼤，所以⾼阶多项式函数⽐低阶多项式函数的复杂度更⾼。因此，⾼阶多项式函数⽐低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差。给定训练数据集，模型复杂度和误差之间的关系通常如图3.4所⽰。给定训练数据集，如果模型的复杂度过低，很容易出现⽋拟合；如果模型复杂度过⾼，很容易出现过拟合。应对⽋拟合和过拟合的⼀个办法是针对数据集选择合适复杂度的模型。</p>
<p><img src="/posts/b96b4738/image-20220409220107017.png" alt="image-20220409220107017"></p>
<p>影响⽋拟合和过拟合的另⼀个重要因素是训练数据集的⼤小。⼀般来说，如果训练数据集中样本数过少，特别是⽐模型参数数量（按元素计）更少时，过拟合更容易发⽣。此外，泛化误差不会随训练数据集⾥样本数量增加而增⼤。因此，在计算资源允许的范围之内，我们通常希望训练数据集⼤⼀些，特别是在模型复杂度较⾼时，如层数较多的深度学习模型。  </p>
<figure class="highlight pgsql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib <span class="keyword">inline</span></span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd,gluon,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata,loss <span class="keyword">as</span> gloss,nn</span><br><span class="line"></span><br><span class="line">n_train,n_test,true_w,true_b=<span class="number">100</span>,<span class="number">100</span>,[<span class="number">1.2</span>,<span class="number">-3.4</span>,<span class="number">5.6</span>],<span class="number">5</span></span><br><span class="line">features=nd.random.normal(shape=(n_train+n_test,<span class="number">1</span>))</span><br><span class="line">#拼接,ploy_featuers=x,x^<span class="number">2</span>,x^<span class="number">3</span></span><br><span class="line">ploy_features=nd.concat(features,nd.power(features,<span class="number">2</span>),nd.power(features,<span class="number">3</span>))</span><br><span class="line"><span class="meta">#y=1.2x-3.4x^2+5.6x^3+5</span></span><br><span class="line">labels=(true_w[<span class="number">0</span>]*ploy_features[:,<span class="number">0</span>]+true_w[<span class="number">1</span>]*ploy_features[:,<span class="number">1</span>]+true_w[<span class="number">2</span>]*ploy_features[:,<span class="number">2</span>]+true_b)</span><br><span class="line"><span class="meta">#y+=噪声项</span></span><br><span class="line">labels+=nd.random.normal(scale=<span class="number">0.1</span>,shape=labels.shape)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409225842985.png" alt="image-20220409225842985"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#作图函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">semilogy</span>(<span class="params">x_vals,y_vals,x_label,y_label,x2_vals=<span class="literal">None</span>,y2_vals=<span class="literal">None</span>,legend=<span class="literal">None</span>,figsize=(<span class="params"><span class="number">3.5</span>,<span class="number">2.5</span></span>)</span>):</span><br><span class="line">    d2l.set_figsize(figsize)</span><br><span class="line">    d2l.plt.xlabel(x_label)</span><br><span class="line">    d2l.plt.ylabel(y_label)</span><br><span class="line">    d2l.plt.semilogy(x_vals,y_vals)</span><br><span class="line">    <span class="keyword">if</span> x2_vals <span class="keyword">and</span> y2_vals:</span><br><span class="line">       d2l.plt.semilogy(x2_vals,y2_vals,linestyle=<span class="string">':'</span>)</span><br><span class="line">       d2l.plt.legend(legend)</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">num_epochs,<span class="attribute">loss</span>=100,gloss.L2Loss()</span><br><span class="line">def fit_and_plot(train_features,test_features,train_labels,test_labels):</span><br><span class="line">    <span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(1)) #单层神经网络，一个输出单元</span><br><span class="line">    net.initialize() #初始化参数</span><br><span class="line">    <span class="attribute">batch_size</span>=min(10,train_labels.shape[0])</span><br><span class="line">    <span class="attribute">train_iter</span>=gdata.DataLoader(gdata.ArrayDataset(train_features,train_labels),batch_size,shuffle=True)</span><br><span class="line">    <span class="attribute">trainer</span>=gluon.Trainer(net.collect_params(),'sgd',{<span class="string">'learning_rate'</span>:0.01})</span><br><span class="line">    train_ls,test_ls=[], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            with autograd.record():</span><br><span class="line">                <span class="attribute">l</span>=loss(net(X),y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.<span class="keyword">step</span>(batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features),train_labels).mean().asscalar())</span><br><span class="line">        test_ls.append(loss(net(test_features),test_labels).mean().asscalar())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'final epoch:train loss'</span>,train_ls[-1],<span class="string">'test loss'</span>,test_ls[-1])</span><br><span class="line">    semilogy(range(1,num_epochs+1),train_ls,<span class="string">'epochs'</span>,<span class="string">'loss'</span>,range(1,num_epochs+1),test_ls,[<span class="string">'train'</span>,<span class="string">'test'</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'weight:'</span>,net[0].weight.data().asnumpy(),<span class="string">'\nbias:'</span>,net[0].bias.data().asnumpy())</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#三阶多项式函数拟合(正常)</span><br><span class="line">#我们先使⽤与数据⽣成函数同阶的三阶多项式函数拟合。实验表明，这个模型的训练误差和在测</span><br><span class="line">#试数据集的误差都较低。训练出的模型参数也接近真实值： w1 = <span class="number">1</span>:<span class="number">2</span>; w2 = −<span class="number">3</span>:<span class="number">4</span>; w3 = <span class="number">5</span>:<span class="number">6</span>; b = <span class="number">5</span>。</span><br><span class="line">#ploy_features<span class="literal">[:<span class="identifier">n_train</span>,:]</span>取前<span class="number">100</span>个元素，ploy_features<span class="literal">[<span class="identifier">n_train</span>:,:]</span>取后<span class="number">100</span>个元素</span><br><span class="line">fit<span class="constructor">_and_plot(<span class="params">ploy_features</span>[:<span class="params">n_train</span>,:],<span class="params">ploy_features</span>[<span class="params">n_train</span>:,:],<span class="params">labels</span>[:<span class="params">n_train</span>],<span class="params">labels</span>[<span class="params">n_train</span>:])</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409232656580.png" alt="image-20220409232656580"></p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#线性函数拟合(欠拟合)</span><br><span class="line">#这个地方，生成的函数是线性的，所以有欠拟合状态</span><br><span class="line">fit<span class="constructor">_and_plot(<span class="params">features</span>[:<span class="params">n_train</span>,:],<span class="params">features</span>[<span class="params">n_train</span>:,:],<span class="params">labels</span>[:<span class="params">n_train</span>],<span class="params">labels</span>[<span class="params">n_train</span>:])</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220410092456601.png" alt="image-20220410092456601"></p>
<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#训练样本不足(过拟合)</span><br><span class="line">fit<span class="constructor">_and_plot(<span class="params">ploy_features</span>[0:2,:],<span class="params">ploy_features</span>[<span class="params">n_train</span>:,:],<span class="params">labels</span>[0:2],<span class="params">labels</span>[<span class="params">n_train</span>:])</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220410092549529.png" alt="image-20220410092549529"></p>
<h2 id="权重衰减-原理暂不解释"><a href="#权重衰减-原理暂不解释" class="headerlink" title="权重衰减-原理暂不解释"></a>权重衰减-原理暂不解释</h2><p>​    <img src="/posts/b96b4738/image-20220410103609151.png" alt="image-20220410103609151"></p>
<p><img src="/posts/b96b4738/权重衰减1.png" alt="权重衰减1"></p>
<p>在上图中我们的更新公式为</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j(1-a\frac{\lambda}{m})-a\frac{1}{m}(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)})</script><script type="math/tex; mode=display">1-a\frac{\lambda}{m}<1$$不大于1，这样进行逐步进行迭代。

<figure class="highlight clean"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib <span class="keyword">inline</span></span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd,gluon,init,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata,loss <span class="keyword">as</span> gloss,nn</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">n_train,n_test,<span class="attribute">num_inputs</span>=20,100,100</span><br><span class="line">true_w,<span class="attribute">true_b</span>=nd.ones((num_inputs,1))*0.01,0.05</span><br><span class="line"></span><br><span class="line"><span class="attribute">features</span>=nd.random.normal(shape=(n_train+n_test,num_inputs))</span><br><span class="line"><span class="attribute">labels</span>=nd.dot(features,true_w)+true_b</span><br><span class="line">labels+=nd.random.normal(<span class="attribute">scale</span>=0.01,shape=labels.shape)</span><br><span class="line"></span><br><span class="line">train_features,<span class="attribute">test_features</span>=features[:n_train,:],features[n_train:,:]</span><br><span class="line">train_labels,<span class="attribute">test_labels</span>=labels[:n_train],labels[n_train:]</span><br></pre></td></tr></table></figure>

![image-20220410104348188](动手学深度学习总结/image-20220410104348188.png)

<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#初始化模型参数</span><br><span class="line">def init_params():</span><br><span class="line">    w=nd.random.<span class="built_in">normal</span>(scale=<span class="number">1</span>,shape=(num_inputs,<span class="number">1</span>))</span><br><span class="line">    b=nd.<span class="built_in">zeros</span>(shape=(<span class="number">1</span>,))</span><br><span class="line">    w.<span class="built_in">attach_grad</span>()</span><br><span class="line">    b.<span class="built_in">attach_grad</span>()</span><br><span class="line">    return [w,b]</span><br><span class="line"></span><br><span class="line">#定义l2范数惩罚项</span><br><span class="line">def <span class="built_in">l2_penalty</span>(w):</span><br><span class="line">    return (w**<span class="number">2</span>).<span class="built_in">sum</span>()/<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">#定义训练与测试</span><br><span class="line">batch_size,num_epochs,lr=<span class="number">1</span>,<span class="number">100</span>,<span class="number">0.003</span></span><br><span class="line">net,loss=d2l.linreg,d2l.squared_loss</span><br><span class="line">train_iter=gdata.<span class="built_in">DataLoader</span>(gdata.<span class="built_in">ArrayDataset</span>(train_features,train_labels),batch_size,shuffle=True)</span><br><span class="line"></span><br><span class="line">def <span class="built_in">fit_and_plot</span>(lambd):</span><br><span class="line">    w,b=<span class="built_in">init_params</span>()</span><br><span class="line">    train_ls,test_ls=[],[]</span><br><span class="line">    for _ in <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        for X,y in train_iter:</span><br><span class="line">            with autograd.<span class="built_in">record</span>():</span><br><span class="line">                l=<span class="built_in">loss</span>(<span class="built_in">net</span>(X,w,b),y)+lambd*<span class="built_in">l2_penalty</span>(w)</span><br><span class="line">            l.<span class="built_in">backward</span>()</span><br><span class="line">            d2l.<span class="built_in">sgd</span>([w,b],lr,batch_size)</span><br><span class="line">        train_ls.<span class="built_in">append</span>(<span class="built_in">loss</span>(<span class="built_in">net</span>(train_features,w,b),train_labels).<span class="built_in">mean</span>().<span class="built_in">asscalar</span>())</span><br><span class="line">        test_ls.<span class="built_in">append</span>(<span class="built_in">loss</span>(<span class="built_in">net</span>(test_features,w,b),test_labels).<span class="built_in">mean</span>().<span class="built_in">asscalar</span>())</span><br><span class="line">    d2l.<span class="built_in">semilogy</span>(<span class="built_in">range</span>(<span class="number">1</span>,num_epochs+<span class="number">1</span>),train_ls,<span class="string">&#x27;epochs&#x27;</span>,<span class="string">&#x27;loss&#x27;</span>,<span class="built_in">range</span>(<span class="number">1</span>,num_epochs+<span class="number">1</span>),test_ls,[<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;L2 norm of w:&#x27;</span>,w.<span class="built_in">norm</span>().<span class="built_in">asscalar</span>())</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#观察过拟合</span></span><br><span class="line">fit_and_plot(<span class="attribute">lambd</span>=0)</span><br></pre></td></tr></table></figure>

![image-20220410104604559](动手学深度学习总结/image-20220410104604559.png)

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用权重衰减</span></span><br><span class="line">fit_and_plot(<span class="attribute">lambd</span>=3)</span><br></pre></td></tr></table></figure>

![image-20220410104636790](动手学深度学习总结/image-20220410104636790.png)

## 权重衰减简洁实现

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import d2lzh as d2l</span><br><span class="line"><span class="keyword">from</span> mxnet import autograd,gluon,init,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import data as gdata,loss as gloss,nn</span><br><span class="line"></span><br><span class="line">n_train,n_test,<span class="attribute">num_inputs</span>=20,100,100</span><br><span class="line">true_w,<span class="attribute">true_b</span>=nd.ones((num_inputs,1))*0.01,0.05</span><br><span class="line"></span><br><span class="line"><span class="attribute">features</span>=nd.random.normal(shape=(n_train+n_test,num_inputs))</span><br><span class="line"><span class="attribute">labels</span>=nd.dot(features,true_w)+true_b</span><br><span class="line">labels+=nd.random.normal(<span class="attribute">scale</span>=0.01,shape=labels.shape)</span><br><span class="line"></span><br><span class="line">train_features,<span class="attribute">test_features</span>=features[:n_train,:],features[n_train:,:]</span><br><span class="line">train_labels,<span class="attribute">test_labels</span>=labels[:n_train],labels[n_train:]</span><br><span class="line"></span><br><span class="line">batch_size,num_epochs,<span class="attribute">lr</span>=1,100,0.003</span><br><span class="line">net,<span class="attribute">loss</span>=d2l.linreg,d2l.squared_loss</span><br><span class="line"><span class="attribute">train_iter</span>=gdata.DataLoader(gdata.ArrayDataset(train_features,train_labels),batch_size,shuffle=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fit_and_plot_gluon(wd):</span><br><span class="line">    <span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">    #nn.Dense</span><br><span class="line">    #Dense实现了如下操作:output = activation(dot(input, weight) + bias)，其中activation是	  #作为激活参数传递的元素激活函数，weight是由层创建的权重矩阵，bias是由层创建的偏差向量(仅在    	 #use_bias为<span class="literal">True</span>时适用)。  </span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(1))</span><br><span class="line">    net.initialize(init.Normal(<span class="attribute">sigma</span>=1))</span><br><span class="line">    #nn.collect_params</span><br><span class="line">    #返回一个包含该Block及其所有子块Parameters的参数字典(默认)，也可以返回匹配某些给定正则表达式的	#select参数字典。</span><br><span class="line">    #wd也为函数的一个参数</span><br><span class="line">    #对权重参数衰减，权重名称一般以weight结尾</span><br><span class="line">    #不对偏差参数衰减，权重名称一般以bias结尾</span><br><span class="line">    <span class="attribute">trainer_w</span>=gluon.Trainer(net.collect_params(&#x27;.*weight&#x27;),&#x27;sgd&#x27;,&#123;<span class="string">&#x27;learning_rate&#x27;</span>:lr,<span class="string">&#x27;wd&#x27;</span>:wd&#125;)</span><br><span class="line">    <span class="attribute">trainer_b</span>=gluon.Trainer(net.collect_params(&#x27;.*bias&#x27;),&#x27;sgd&#x27;,&#123;<span class="string">&#x27;learning_rate&#x27;</span>:lr&#125;)</span><br><span class="line">    train_ls,test_ls=[],[]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            with autograd.record():</span><br><span class="line">                <span class="attribute">l</span>=loss(net(X),y)</span><br><span class="line">            l.backward()</span><br><span class="line">            #更新权重和偏差</span><br><span class="line">            trainer_w.<span class="keyword">step</span>(batch_size) #设置批量大小，设置梯度下降</span><br><span class="line">            trainer_b.<span class="keyword">step</span>(batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features),train_labels).mean().asscalar())</span><br><span class="line">        test_ls.append(loss(net(test_features),test_labels).mean().asscalar())</span><br><span class="line">    d2l.semilogy(range(1,num_epochs+1),train_ls,<span class="string">&#x27;epochs&#x27;</span>,<span class="string">&#x27;loss&#x27;</span>,range(1,num_epochs+1),test_ls,[<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;L2 norm of w:&#x27;</span>,net[0].weight.data().norm().asscalar())</span><br></pre></td></tr></table></figure>

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">fit_and_plot_gluon</span><span class="params">(<span class="number">0</span>)</span></span></span><br></pre></td></tr></table></figure>

![image-20220410115627382](动手学深度学习总结/image-20220410115627382.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">fit_and_plot_gluon</span><span class="params">(<span class="number">3</span>)</span></span></span><br></pre></td></tr></table></figure>

![image-20220410115644377](动手学深度学习总结/image-20220410115644377.png)

## 数值稳定性和模型初始化

![image-20220410123144883](动手学深度学习总结/image-20220410123144883.png)

# 深度学习计算

## 模型构造

****

![image-20220410173714617](动手学深度学习总结/image-20220410173714617.png)

<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">class FancyMLP(nn<span class="selector-class">.Block</span>):</span><br><span class="line">    def <span class="built_in">__init__</span>(self,**kwargs):</span><br><span class="line">        <span class="built_in">super</span>(FancyMLP,self).<span class="built_in">__init__</span>(**kwargs)</span><br><span class="line">        self.rand_weight=self.params.<span class="built_in">get_constant</span>(<span class="string">&#x27;rand_weight&#x27;</span>,nd.random.<span class="built_in">uniform</span>(shape=(<span class="number">20</span>,<span class="number">20</span>)))</span><br><span class="line">        self.dense=nn.<span class="built_in">Dense</span>(<span class="number">20</span>,activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    def <span class="built_in">forward</span>(self,x):</span><br><span class="line">    # 使⽤创建的常数参数，以及NDArray的relu函数和dot函数</span><br><span class="line">        x=self.<span class="built_in">dense</span>(x)</span><br><span class="line">    # 复⽤全连接层。等价于两个全连接层共享参数</span><br><span class="line">        x=nd.<span class="built_in">relu</span>(nd.<span class="built_in">dot</span>(x,self.rand_weight.<span class="built_in">data</span>())+<span class="number">1</span>)</span><br><span class="line">    # 控制流，这⾥我们需要调⽤asscalar函数来返回标量进⾏⽐较</span><br><span class="line">        x=self.<span class="built_in">dense</span>(x)</span><br><span class="line">        x</span><br><span class="line">        while x.<span class="built_in">norm</span>().<span class="built_in">asscalar</span>()&gt;<span class="number">1</span>:</span><br><span class="line">            x/=<span class="number">2</span></span><br><span class="line">        if x.<span class="built_in">norm</span>().<span class="built_in">asscalar</span>()&lt;<span class="number">0.8</span>:</span><br><span class="line">            x*=<span class="number">10</span></span><br><span class="line">        return x.<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">net=<span class="built_in">FancyMLP</span>()</span><br><span class="line">net.<span class="built_in">initialize</span>()</span><br><span class="line"><span class="built_in">net</span>(X)</span><br></pre></td></tr></table></figure>

其中神经模型为以下过程

![构造复杂的模型](动手学深度学习总结/构造复杂的模型.png)

其中前后两个隐藏单元为同一层，共用同一参数ReLu(X)

## 模型参数的访问、初始化和共享

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet import init,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line"><span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(256,<span class="attribute">activation</span>=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(10))</span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line"><span class="attribute">X</span>=nd.random.uniform(shape=(2,20))</span><br><span class="line"><span class="attribute">Y</span>=net(X)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight crmsh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#访问第一层的参数</span></span><br><span class="line">net[<span class="number">0</span>].<span class="keyword">params</span>,<span class="keyword">type</span>(net[<span class="number">0</span>].<span class="keyword">params</span>)</span><br></pre></td></tr></table></figure>

![image-20220410203825121](动手学深度学习总结/image-20220410203825121.png)

<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#访问第一层权重的数据</span></span><br><span class="line"><span class="title">net</span>[<span class="number">0</span>].weight.<span class="class"><span class="keyword">data</span>()</span></span><br></pre></td></tr></table></figure>

![image-20220410203912239](动手学深度学习总结/image-20220410203912239.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#第一层权重的梯度</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.grad</span>()</span><br></pre></td></tr></table></figure>

![image-20220410203953273](动手学深度学习总结/image-20220410203953273.png)

<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#第二层权重数据</span></span><br><span class="line"><span class="title">net</span>[<span class="number">1</span>].weight.<span class="class"><span class="keyword">data</span>()</span></span><br></pre></td></tr></table></figure>

![image-20220410204035268](动手学深度学习总结/image-20220410204035268.png)

<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#神经网络中所有参数</span></span><br><span class="line">net.collect_params()</span><br></pre></td></tr></table></figure>

![image-20220410204113149](动手学深度学习总结/image-20220410204113149.png)

<figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#神经网络中所有权重(不含偏差)</span></span><br><span class="line">net.collect_params(&#x27;.*weight&#x27;)</span><br></pre></td></tr></table></figure>

![image-20220410204153597](动手学深度学习总结/image-20220410204153597.png)

<figure class="highlight dos"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">net</span>.collect_params()</span><br></pre></td></tr></table></figure>

![image-20220410221835680](动手学深度学习总结/image-20220410221835680.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#模型的默认初始化⽅法：权重参数元素为<span class="selector-attr">[-0.07, 0.07]</span>之间均匀分布的随机数，偏差参数则全为<span class="number">0</span>。但我们经常需#要使⽤其他⽅法来初始化权重。 MXNet的init模块⾥提供了多种预设的初始化⽅法。在下⾯的例⼦中，我们将权重</span><br><span class="line">#参数初始化成均值为<span class="number">0</span>、标准差为<span class="number">0.01</span>的正态分布随机数，并依然将偏差参数清零。</span><br><span class="line">net<span class="selector-class">.initialize</span>(init=init<span class="selector-class">.Normal</span>(sigma=<span class="number">0.01</span>),force_reinit=True)</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>()<span class="selector-attr">[0]</span></span><br></pre></td></tr></table></figure>

![image-20220410221400528](动手学深度学习总结/image-20220410221400528.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">net<span class="selector-class">.initialize</span>(init=init<span class="selector-class">.Normal</span>(sigma=<span class="number">0.01</span>),force_reinit=True)</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>()</span><br></pre></td></tr></table></figure>

![image-20220410221926023](动手学深度学习总结/image-20220410221926023.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#使用常数来初始化权重参数</span><br><span class="line">net<span class="selector-class">.initialize</span>(init=init<span class="selector-class">.Constant</span>(<span class="number">1</span>),force_reinit=True)</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>()</span><br></pre></td></tr></table></figure>

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#我们对隐藏层的权重使用Xavier随机初始化方法</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.initialize</span>(init=init<span class="selector-class">.Xavier</span>(),force_reinit=True)</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>()<span class="selector-attr">[0]</span></span><br></pre></td></tr></table></figure>

![image-20220410222224319](动手学深度学习总结/image-20220410222224319.png)

![image-20220410222411304](动手学深度学习总结/image-20220410222411304.png)

## 模型参数的延后初始化

![image-20220410223649237](动手学深度学习总结/image-20220410223649237.png)

## 自定义层

![image-20220411113908180](动手学深度学习总结/image-20220411113908180.png)

# 卷积神经网络

## 二维卷积层

![image-20220411162009820](动手学深度学习总结/image-20220411162009820.png)

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">from</span> mxnet import autograd,nd</span><br><span class="line"><span class="attribute">from</span> mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入与核矩阵相乘</span></span><br><span class="line"><span class="attribute">def</span> corr2d(X,K):</span><br><span class="line">    <span class="attribute">h</span>,w=K.shape</span><br><span class="line">    <span class="attribute">Y</span>=nd.zeros((X.shape[<span class="number">0</span>]-h+<span class="number">1</span>,X.shape[<span class="number">1</span>]-w+<span class="number">1</span>))</span><br><span class="line">    <span class="attribute">for</span> i in range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="attribute">for</span> j in range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="attribute">Y</span>[i,j]=(X[i:i+h,j:j+w]*K).sum()</span><br><span class="line">    <span class="attribute">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="attribute">X</span>=nd.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line"><span class="attribute">K</span>=nd.array([[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="attribute">corr2d</span>(X,K)</span><br></pre></td></tr></table></figure>

![image-20220411162449392](动手学深度学习总结/image-20220411162449392.png)

<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加上偏差</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2</span>D(nn.Block):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"><span class="variable language_">self</span>,kernel_size,**kwargs</span>):</span><br><span class="line">        <span class="variable language_">super</span>(Conv2D,<span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.weight=<span class="variable language_">self</span>.params.get(<span class="string">&#x27;weight&#x27;</span>,shape=kernel_size)</span><br><span class="line">        <span class="variable language_">self</span>.bias=<span class="variable language_">self</span>.params_get(<span class="string">&#x27;bias&#x27;</span>,shape=(<span class="number">1</span>,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"><span class="variable language_">self</span>,x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x,<span class="variable language_">self</span>.weight.data()+<span class="variable language_">self</span>.bias.data())</span><br></pre></td></tr></table></figure>

![image-20220411162714775](动手学深度学习总结/image-20220411162714775.png)

### 通过数据学习核数组

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构造一个输出通道数为1，核数组形状是(1,2)的二维卷积层</span></span><br><span class="line"><span class="attribute">conv2d</span>=nn.Conv2D(<span class="number">1</span>,kernel_size=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"><span class="attribute">conv2d</span>.initialize()</span><br><span class="line"></span><br><span class="line"><span class="comment">#二维卷积层使用4维输入输出，格式为(样本，通道，高，宽)，这里批量大小(批量中的样本数)和通道数均为1</span></span><br><span class="line"><span class="attribute">X</span>=X.reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">6</span>,<span class="number">8</span>))</span><br><span class="line"><span class="attribute">Y</span>=Y.reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">6</span>,<span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="attribute">for</span> i in range(<span class="number">10</span>):</span><br><span class="line">    <span class="attribute">with</span> autograd.record():</span><br><span class="line">        <span class="attribute">Y_hat</span>=conv2d(X)</span><br><span class="line">        <span class="attribute">l</span>=(Y_hat-Y)**<span class="number">2</span></span><br><span class="line">    <span class="attribute">l</span>.backward()</span><br><span class="line">    <span class="attribute">conv2d</span>.weight.data()[:]-=<span class="number">3</span>e-<span class="number">2</span>*conv2d.weight.grad()</span><br><span class="line">    <span class="attribute">if</span> (i+<span class="number">1</span>)%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="attribute">print</span>(&#x27;batch %d, loss %.<span class="number">3</span>f&#x27; %(i+<span class="number">1</span>,l.sum().asscalar()))</span><br></pre></td></tr></table></figure>

![image-20220411163718012](动手学深度学习总结/image-20220411163718012.png)

<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="title">conv2d</span>.weight.<span class="class"><span class="keyword">data</span>().reshape((1,2))</span></span><br></pre></td></tr></table></figure>

![image-20220411163737479](动手学深度学习总结/image-20220411163737479.png)

可以看出参数与图像中物体边缘检测中我们设置的参数[1,-1]接近，所以我们对核进行了参数迭代。

![通过数据学习核数组](动手学深度学习总结/通过数据学习核数组.png)

## 填充与步幅

![image-20220411173136493](动手学深度学习总结/image-20220411173136493.png)

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet import<span class="built_in"> nd</span></span><br><span class="line"><span class="built_in"></span><span class="keyword">from</span> mxnet.gluon import nn</span><br><span class="line"><span class="comment">#定义一个函数来计算卷积层。它初始化卷积层权重，并对输入和输出做相应的升维与降维</span></span><br><span class="line">def comp_conv2d(conv2d,X):</span><br><span class="line">    conv2d.initialize()</span><br><span class="line">    #(1,1)代表批量大小和通道数均为1</span><br><span class="line">    <span class="attribute">X</span>=X.reshape((1,1)+X.shape)</span><br><span class="line">    <span class="attribute">Y</span>=conv2d(X)</span><br><span class="line">    return Y.reshape(Y.shape[2:])#排除不关心的前两维：批量与通道</span><br><span class="line"><span class="comment">#这里是两侧分别填充1行或列，所以在两侧一共填充2行与列</span></span><br><span class="line"><span class="attribute">conv2d</span>=nn.Conv2D(1,kernel_size=3,padding=1)</span><br><span class="line">conv2d</span><br><span class="line"></span><br><span class="line"><span class="attribute">X</span>=nd.random.uniform(shape=(8,8))</span><br><span class="line">comp_conv2d(conv2d,X).shape</span><br></pre></td></tr></table></figure>

![填充与步幅](动手学深度学习总结/填充与步幅.png)

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用高为5，宽为3的卷积核，在高和宽两侧的填充数分别为2和1</span></span><br><span class="line"><span class="attribute">conv2d</span>=nn.Conv2D(<span class="number">1</span>,kernel_size=(<span class="number">5</span>,<span class="number">3</span>),padding=(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line"><span class="attribute">comp_conv2d</span>(conv2d,X).shape</span><br></pre></td></tr></table></figure>

![image-20220411174831149](动手学深度学习总结/image-20220411174831149.png)

![image-20220411174859742](动手学深度学习总结/image-20220411174859742.png)

## 多输入通道和多输出通道

### 多输入通道

![image-20220411180109484](动手学深度学习总结/image-20220411180109484.png)

![image-20220411180155152](动手学深度学习总结/image-20220411180155152.png)

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">import</span> d2lzh as d2l</span><br><span class="line"><span class="attribute">from</span> mxnet import nd </span><br><span class="line"></span><br><span class="line"><span class="attribute">def</span> corr2d_multi_in(X,K):</span><br><span class="line">    <span class="attribute">return</span> nd.add_n(*[d2l.corr2d(X,K) for X,K in zip(X,K)])</span><br><span class="line">    </span><br><span class="line"><span class="attribute">X</span>=nd.array([[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]],[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]]])</span><br><span class="line"><span class="attribute">K</span>=nd.array([[[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]],[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="attribute">corr2d_multi_in</span>(X,K)</span><br></pre></td></tr></table></figure>

通过在corr2d_multi_in函数里加*使得两个矩阵相加

![image-20220411180307603](动手学深度学习总结/image-20220411180307603.png)

### 多输出通道

![image-20220411180944244](动手学深度学习总结/image-20220411180944244.png)

<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X,K</span>):</span><br><span class="line">	<span class="comment">#对K的第0维遍历，每次同输入X做互相关计算，所有结果使用stack函数合并在一起</span></span><br><span class="line">    <span class="keyword">return</span> nd.stack(*[corr2d_multi_in(X,k) <span class="keyword">for</span> k <span class="keyword">in</span> K])</span><br></pre></td></tr></table></figure>

<figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">#k</span><span class="operator">+</span><span class="number">1</span>等于<span class="built_in">K</span>中的每个元素加一</span><br><span class="line"><span class="built_in">K</span><span class="operator">=</span><span class="variable">nd</span><span class="operator">.</span><span class="variable">stack</span><span class="punctuation">(</span><span class="built_in">K</span><span class="operator">,</span><span class="built_in">K</span><span class="operator">+</span><span class="number">1</span><span class="operator">,</span><span class="built_in">K</span><span class="operator">+</span><span class="number">2</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">K</span><span class="operator">.</span><span class="variable">shape</span></span><br></pre></td></tr></table></figure>

![image-20220411181123216](动手学深度学习总结/image-20220411181123216.png)

3表示样本数，2表示通道数，2表示高，2表示宽

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">corr2d_multi_in_out</span><span class="params">(X,K)</span></span></span><br></pre></td></tr></table></figure>

 ![image-20220411181233717](动手学深度学习总结/image-20220411181233717.png)

## 1x1卷积层

![image-20220411203515169](动手学深度学习总结/image-20220411203515169.png)

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">def corr2d_multi_in_out_1x1(X,K):</span><br><span class="line">	#<span class="attribute">c_i</span>=3表三个输入,h高度,w宽度</span><br><span class="line">    c_i,h,<span class="attribute">w</span>=X.shape</span><br><span class="line">    #输出个数</span><br><span class="line">    <span class="attribute">c_o</span>=K.shape[0]</span><br><span class="line">    #3<span class="number">*9</span></span><br><span class="line">    <span class="attribute">X</span>=X.reshape((c_i,h*w))</span><br><span class="line">    #2<span class="number">*3</span></span><br><span class="line">    <span class="attribute">K</span>=K.reshape((c_o,c_i))</span><br><span class="line">    <span class="attribute">Y</span>=nd.dot(K,X)</span><br><span class="line">    return Y.reshape((c_o,h,w))</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#输入3*3*3</span></span><br><span class="line"><span class="attribute">X</span>=nd.random.uniform(shape=(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#核通道2，个数3，高和宽1*1</span></span><br><span class="line"><span class="attribute">K</span>=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

![image-20220411203933318](动手学深度学习总结/image-20220411203933318.png)

<figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">Y</span><span class="number">1</span>=corr2d_multi_in_out_1x<span class="number">1</span>(<span class="keyword">X</span>,K)</span><br><span class="line"><span class="keyword">Y</span><span class="number">2</span>=corr2d_multi_in_out(<span class="keyword">X</span>,K)</span><br><span class="line">(<span class="keyword">Y</span><span class="number">1</span>-<span class="keyword">Y</span><span class="number">2</span>).norm().asscalar()&lt;<span class="number">1e-6</span></span><br></pre></td></tr></table></figure>

![image-20220411204101977](动手学深度学习总结/image-20220411204101977.png)

## 池化层

![image-20220411213253633](动手学深度学习总结/image-20220411213253633.png)

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">from</span> mxnet import nd</span><br><span class="line"><span class="attribute">from</span> mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line"><span class="attribute">def</span> pool2d(X,pool_size,mode=&#x27;max&#x27;):</span><br><span class="line">    <span class="attribute">p_h</span>,p_w=pool_size</span><br><span class="line">    <span class="attribute">Y</span>=nd.zeros((X.shape[<span class="number">0</span>]-p_h+<span class="number">1</span>,X.shape[<span class="number">1</span>]-p_w+<span class="number">1</span>))</span><br><span class="line">    <span class="attribute">for</span> i in range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="attribute">for</span> j in range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="attribute">if</span> mode==&#x27;max&#x27;:</span><br><span class="line">                <span class="attribute">Y</span>[i,j]=X[i:i+p_h,j:j+p_w].max()</span><br><span class="line">            <span class="attribute">elif</span> mode==&#x27;avg&#x27;:</span><br><span class="line">                <span class="attribute">Y</span>[i,j]=X[i:i+p_h,j:j+p_w].mean()</span><br><span class="line">    <span class="attribute">return</span> Y</span><br><span class="line">    </span><br><span class="line"><span class="attribute">X</span>=nd.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line"><span class="attribute">pool2d</span>(X,(<span class="number">2</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

![image-20220411211219896](动手学深度学习总结/image-20220411211219896.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">pool2d</span><span class="params">(X,(<span class="number">2</span>,<span class="number">2</span>)</span></span>,<span class="string">&#x27;avg&#x27;</span>)</span><br></pre></td></tr></table></figure>

![image-20220411211238878](动手学深度学习总结/image-20220411211238878.png)

![image-20220411213329939](动手学深度学习总结/image-20220411213329939.png)

## 卷积神经网络(LeNet)

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">import d2lzh <span class="keyword">as</span> d2l</span><br><span class="line">import mxnet <span class="keyword">as</span> mx</span><br><span class="line">from mxnet import autograd,gluon,init,nd</span><br><span class="line">from mxnet.gluon import loss <span class="keyword">as</span> gloss,nn</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">net=nn.<span class="constructor">Sequential()</span></span><br><span class="line"># channels表过滤器的数量</span><br><span class="line">net.add(nn.<span class="constructor">Conv2D(<span class="params">channels</span>=6,<span class="params">kernel_size</span>=5,<span class="params">activation</span>=&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Conv2D(<span class="params">channels</span>=16,<span class="params">kernel_size</span>=5,<span class="params">activation</span>=&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">        # Dense会默认将(批量⼤⼩, 通道, ⾼, 宽)形状的输⼊转换成</span><br><span class="line">		# (批量⼤⼩, 通道<span class="operator"> * </span>⾼<span class="operator"> * </span>宽)形状的输⼊</span><br><span class="line">        nn.<span class="constructor">Dense(120,<span class="params">activation</span>=&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(84,<span class="params">activation</span>=&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(10)</span>)</span><br><span class="line"></span><br><span class="line">X=nd.random.uniform(shape=(<span class="number">1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">net.initialize<span class="literal">()</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X=layer(X)</span><br><span class="line">    print(layer.name,&#x27;output shape:\t&#x27;,<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>shape)</span><br></pre></td></tr></table></figure>

![image-20220412105434810](动手学深度学习总结/image-20220412105434810.png)

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">batch_size</span>=256</span><br><span class="line">train_iter,<span class="attribute">test_iter</span>=d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line"><span class="comment">#尝试使用gpu</span></span><br><span class="line">def try_gpu():</span><br><span class="line">    try:</span><br><span class="line">        <span class="attribute">ctx</span>=mx.gpu()</span><br><span class="line">        <span class="attribute">_</span>=nd.zeros((1,),ctx=ctx)</span><br><span class="line">    except mx.base.MXNetError:</span><br><span class="line">        <span class="attribute">ctx</span>=mx.cpu()</span><br><span class="line">    return ctx</span><br><span class="line"><span class="attribute">ctx</span>=try_gpu()</span><br><span class="line">ctx</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这个函数是通过样本X，经过卷积神经网络来得到生成的y_hat是否与y相同</span></span><br><span class="line">def evaluate_accuracy(data_iter,net,ctx):</span><br><span class="line">    acc_sum,<span class="attribute">n</span>=nd.array([0],<span class="attribute">ctx</span>=ctx),0</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        X,<span class="attribute">y</span>=X.as_in_context(ctx),y.as_in_context(ctx).astype(&#x27;float32&#x27;)</span><br><span class="line">        #net(X).argmax(<span class="attribute">axis</span>=1)表示返回矩阵中每行最大元素的索引</span><br><span class="line">        #相等差别式(y_hat.argmax(<span class="attribute">axis</span>=1)==y)是一个值为0(相等为假)或1(相等为真)的NDArray</span><br><span class="line">        acc_sum+=(net(X).argmax(<span class="attribute">axis</span>=1)==y).sum()</span><br><span class="line">        n+=y.size</span><br><span class="line">    return acc_sum.asscalar()/n</span><br></pre></td></tr></table></figure>

<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">def train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>,ctx)</span><br><span class="line">    loss=gloss.<span class="built_in">SoftmaxCrossEntropyLoss</span>()</span><br><span class="line">    for epoch in <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum,train_acc_sum,n,start=<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0</span>,time.<span class="built_in">time</span>()</span><br><span class="line">        for X,y in train_iter:</span><br><span class="line">            X,y=X.<span class="built_in">as_in_context</span>(ctx),y.<span class="built_in">as_in_context</span>(ctx)</span><br><span class="line">            with autograd.<span class="built_in">record</span>():</span><br><span class="line">                y_hat=<span class="built_in">net</span>(X)</span><br><span class="line">                l=<span class="built_in">loss</span>(y_hat,y).<span class="built_in">sum</span>()</span><br><span class="line">            l.<span class="built_in">backward</span>()</span><br><span class="line">            trainer.<span class="built_in">step</span>(batch_size)</span><br><span class="line">            y=y.<span class="built_in">astype</span>(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            train_l_sum+=l.<span class="built_in">asscalar</span>()</span><br><span class="line">            train_acc_sum+=(y_hat.<span class="built_in">argmax</span>(axis=<span class="number">1</span>)==y).<span class="built_in">sum</span>().<span class="built_in">asscalar</span>()</span><br><span class="line">            n+=y.size</span><br><span class="line">        test_acc=<span class="built_in">evaluate_accuracy</span>(test_iter,net,ctx)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d,loss %.4f, train acc %.3f,test acc %.3f,time %.1f sec&#x27;</span> %(epoch+<span class="number">1</span>,train_l_sum/n,train_acc_sum/n,test_acc,time.<span class="built_in">time</span>()-start))</span><br></pre></td></tr></table></figure>

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">lr,num_epochs=<span class="number">0.9</span>,<span class="number">5</span></span><br><span class="line">net.initialize(force_reinit=True,ctx=ctx,init=init.<span class="constructor">Xavier()</span>)</span><br><span class="line">trainer=gluon.<span class="constructor">Trainer(<span class="params">net</span>.<span class="params">collect_params</span>()</span>,&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr&#125;)</span><br><span class="line">train<span class="constructor">_ch5(<span class="params">net</span>,<span class="params">train_iter</span>,<span class="params">test_iter</span>,<span class="params">batch_size</span>,<span class="params">trainer</span>,<span class="params">ctx</span>,<span class="params">num_epochs</span>)</span></span><br></pre></td></tr></table></figure>

![image-20220412110156595](动手学深度学习总结/image-20220412110156595.png)

## 批量归一化

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br></pre></td><td class="code"><pre><span class="line">import d2lzh <span class="keyword">as</span> d2l</span><br><span class="line">from mxnet import autograd,gluon,init,nd</span><br><span class="line">from mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line">def batch<span class="constructor">_norm(X,<span class="params">gamma</span>,<span class="params">beta</span>,<span class="params">moving_mean</span>,<span class="params">moving_var</span>,<span class="params">eps</span>,<span class="params">momentum</span>)</span>:</span><br><span class="line">		#通过autograd来判断当前模式是训练模式还是预测模式</span><br><span class="line">    <span class="keyword">if</span> not autograd.is<span class="constructor">_training()</span>:</span><br><span class="line">    	#如果是在预测模式下，直接使用传入的移动平均所得的均值与方差</span><br><span class="line">        X_hat=(X-moving_mean)/nd.sqrt(moving_var+eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> len(<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>shape) <span class="keyword">in</span> (<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">        #使用全连接层的情况，计算特征维上的均值与方差</span><br><span class="line">        <span class="keyword">if</span> len(<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>shape)==<span class="number">2</span>:</span><br><span class="line">            mean=<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>mean(axis=<span class="number">0</span>)</span><br><span class="line">            var=((X-mean)**<span class="number">2</span>).mean(axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        #使用二维卷积层的情况，计算通道维上(axis=<span class="number">1</span>)的均值与方差，这里我们需要保持</span><br><span class="line">        #X的开关以便后面可以做广播运算</span><br><span class="line">            mean=<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdims=True)</span><br><span class="line">            var=((X-mean)**<span class="number">2</span>).mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdims=True)</span><br><span class="line">        #训练模式下用当前的均值和方差做标准化</span><br><span class="line">        X_hat=(X-mean)/nd.sqrt(var+eps)</span><br><span class="line">        #更新移动平均的均值与方差</span><br><span class="line">        moving_mean=momentum*moving_mean+(<span class="number">1.0</span>-momentum)*mean</span><br><span class="line">        moving_var=momentum*moving_var+(<span class="number">1.0</span>-momentum)*var</span><br><span class="line">    Y=gamma*X_hat+beta</span><br><span class="line">    return Y,moving_mean,moving_var</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="constructor">BatchNorm(<span class="params">nn</span>.Block)</span>:</span><br><span class="line">    def <span class="constructor">__init__(<span class="params">self</span>,<span class="params">num_features</span>,<span class="params">num_dims</span>,<span class="operator">**</span><span class="params">kwargs</span>)</span>:</span><br><span class="line">        super(BatchNorm,self).<span class="constructor">__init__(<span class="operator">**</span><span class="params">kwargs</span>)</span></span><br><span class="line">        <span class="keyword">if</span> num_dims==<span class="number">2</span>:</span><br><span class="line">            shape=(<span class="number">1</span>,num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape=(<span class="number">1</span>,num_features,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        #参与求梯度和迭代的拉伸和偏移参数，分别初始化为<span class="number">0</span>和<span class="number">1</span></span><br><span class="line">        self.gamma=self.params.get(&#x27;gamma&#x27;,shape=shape,init=init.<span class="constructor">One()</span>)</span><br><span class="line">        self.beta=self.params.get(&#x27;beta&#x27;,shape=shape,init=init.<span class="constructor">Zero()</span>)</span><br><span class="line">        #不参与求梯度和迭代的变量，全在内存上初始化成<span class="number">0</span></span><br><span class="line">        self.moving_mean=nd.zeros(shape)</span><br><span class="line">        self.moving_var=nd.zeros(shape)</span><br><span class="line">        </span><br><span class="line">    def forward(self,X):</span><br><span class="line">    	#如果X不在内存上，将moving_mean和moving_var复制到X所在的显存上</span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.context!=<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>context:</span><br><span class="line">            self.moving_mean=self.moving_mean.copyto(<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>context)</span><br><span class="line">            self.moving_var=self.moving_var.copyto(<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>context)</span><br><span class="line">        #保存更新过的moving_mean和moving_var</span><br><span class="line">        Y,self.moving_mean,self.moving_var=batch<span class="constructor">_norm(X,<span class="params">self</span>.<span class="params">gamma</span>.<span class="params">data</span>()</span>,</span><br><span class="line">        self.beta.data<span class="literal">()</span>,self.moving_mean,self.moving_var,eps=<span class="number">1e-5</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line">        return Y</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">net=nn.<span class="constructor">Sequential()</span></span><br><span class="line">net.add(nn.<span class="constructor">Conv2D(6,<span class="params">kernel_size</span>=5)</span>,</span><br><span class="line">        <span class="constructor">BatchNorm(6,<span class="params">num_dims</span>=4)</span>,</span><br><span class="line">        nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Conv2D(16,<span class="params">kernel_size</span>=4)</span>,</span><br><span class="line">        <span class="constructor">BatchNorm(16,<span class="params">num_dims</span>=4)</span>,</span><br><span class="line">        nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(120)</span>,</span><br><span class="line">        <span class="constructor">BatchNorm(120,<span class="params">num_dims</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(84)</span>,</span><br><span class="line">        <span class="constructor">BatchNorm(84,<span class="params">num_dims</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(10)</span>)</span><br><span class="line"> </span><br><span class="line">lr,num_epochs,batch_size,ctx=<span class="number">1.0</span>,<span class="number">5</span>,<span class="number">256</span>,d2l.<span class="keyword">try</span><span class="constructor">_gpu()</span></span><br><span class="line">net.initialize(ctx=ctx,init=init.<span class="constructor">Xavier()</span>)</span><br><span class="line">trainer=gluon.<span class="constructor">Trainer(<span class="params">net</span>.<span class="params">collect_params</span>()</span>,&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr&#125;)</span><br><span class="line">train_iter,test_iter=d2l.load<span class="constructor">_data_fashion_mnist(<span class="params">batch_size</span>)</span></span><br><span class="line">d2l.train<span class="constructor">_ch5(<span class="params">net</span>,<span class="params">train_iter</span>,<span class="params">test_iter</span>,<span class="params">batch_size</span>,<span class="params">trainer</span>,<span class="params">ctx</span>,<span class="params">num_epochs</span>)</span></span><br></pre></td></tr></table></figure>

![image-20220412162432934](动手学深度学习总结/image-20220412162432934.png)

<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#查看第一批量归化一层学习到的拉伸参数gamma和偏移参数beta</span></span><br><span class="line"><span class="title">net</span>[<span class="number">1</span>].gamma.<span class="class"><span class="keyword">data</span>().reshape((-1,)),net[1].beta.<span class="keyword">data</span>().reshape((-1,))</span></span><br></pre></td></tr></table></figure>

![image-20220412162614282](动手学深度学习总结/image-20220412162614282.png)

## 批量归一化的简洁实现

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">    #使用gluon的批量归一化</span><br><span class="line">    net=nn.<span class="constructor">Sequential()</span></span><br><span class="line">    net.add(nn.<span class="constructor">Conv2D(6,<span class="params">kernel_size</span>=5)</span>,</span><br><span class="line">    nn.<span class="constructor">BatchNorm()</span>,</span><br><span class="line">    nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">    nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">    nn.<span class="constructor">Conv2D(16,<span class="params">kernel_size</span>=5)</span>,</span><br><span class="line">    nn.<span class="constructor">BatchNorm()</span>,</span><br><span class="line">     nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">    nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">    nn.<span class="constructor">Dense(120)</span>,</span><br><span class="line">    nn.<span class="constructor">BatchNorm()</span>,</span><br><span class="line">     nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">    nn.<span class="constructor">Dense(84)</span>,</span><br><span class="line">    nn.<span class="constructor">BatchNorm()</span>,</span><br><span class="line">      nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">    nn.<span class="constructor">Dense(10)</span></span><br><span class="line">    )</span><br><span class="line"> net.initialize(ctx=ctx,init=init.<span class="constructor">Xavier()</span>)</span><br><span class="line">trainer=gluon.<span class="constructor">Trainer(<span class="params">net</span>.<span class="params">collect_params</span>()</span>,&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr&#125;)</span><br><span class="line">d2l.train<span class="constructor">_ch5(<span class="params">net</span>,<span class="params">train_iter</span>,<span class="params">test_iter</span>,<span class="params">batch_size</span>,<span class="params">trainer</span>,<span class="params">ctx</span>,<span class="params">num_epochs</span>)</span></span><br></pre></td></tr></table></figure>

![image-20220412165145598](动手学深度学习总结/image-20220412165145598.png)

## 残差网络

![image-20220412180722522](动手学深度学习总结/image-20220412180722522.png)

<figure class="highlight haskell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="title">from</span> mxnet <span class="keyword">import</span> gluon,init,nd</span><br><span class="line"><span class="title">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Residual</span>(<span class="title">nn</span>.<span class="type">Block</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>,<span class="title">num_channels</span>,<span class="title">use_1x1conv</span>=<span class="type">False</span>,<span class="title">strides</span>=1,**<span class="title">kwargs</span>):</span></span><br><span class="line"><span class="class">        super(<span class="type">Residual</span>,<span class="title">self</span>).__init__(**<span class="title">kwargs</span>)</span></span><br><span class="line"><span class="class">        self.conv1=nn.<span class="type">Conv2D</span>(<span class="title">num_channels</span>,<span class="title">kernel_size</span>=3,<span class="title">padding</span>=1,<span class="title">strides</span>=<span class="title">strides</span>)</span></span><br><span class="line"><span class="class">        self.conv2=nn.<span class="type">Conv2D</span>(<span class="title">num_channels</span>,<span class="title">kernel_size</span>=3,<span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        </span></span><br><span class="line"><span class="class">        if use_1x1conv:</span></span><br><span class="line"><span class="class">            self.conv3=nn.<span class="type">Conv2D</span>(<span class="title">num_channels</span>,<span class="title">kernel_size</span>=1,<span class="title">strides</span>=<span class="title">strides</span>)</span></span><br><span class="line"><span class="class">        else:</span></span><br><span class="line"><span class="class">            self.conv3=<span class="type">None</span></span></span><br><span class="line"><span class="class">        self.bn1=nn.<span class="type">BatchNorm</span>()</span></span><br><span class="line"><span class="class">        self.bn2=nn.<span class="type">BatchNorm</span>()</span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>,<span class="type">X</span>):</span></span><br><span class="line"><span class="class">        <span class="type">Y</span>=nd.relu(<span class="title">self</span>.<span class="title">bn1</span>(<span class="title">self</span>.<span class="title">conv1</span>(<span class="type">X</span>)))</span></span><br><span class="line"><span class="class">        <span class="type">Y</span>=self.bn2(<span class="title">self</span>.<span class="title">conv2</span>(<span class="type">Y</span>))</span></span><br><span class="line"><span class="class">        if self.conv3:</span></span><br><span class="line"><span class="class">            <span class="type">X</span>=self.conv3(<span class="type">X</span>)</span></span><br><span class="line"><span class="class">        return nd.relu(<span class="type">Y</span>+<span class="type">X</span>)</span></span><br></pre></td></tr></table></figure>

可以通过调用跳过第三层卷积层。

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">blk=<span class="built_in">Residual</span>(<span class="number">3</span>)</span><br><span class="line">blk<span class="selector-class">.initialize</span>()</span><br><span class="line">X=nd<span class="selector-class">.random</span><span class="selector-class">.uniform</span>(shape=(<span class="number">4</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line"><span class="function"><span class="title">blk</span><span class="params">(X)</span></span>.shape</span><br></pre></td></tr></table></figure>

![image-20220412180853217](动手学深度学习总结/image-20220412180853217.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">blk=<span class="built_in">Residual</span>(<span class="number">6</span>,use_1x1conv=True,strides=<span class="number">2</span>)</span><br><span class="line">blk<span class="selector-class">.initialize</span>()</span><br><span class="line"><span class="function"><span class="title">blk</span><span class="params">(X)</span></span>.shape</span><br></pre></td></tr></table></figure>

![image-20220412180949437](动手学深度学习总结/image-20220412180949437.png)

### ResNet模型

![image-20220412181057419](动手学深度学习总结/image-20220412181057419.png)

![image-20220412190440555](动手学深度学习总结/image-20220412190440555.png)

# 循环神经网络

假设有一段长度为T的文本中的词依次为$$w_1,w_2,...,w_t$$，那么在离散的时间序列中，$$w_t(1\leq t\leq T)$$可看作在时间步T的输出或标签，即有语言模型</script><p>P(w_1,w_2,…,w_t)</p>
<script type="math/tex; mode=display">
假设序列$$w_1,w_2,...,w_t$$中的每个词依次生成的，我们有</script><p>P(w_1,w_2,…,w_t)=\prod_{t=1}^Tp(w_t|w_1,…,w_{t-1})</p>
<script type="math/tex; mode=display">
其中$$p(w_2|w_1)$$可以计算为$$w_1$$和$$w_2$$两词相邻的频率与$$w_1$$词频的比值，因为该比值即$$p(w_1,w_2)$$与$$p(w_1)$$之比

![image-20220415124314823](动手学深度学习总结/image-20220415124314823.png)

对这部分我们有公式

![image-20220415124353842](动手学深度学习总结/image-20220415124353842.png)

$$H_{t-1}$$为上一层的隐藏状态，所以这表示该层的隐藏状态与上一层的隐藏状态有关，所以会导致上一层的隐藏状态会影响这一层的隐藏状态，而输出层又为

![image-20220415124442889](动手学深度学习总结/image-20220415124442889.png)



## 语言模型数据集

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random </span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">&#x27;data/jaychou_lyrics.txt.zip&#x27;</span>) <span class="keyword">as</span> zin:</span><br><span class="line">    <span class="keyword">with</span> zin.<span class="built_in">open</span>(<span class="string">&#x27;jaychou_lyrics.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        corpus_chars=f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">corpus_chars[:<span class="number">40</span>] <span class="comment">#读取前40个字符 </span></span><br></pre></td></tr></table></figure>

![image-20220414214551904](动手学深度学习总结/image-20220414214551904.png)

<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#把换行符替换成空格，使用前1万个字符来训练模型</span></span><br><span class="line"><span class="attr">corpus_chars</span>=corpus_chars.replace(<span class="string">&#x27;\n&#x27;</span>,<span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\r&#x27;</span>,<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="attr">corpus_chars</span>=corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#建立字符索引，将字符映射成数字</span></span><br><span class="line"><span class="attribute">idx_to_char</span>=list(set(corpus_chars))</span><br><span class="line"><span class="comment">#字符组映射成数字组</span></span><br><span class="line"><span class="attribute">char_to_idx</span>=dict([(char,i)<span class="keyword">for</span> i,char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line"><span class="attribute">vocab_size</span>=len(char_to_idx)</span><br><span class="line">vocab_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#分别打印出字符组与所对应的数字组</span></span><br><span class="line">corpus_indices=[char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line"><span class="attribute">sample</span>=corpus_indices[:20]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;chars:&#x27;</span>,<span class="string">&#x27;&#x27;</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;indices:&#x27;</span>,sample)</span><br></pre></td></tr></table></figure>

![image-20220414215208400](动手学深度学习总结/image-20220414215208400.png)

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机采样 </span></span><br><span class="line"><span class="comment">#batch_size指每个小批量的样本数，num_steps为每个样本包含的时间步数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter_random</span>(<span class="params">corpus_indices,batch_size,num_steps,ctx=<span class="literal">None</span></span>):</span><br><span class="line">	<span class="comment">#减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    num_examples=(<span class="built_in">len</span>(corpus_indices)-<span class="number">1</span>)//num_steps</span><br><span class="line">    <span class="comment">#迭代数=样本数/批量大小</span></span><br><span class="line">    epoch_size=num_examples//batch_size</span><br><span class="line">    example_indices=<span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line">    <span class="comment">#返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos:pos+num_steps]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch_size):</span><br><span class="line">        i=i*batch_size</span><br><span class="line">        batch_indices=example_indices[i:i+batch_size]</span><br><span class="line">        X=[_data(j*num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y=[_data(j*num_steps+<span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X,ctx),nd.array(Y,ctx)</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#my_seq为人工序列，batch_size为批量大小，num_steps为时间步数</span></span><br><span class="line"><span class="attribute">my_seq</span>=list(range(30))</span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> data_iter_random(my_seq,<span class="attribute">batch_size</span>=2,num_steps=6):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>,X,<span class="string">&#x27;\nY:&#x27;</span>,Y,<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

![image-20220414215323211](动手学深度学习总结/image-20220414215323211.png)

<figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#相邻采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span></span>(corpus_indices,batch_size,num_steps,ctx=<span class="title class_">None</span>):</span><br><span class="line">    corpus_indices=nd.array(corpus_indices,ctx=ctx)</span><br><span class="line">    data_len=len(corpus_indices)</span><br><span class="line">    batch_len=data_len//batch_size</span><br><span class="line">    indices=corpus_indices[<span class="number">0</span><span class="symbol">:batch_size*batch_len</span>].reshape((batch_size,batch_len))</span><br><span class="line">    epoch_size=(batch_len<span class="number">-1</span>)//num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i=i*num_steps</span><br><span class="line">        X=indices[<span class="symbol">:</span>,<span class="symbol">i:</span>i+num_steps]</span><br><span class="line">        Y=indices[<span class="symbol">:</span>,i+<span class="number">1</span><span class="symbol">:i+num_steps+</span><span class="number">1</span>]</span><br><span class="line">        yield X,Y</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> data_iter_consecutive(my_seq,<span class="attribute">batch_size</span>=2,num_steps=6):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>,X,<span class="string">&#x27;\nY:&#x27;</span>,Y,<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

![image-20220414215520491](动手学深度学习总结/image-20220414215520491.png)

## 循环神经网络的从零开始实现

<figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="title">from</span> mxnet <span class="keyword">import</span> autograd,nd</span><br><span class="line"><span class="title">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">(corpus_indices,char_to_idx,idx_to_char,vocab_size)=d2l.load_data_jay_lyrics()</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这是one-hot向量</span></span><br><span class="line"><span class="attribute">nd</span>.one_hot(nd.array([<span class="number">0</span>,<span class="number">2</span>]),vocab_size)</span><br></pre></td></tr></table></figure>

![image-20220414215644013](动手学深度学习总结/image-20220414215644013.png)

<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#one-hot向量定义</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_onehot</span>(<span class="params">X,size</span>):</span><br><span class="line">    <span class="keyword">return</span> [nd.one_hot(x,size) <span class="keyword">for</span> x <span class="keyword">in</span> X.T]</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">X</span>=nd.arange(<span class="number">10</span>).reshape((<span class="number">2</span>,<span class="number">5</span>))</span><br><span class="line"><span class="attribute">X</span></span><br></pre></td></tr></table></figure>

![image-20220414215810664](动手学深度学习总结/image-20220414215810664.png)

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs=<span class="keyword">to</span><span class="constructor">_onehot(X,<span class="params">vocab_size</span>)</span></span><br><span class="line">inputs</span><br></pre></td></tr></table></figure>

![image-20220414215828076](动手学深度学习总结/image-20220414215828076.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">len</span><span class="params">(inputs)</span></span>,inputs<span class="selector-attr">[0]</span>.shape</span><br></pre></td></tr></table></figure>

![image-20220414215844220](动手学深度学习总结/image-20220414215844220.png)

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">num_inputs,num_hiddens,<span class="attribute">num_outputs</span>=vocab_size,256,vocab_size</span><br><span class="line"><span class="attribute">ctx</span>=d2l.try_gpu()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;will use&#x27;</span>,ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">def get_params():</span><br><span class="line">    def _one(shape):</span><br><span class="line">        return nd.random.normal(<span class="attribute">scale</span>=0.01,shape=shape,ctx=ctx)</span><br><span class="line">    #隐藏层参数</span><br><span class="line">    <span class="attribute">W_xh</span>=_one((num_inputs,num_hiddens))</span><br><span class="line">    <span class="attribute">W_hh</span>=_one((num_hiddens,num_hiddens))</span><br><span class="line">    <span class="attribute">b_h</span>=nd.zeros(num_hiddens,ctx=ctx)</span><br><span class="line">    #输出层参数</span><br><span class="line">    <span class="attribute">W_hq</span>=_one((num_hiddens,num_outputs))</span><br><span class="line">    <span class="attribute">b_q</span>=nd.zeros(num_outputs,ctx=ctx)</span><br><span class="line">    #附上模型</span><br><span class="line">    params=[W_xh,W_hh,b_h,W_hq,b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.attach_grad()</span><br><span class="line">    return params</span><br></pre></td></tr></table></figure>

<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#初始化rnn模型状态</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size,num_hiddens,ctx</span>):</span><br><span class="line">    <span class="keyword">return</span> (nd.zeros(shape=(batch_size,num_hiddens),ctx=ctx),)</span><br></pre></td></tr></table></figure>

<figure class="highlight pf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def rnn(inputs,<span class="keyword">state</span>,params):</span><br><span class="line">    W_xh,W_hh,b_h,W_hq,b_q=params</span><br><span class="line">    H,=<span class="keyword">state</span></span><br><span class="line">    outputs=[]</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H=nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h)</span><br><span class="line">        Y=nd.dot(H,W_hq)+b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    return outputs,(H,)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

![循环神经网络rnn公式](动手学深度学习总结/循环神经网络rnn公式.png)

<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">state=<span class="built_in">init_rnn_state</span>(X<span class="selector-class">.shape</span><span class="selector-attr">[0]</span>,num_hiddens,ctx)</span><br><span class="line">inputs=<span class="built_in">to_onehot</span>(X<span class="selector-class">.as_in_context</span>(ctx),vocab_size)</span><br><span class="line">params=<span class="built_in">get_params</span>()</span><br><span class="line">outputs,state_new=<span class="built_in">rnn</span>(inputs,state,params)</span><br><span class="line"><span class="function"><span class="title">len</span><span class="params">(outputs)</span></span>,outputs<span class="selector-attr">[0]</span><span class="selector-class">.shape</span>,state_new<span class="selector-attr">[0]</span>.shape</span><br></pre></td></tr></table></figure>

![image-20220414220338235](动手学深度学习总结/image-20220414220338235.png)

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#本函数基于前缀prefix(含有数个字符的字符串)来预测接下来的num_chars个字符。</span><br><span class="line">def predict<span class="constructor">_rnn(<span class="params">prefix</span>,<span class="params">num_chars</span>,<span class="params">rnn</span>,<span class="params">params</span>,<span class="params">init_run_state</span>,<span class="params">num_hiddens</span>,<span class="params">vocab_size</span>,<span class="params">ctx</span>,<span class="params">idx_to_char</span>,<span class="params">char_to_idx</span>)</span>:</span><br><span class="line">    state=init<span class="constructor">_run_state(1,<span class="params">num_hiddens</span>,<span class="params">ctx</span>)</span></span><br><span class="line">    output=<span class="literal">[<span class="identifier">char_to_idx</span>[<span class="identifier">prefix</span>[<span class="number">0</span>]</span>]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars+len(prefix)-<span class="number">1</span>):</span><br><span class="line">    	#将上一时间步的输出当作当前时间步的输入，第一次输入为output<span class="literal">[-<span class="number">1</span>]</span>为output<span class="literal">[<span class="number">0</span>]</span>，第二次为上次         #时间步</span><br><span class="line">        X=<span class="keyword">to</span><span class="constructor">_onehot(<span class="params">nd</span>.<span class="params">array</span>([<span class="params">output</span>[-1]],<span class="params">ctx</span>=<span class="params">ctx</span>)</span>,vocab_size)</span><br><span class="line">        #计算输出和更新隐藏状态</span><br><span class="line">        (Y,state)=rnn(X,state,params)</span><br><span class="line">        #下一个时间步的输入是prefix里的字符或者当前的最佳预测字符 </span><br><span class="line">        <span class="keyword">if</span> t&lt;len(prefix)-<span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx<span class="literal">[<span class="identifier">prefix</span>[<span class="identifier">t</span>+<span class="number">1</span>]</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(<span class="built_in">int</span>(Y<span class="literal">[<span class="number">0</span>]</span>.argmax(axis=<span class="number">1</span>).asscalar<span class="literal">()</span>))#取概率为最大的那一个</span><br><span class="line">    return &#x27;&#x27;.join(<span class="literal">[<span class="identifier">idx_to_char</span>[<span class="identifier">i</span>]</span> <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure>

<figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict<span class="constructor">_rnn(&#x27;分开&#x27;,10,<span class="params">rnn</span>,<span class="params">params</span>,<span class="params">init_rnn_state</span>,<span class="params">num_hiddens</span>,<span class="params">vocab_size</span>,<span class="params">ctx</span>,<span class="params">idx_to_char</span>,<span class="params">char_to_idx</span>)</span></span><br></pre></td></tr></table></figure>

![image-20220414222335439](动手学深度学习总结/image-20220414222335439.png)

<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#裁剪梯度</span></span><br><span class="line">def grad_clipping(<span class="built_in">params</span>,theta,ctx):</span><br><span class="line">    norm=nd.array([<span class="number">0</span>],ctx)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">param</span> <span class="keyword">in</span> <span class="built_in">params</span>:</span><br><span class="line">        norm+=(<span class="built_in">param</span>.grad**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    norm=norm.<span class="built_in">sqrt</span>().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm&gt;theta:</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">param</span> <span class="keyword">in</span> <span class="built_in">params</span>:</span><br><span class="line">            <span class="built_in">param</span>.grad[:]*=theta/norm</span><br></pre></td></tr></table></figure>

![image-20220414222559582](动手学深度学习总结/image-20220414222559582.png)

<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#定义模型训练函数</span></span><br><span class="line">def train_and_predict_rnn(rnn,get_params,init_rnn_state,num_hiddens,vocab_size,ctx,corpus_indices,idx_to_char,char_to_idx,</span><br><span class="line">                          is_random_iter,num_epochs,num_steps,lr,clipping_theta,batch_size,pred_period,pred_len,prefixes):</span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        <span class="attribute">data_iter_fn</span>=d2l.data_iter_random</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attribute">data_iter_fn</span>=d2l.data_iter_consecutive</span><br><span class="line">    <span class="attribute">params</span>=get_params()</span><br><span class="line">    <span class="attribute">loss</span>=gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">   # <span class="built_in">print</span>(num_epochs)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">       # <span class="built_in">print</span>(epoch)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:</span><br><span class="line">            <span class="attribute">state</span>=init_rnn_state(batch_size,num_hiddens,ctx)</span><br><span class="line">        l_sum,n,<span class="attribute">start</span>=0.0,0,time.time()</span><br><span class="line">        <span class="attribute">data_iter</span>=data_iter_fn(corpus_indices,batch_size,num_steps,ctx)</span><br><span class="line">        <span class="keyword">for</span> X,Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:</span><br><span class="line">                <span class="attribute">state</span>=init_rnn_state(batch_size,num_hiddens,ctx)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach()</span><br><span class="line">            with autograd.record():</span><br><span class="line">                <span class="attribute">inputs</span>=to_onehot(X,vocab_size)</span><br><span class="line">                (outputs,state)=rnn(inputs,state,params)</span><br><span class="line">                <span class="attribute">outputs</span>=nd.concat(*outputs,dim=0)</span><br><span class="line">                <span class="attribute">y</span>=Y.T.reshape((-1,))</span><br><span class="line">                <span class="attribute">l</span>=loss(outputs,y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params,clipping_theta,ctx)</span><br><span class="line">            d2l.sgd(params,lr,1)</span><br><span class="line">            l_sum+=l.asscalar()*y.size</span><br><span class="line">            n+=y.size</span><br><span class="line">            <span class="keyword">if</span> (epoch+1)%<span class="attribute">pred_period</span>==0:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;epoch %d,perplexity %f,time %.2f sec&#x27;</span>%(epoch+1,math.exp(l_sum/n),time.time()-start))</span><br><span class="line">                <span class="keyword">for</span><span class="built_in"> prefix </span><span class="keyword">in</span> prefixes:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span>,predict_rnn(prefix,pred_len,rnn,params,init_rnn_state,num_hiddens,vocab_size,ctx,</span><br><span class="line">                          idx_to_char,char_to_idx))</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">num_epochs</span>,num_steps,batch_size,lr,clipping_theta=<span class="number">250</span>,<span class="number">35</span>,<span class="number">32</span>,<span class="number">1</span>e2,<span class="number">1</span>e-<span class="number">2</span></span><br><span class="line"><span class="attribute">pred_period</span>,pred_len,prefixes=<span class="number">50</span>,<span class="number">50</span>,[&#x27;分开&#x27;,&#x27;不分开&#x27;]</span><br><span class="line"></span><br><span class="line"><span class="attribute">train_and_predict_rnn</span>(rnn,get_params,init_rnn_state,num_hiddens,vocab_size,ctx,corpus_indices,idx_to_char,</span><br><span class="line">                       <span class="attribute">char_to_idx</span>,True,num_epochs,num_steps,lr,clipping_theta,batch_size,pred_period,pred_len,</span><br><span class="line">                         <span class="attribute">prefixes</span>)</span><br></pre></td></tr></table></figure>

![image-20220414222728131](动手学深度学习总结/image-20220414222728131.png)

## 门控循环单元(GRU)

门控循环单元结构 

![image-20220415130545022](动手学深度学习总结/image-20220415130545022.png)

候选隐藏单元

![image-20220415130715939](动手学深度学习总结/image-20220415130715939.png)

可以看出这个地方是由重置门来控制$$H_{t-1}$$，当重置门为0时，表示上一时间步的隐藏状态$$H_{t-1}$$不加入该时间步中，即上一时间步不影响这一时间步隐藏单元$$\tilde{H_t}$$的生成，当重置门为1时，表示上一时间步的隐藏状态$$H_{t-1}$$会影响该时间步隐藏单元$$\tilde{H_t}$$的的生成。

![image-20220415131115611](动手学深度学习总结/image-20220415131115611.png)

可以看到我们有t时刻的隐藏状态公式

![image-20220415131637947](动手学深度学习总结/image-20220415131637947.png)

这表示若更新门当从t-1到t时之间近似为1，表示在时间步t-1到t之间的输入信息几乎没有流入时间步t的隐藏状态$$H_t$$，这表示我从所状态$$H_{t-1}$$隐藏状态，经过t时间步的单元时没有怎么改变，并且可以以$$H_{t}$$的状态输入到时刻t+1处，这可以保证时间序列之间的长依赖。

**重置门有助力捕捉时间序列里短期的依赖关系，从t-1到t时刻**

**更新门有助于捕捉时间序列里长期的依赖关系，可以通过更新门把状态从更加长时间的依赖**

![GRUpng](动手学深度学习总结/GRUpng.png)</script><p>(Y^{<em>})^{<t>}=H_t</t></em>W_{hq}+b</p>
<p>$$</p>
<h2 id="长短期记忆"><a href="#长短期记忆" class="headerlink" title="长短期记忆"></a>长短期记忆</h2><p><img src="/posts/b96b4738/image-20220415182710558.png" alt="image-20220415182710558"></p>
<p><img src="/posts/b96b4738/image-20220415182738771.png" alt="image-20220415182738771"></p>
<p><img src="/posts/b96b4738/image-20220415183135068.png" alt="image-20220415183135068"></p>
<p><img src="/posts/b96b4738/image-20220415183156489.png" alt="image-20220415183156489"></p>
<p></p>
    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
              <a href="/tags/MXNet/" rel="tag"># MXNet</a>
              <a href="/tags/%E5%8A%A8%E6%89%8B%E5%AD%A6%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 动手学深度学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/4fb3f4ca/" rel="prev" title="Qt5.9-开发指南">
      <i class="fa fa-chevron-left"></i> Qt5.9-开发指南
    </a></div>
      <div class="post-nav-item">
    <a href="/posts/7740304/" rel="next" title="mysql必知必会">
      mysql必知必会 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="nav-number">1.</span> <span class="nav-text">深度学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-number">1.1.</span> <span class="nav-text">线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E5%9F%BA%E6%9C%AC%E8%A6%81%E7%B4%A0"><span class="nav-number">1.1.1.</span> <span class="nav-text">线性回归的基本要素</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">模型训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="nav-number">1.1.1.3.</span> <span class="nav-text">训练数据</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-number">1.1.1.4.</span> <span class="nav-text">损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="nav-number">1.1.1.5.</span> <span class="nav-text">优化算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E8%A1%A8%E7%A4%BA%E6%96%B9%E6%B3%95"><span class="nav-number">1.2.</span> <span class="nav-text">线性回归的表示方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.3.</span> <span class="nav-text">线性回归从零开始实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.4.</span> <span class="nav-text">线性回归的简洁实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E7%B1%BB%E6%95%B0%E6%8D%AE%E9%9B%86-Fashion-MNIST"><span class="nav-number">1.5.</span> <span class="nav-text">图像分类数据集(Fashion-MNIST)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax%E5%9B%9E%E5%BD%92%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.6.</span> <span class="nav-text">softmax回归从零开始实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#softmax%E5%9B%9E%E5%BD%92%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.7.</span> <span class="nav-text">softmax回归简洁实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA"><span class="nav-number">1.8.</span> <span class="nav-text">多层感知机</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0"><span class="nav-number">1.9.</span> <span class="nav-text">激活函数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.10.</span> <span class="nav-text">多层感知机的从零开始实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%E7%9A%84%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.11.</span> <span class="nav-text">多层感知机的简洁实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%AC%A0%E6%8B%9F%E5%90%88%E4%B8%8E%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">1.12.</span> <span class="nav-text">欠拟合与过拟合</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9D%83%E9%87%8D%E8%A1%B0%E5%87%8F-%E5%8E%9F%E7%90%86%E6%9A%82%E4%B8%8D%E8%A7%A3%E9%87%8A"><span class="nav-number">1.13.</span> <span class="nav-text">权重衰减-原理暂不解释</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%95%BF%E7%9F%AD%E6%9C%9F%E8%AE%B0%E5%BF%86"><span class="nav-number">1.14.</span> <span class="nav-text">长短期记忆</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">刘王权</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">12</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">15</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wangquanliu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wangquanliu" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="/liuwangquan@163.com" title="E-Mail → liuwangquan@163.com"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘王权</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">75k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:08</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
