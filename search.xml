<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Qt5.9-开发指南</title>
    <url>/posts/4fb3f4ca/</url>
    <content><![CDATA[<p>本文为QT，C++开发知识点整理。</p>
<span id="more"></span>
<h1 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h1><h2 id="QThread创建多线程"><a href="#QThread创建多线程" class="headerlink" title="QThread创建多线程"></a>QThread创建多线程</h2><p>一个QThread类的对象管理一个线程，一般从QThread继承一个自定义类，并重定义虚函数run()，在run()函数里实现线程所需要的任务。</p>
<p>将应用程序的线程称为<strong>主线程</strong>，额外创建的线程则称之为<strong>工作线程</strong>，一般在主线程里创建工作线程，并调用start()开始执行工作线程的任务，start()会在内部调用run()函数，在run()函数里调用exit()或quit()可以结束线程的事件循环，或在主线程里调用terminate()强制结束线程。</p>
<p><img src="/posts/4fb3f4ca/绘图1-16488023194702.png" alt="绘图1" style="zoom:20%;"></p>
<p>myQthread类是我们自己定义的类，我们需要对run()函数进行重写。</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">run</span>() Q_DECL_OVERRIDE</span>;</span><br></pre></td></tr></table></figure>
<p>Q_DECL_OVERRIDE相当于C++的override，加上以便区分。</p>
<p>通过这样继承之后，我们在调用时，有</p>
<figure class="highlight abnf"><table><tr><td class="code"><pre><span class="line">myQthread a<span class="comment">;</span></span><br><span class="line">a.start()<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>这样通过start()函数来进行调用run()函数。</p>
<center>Qthread类的主要接口</center>

<div class="table-container">
<table>
<thead>
<tr>
<th>类型</th>
<th style="text-align:center">函数</th>
<th style="text-align:left">功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>公共函数</td>
<td style="text-align:center">bool isFinished()</td>
<td style="text-align:left">线程是否结束</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">bool isRunning()</td>
<td style="text-align:left">线程是否正在运行</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">Priority priority()</td>
<td style="text-align:left">返回线程的优先级</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">void exit(int retrunCode=0)</td>
<td style="text-align:left">退出线程，return Code为0表成功，否则有错误</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">void setPriority(Priority priority)</td>
<td style="text-align:left">设置线程优先级</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">bool wait(unsigned long time)</td>
<td style="text-align:left">阻止线程执行，直到线程结束。或等待时间超过time毫秒</td>
</tr>
<tr>
<td>公共槽函数</td>
<td style="text-align:center">void quit()</td>
<td style="text-align:left">退出线程，返回代码0</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">void start(Priority priority)</td>
<td style="text-align:left">内部调用run()开始执行线程，操作系统根据priority进行调度终止线程的运行</td>
</tr>
<tr>
<td>信号</td>
<td style="text-align:center">void finished()</td>
<td style="text-align:left">在线程要结束时发射此信号</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">void start()</td>
<td style="text-align:left">在线程开始,run()函数被调用之前发射此信号</td>
</tr>
<tr>
<td>静态公共成员</td>
<td style="text-align:center">int idealThreadCount()</td>
<td style="text-align:left">返回系统上能运行的线程的理想个数</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">void msleep(unsigned long msecs)</td>
<td style="text-align:left">强制当前线程休眠msecs毫秒</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">void sleep(unsigned long secs)</td>
<td style="text-align:left">强制当前线程休眠secs秒</td>
</tr>
<tr>
<td></td>
<td style="text-align:center">void usleep(unsigend long usecs)</td>
<td style="text-align:left">强制当前线程休眠usecs微秒</td>
</tr>
<tr>
<td>保护函数</td>
<td style="text-align:center">virtual void run()</td>
</tr>
</tbody>
</table>
</div>
<h2 id="基于互斥量的线程同步"><a href="#基于互斥量的线程同步" class="headerlink" title="基于互斥量的线程同步"></a>基于互斥量的线程同步</h2><p>QMutex和QMutexLocker是基于互斥量的线程同步类，QMutex定义的实例是一个互斥量，QMutex主要提供以下三个函数：</p>
<ol>
<li>lock()：锁定互斥量，如果另外一个线程锁定了这个互斥量，它将阻塞执行直到其它线程解锁这个互斥量。</li>
<li>unlock()：解锁一个互斥量，需要与lock()配对使用。</li>
<li>tryLock()：试图锁定一个互斥量，如果成功锁定就返回true，如果其他线程已经锁定了这个互斥量，就返回false，但不阻塞程序执行。</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">QDiceThread</span>: <span class="keyword">public</span> Qthread</span><br><span class="line">&#123;</span><br><span class="line">Q_OBJECT</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line"></span><br><span class="line">QMutex mutex;<span class="comment">//互斥量</span></span><br><span class="line"><span class="type">int</span> m_seq=<span class="number">0</span>;<span class="comment">//序号</span></span><br><span class="line"><span class="type">int</span> m_diceValue;</span><br><span class="line"></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">run</span><span class="params">()</span> Q_DECL_OVERRIDE</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">readValue</span><span class="params">(<span class="type">int</span>*seq,<span class="type">int</span>*diceValue)</span></span>;<span class="comment">//用于主线程读取数据的函数</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">QDiceThread::run</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">m_stop=<span class="literal">false</span>;</span><br><span class="line">m_seq=<span class="number">0</span>;</span><br><span class="line"><span class="built_in">qsrand</span>(QTime::<span class="built_in">currentTime</span>().<span class="built_in">msec</span>());<span class="comment">//随机数初始化</span></span><br><span class="line"><span class="keyword">while</span>(!m_stop) <span class="comment">//循环主体</span></span><br><span class="line">&#123;</span><br><span class="line">	<span class="keyword">if</span>(!m_paused)&#123;</span><br><span class="line">	</span><br><span class="line">	mutex.<span class="built_in">lock</span>();</span><br><span class="line">	m_diceValue=<span class="built_in">qrand</span>();</span><br><span class="line">	m_diceValue=(m_diceValue%<span class="number">6</span>)+<span class="number">1</span>;</span><br><span class="line">	m_seq++;</span><br><span class="line">	mutex.<span class="built_in">unlock</span>();</span><br><span class="line">	</span><br><span class="line">	&#125;</span><br><span class="line">	<span class="built_in">msleep</span>(<span class="number">500</span>);<span class="comment">//结束休眠</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">QDiceThread::readValue</span><span class="params">(<span class="type">int</span> *seq,<span class="type">int</span>*diceValue)</span></span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(mutex.<span class="built_in">tryLock</span>())&#123;</span><br><span class="line"></span><br><span class="line">	*seq=m_seq;</span><br><span class="line">	*diceValue=m_diceValue;</span><br><span class="line">	mutex.<span class="built_in">unlock</span>();</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line">	<span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在run函数中，对重新计算筛子点数和掷筛子次数的3行代码用互斥量mutex的lock()和unlock()进行了保护，这部分代码的执行不会被其他线程中断。</p>
<p><strong>注</strong>：lock()和unlock()必须配对使用</p>
<p>在readValue()函数中，用互斥量Mutex的tryLock()和unlock()进行了保护。如果tryLock()成功锁定互斥量，读取数值的两行代码执行时不会被中断，执行完后解锁，如果tryLock()锁定失败，函数就立即返回，而不会等待。</p>
<p>QMutexLocker是另外一个简化了互斥量处理的类。QMutexLocker的构造函数接受一个互斥量作为参数并将其锁定，QMutexLocker的析构函数则将此互斥量解锁，所以在QMutexLocker实例变量的生存期内的代码段得到保护，自动进行互斥量的锁定和解锁。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">QDiceThread::run</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">m_stop=<span class="literal">false</span>;</span><br><span class="line">m_seq=<span class="number">0</span>;</span><br><span class="line"><span class="built_in">qsrand</span>(QTime::<span class="built_in">currentTime</span>().<span class="built_in">msec</span>());<span class="comment">//随机数初始化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(!m_stop)&#123;<span class="comment">//循环主体</span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">if</span>(!m_paused)&#123;</span><br><span class="line"> <span class="function">QMutexLocker <span class="title">locker</span><span class="params">(&amp;mutex)</span></span>;</span><br><span class="line"> m_diceValue=<span class="built_in">qrand</span>();<span class="comment">//获取随机数</span></span><br><span class="line"> m_diceValue=(m_diceValue%<span class="number">6</span>)+<span class="number">1</span>;</span><br><span class="line"> m_seq++;</span><br><span class="line"> &#125;</span><br><span class="line">	<span class="built_in">msleep</span>(<span class="number">500</span>);<span class="comment">//线程休眠</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这样定义的QDiceThread类，在主程序中只能调用其readValue()函数来不断读取数值。</p>
<h2 id="基于QReadWriteLock的线程同步"><a href="#基于QReadWriteLock的线程同步" class="headerlink" title="基于QReadWriteLock的线程同步"></a>基于QReadWriteLock的线程同步</h2><p>使用互斥量时存在一个问题：每次只能有一个线程获得互斥量的权限。如果在一个程序中有多个线程读取某个变量，使用互斥量时也必须排除。而实际上若只是读取一个变量，是可以让多个线程同时访问的，这样互斥量会降低程序的性能。</p>
<p>假设有一个数据采集程序，一个线程负责采集数据到缓冲区，一个线程负责读取缓冲区的数据并显示，另一个线程负责读取缓冲区的数据并保存文件，示意代码如下 ：</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line">int buffer<span class="selector-attr">[100]</span>;</span><br><span class="line">QMutex mutex;</span><br><span class="line"></span><br><span class="line">void threadDAQ::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	mutex<span class="selector-class">.lock</span>();</span><br><span class="line">	<span class="built_in">get_data_and_write_in_buffer</span>();	<span class="comment">//数据写入buffer</span></span><br><span class="line">	mutex<span class="selector-class">.unlock</span>();</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void threadShow::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	mutex<span class="selector-class">.lock</span>();</span><br><span class="line">	<span class="built_in">show_buffer</span>();	<span class="comment">//读取buffer里的数据并显示 </span></span><br><span class="line">	mutex<span class="selector-class">.unlock</span>();</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void threadSaveFile::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	mutex<span class="selector-class">.lock</span>();</span><br><span class="line">	<span class="built_in">Save_buffer_toFile</span>();	<span class="comment">//读取buffer里的数据并保存文件</span></span><br><span class="line">	mutex<span class="selector-class">.unlock</span>();</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>数据缓冲区buffer和互斥量mutex都是全局变量，线程threadDAO将数据写到buffer，线程threadShow和threadSaveFile只是读取buffer,但是因为使用互斥量，这3个线程任何时候都只能有一个线程可以访问buffer。实际上,threadShow和threadSaveFile都只是读取buffer的数据，它们同时访问buffer是不会发生冲突。</p>
<p>Qt提供了QReadWriteLock类，它是基于读或写的模式进行代码段锁定的，在多个线程读取一个共享数据时，可以解决上面所说的互斥量存在的问题。</p>
<p>QReadWriteLock以读或写锁定的同步方法允许以读或写的方式保护一段代码，它可以允许多个线程以只读的方式同步访问资源，但是只要有一个线程在以写方式访问资源时，其他线程就必须等待直到写操作结束。</p>
<p>QReadWriteLock提供以下几个主要的函数：</p>
<ul>
<li>lockForRead()，以只读方式锁定资源，如果有其他线程以写入方式锁定，这个函数阻塞。</li>
<li>lockForWrite()，以写入方式锁定资源，如果本线程或其他线程以读或写模式锁定资源，这个函数就阻塞。</li>
<li>unlock()，解锁。</li>
<li>tryLockForRead()，是lockForRead()的非阻塞版本。</li>
<li>tryLockForWrite()，是lockForWrite()的非阻塞版本。</li>
</ul>
<p>使用QReadWriteLock，上面的三线程代码可以必定成如下形式：</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line">int buffer<span class="selector-attr">[100]</span>;</span><br><span class="line">QReadWriteLock Lock;</span><br><span class="line"></span><br><span class="line">void threadDAQ::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	Lock<span class="selector-class">.lockForWrite</span>();</span><br><span class="line">	<span class="built_in">get_data_and_write_in_buffer</span>();	<span class="comment">//数据写入buffer</span></span><br><span class="line">	Lock<span class="selector-class">.unlock</span>();</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void threadShow::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	Lock<span class="selector-class">.lockForRead</span>();</span><br><span class="line">	<span class="built_in">show_buffer</span>();	<span class="comment">//读取buffer里的数据并显示</span></span><br><span class="line">	Lock<span class="selector-class">.unlock</span>();</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void threadSaveFile::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	Lock<span class="selector-class">.lockForRead</span>();</span><br><span class="line">	<span class="built_in">Save_buffer_toFile</span>();	<span class="comment">//读取buffer里的数据并保存文件</span></span><br><span class="line">	Lock<span class="selector-class">.unlock</span>();</span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果threadDAQ没有以lockForWrite()锁定Lock，threadShow和threadSaveFile可以同时访问buffer，否则threadShow和threadSaveFile都被阻塞;如果threadShow和threadSaveFile都没有锁定，那threadDAQ能以写入方式锁定，否则threadDAQ就被阻塞。</p>
<p>QReadLocker和QWriteLocker是QReadWriteLock的简便形式，如同QMutexLocker是QMutex的简便版本一样，无需与unlock()配对使用。使用QReadLocker和QWriteLocker，则上面的代码可以改写为：</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line">int buffer<span class="selector-attr">[100]</span>;</span><br><span class="line">QReadWriteLock lock;</span><br><span class="line"></span><br><span class="line">void threadDAQ::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	QWriteLocker <span class="built_in">locker</span>(&amp;lock);</span><br><span class="line">	<span class="built_in">get_data_and_write_in_buffer</span>();	<span class="comment">//数据写入buffer</span></span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void threadShow::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	QReadLocker <span class="built_in">Locker</span>(&amp;lock);</span><br><span class="line">	<span class="built_in">show_buffer</span>();	<span class="comment">//读取buffer里的数据并显示</span></span><br><span class="line">	...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void threadSaveFile::run()&#123;</span><br><span class="line">	...</span><br><span class="line">	QReadLocker <span class="built_in">locker</span>(&amp;lock);</span><br><span class="line">	<span class="built_in">Save_buffer_toFile</span>();	<span class="comment">//读取buffer里的数据并保存到文件</span></span><br><span class="line">	...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="基于QWaitCondition的线程同步"><a href="#基于QWaitCondition的线程同步" class="headerlink" title="基于QWaitCondition的线程同步"></a>基于QWaitCondition的线程同步</h2><p>QWaitCondition提供另外一种改进的线程同步方法。QWaitCondition与QMutex结合，可以使一个线程在满足一定条件时通知其他多个线程，使它们及时作出响应，这样比只使用互斥量效率要高一些。例如，threadDAQ在写满一个缓冲区之后，及时通知threadShow和threadSaveFile，使它们可以及时读取缓冲区数据。</p>
<p>QWaitCondition提供如下一些函数：</p>
<ul>
<li>wait(QMutex*lockedMutex)，解锁互斥量lockedMutex，并阻塞等待唤醒条件，被唤醒后锁定lockedMutex并退出函数。</li>
<li>wakeAll()，唤醒所有处于等待状态的线程，线程唤醒的顺序不确定，由操作系统的调度策略决定。</li>
<li>wakeOne()，唤醒一个处于等待状态的线程，唤醒哪个线程不确定，由操作系统的调度策略决定。</li>
</ul>
<p>线程类QThreadProducer专门负责掷筛子产生点数，线程类QThreadConsumer专门及时读取数据，并送给主线程进行显示。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">QMutex mutex;</span><br><span class="line">QWaitCondition newdataAvailable;</span><br><span class="line"><span class="type">int</span> seq=<span class="number">0</span>;	<span class="comment">//序号</span></span><br><span class="line"><span class="type">int</span> diceValue;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">QthreadProducer::run</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">	m_stop=<span class="literal">false</span>;</span><br><span class="line">	seq=<span class="number">0</span>;</span><br><span class="line">	<span class="built_in">qsrand</span>(QTime::<span class="built_in">currentTime</span>().<span class="built_in">msec</span>());	<span class="comment">//随机数初始化</span></span><br><span class="line">	</span><br><span class="line">	<span class="keyword">while</span>(!m_stop)&#123;	<span class="comment">//循环主体</span></span><br><span class="line">	mutex.<span class="built_in">lock</span>();</span><br><span class="line">	diceValue=<span class="built_in">qrand</span>();	<span class="comment">//获取随机数</span></span><br><span class="line">	diceValue=(diceValue%<span class="number">6</span>)+<span class="number">1</span>;</span><br><span class="line">	seq++;</span><br><span class="line">	mutex.<span class="built_in">unlock</span>();</span><br><span class="line">	newdataAvailable.<span class="built_in">wakeAll</span>();	<span class="comment">//唤醒所有线程，有新数据</span></span><br><span class="line">	<span class="built_in">msleep</span>(<span class="number">500</span>);	<span class="comment">//线程休眠</span></span><br><span class="line"></span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">QthreadConsumer::run</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">	m_stop=<span class="literal">false</span>;</span><br><span class="line">	<span class="keyword">while</span>(!m_stop) &#123;	<span class="comment">//循环主体</span></span><br><span class="line">	</span><br><span class="line">	mutex.<span class="built_in">lock</span>();</span><br><span class="line">	newdataAvailable.<span class="built_in">wait</span>(&amp;mutex);	<span class="comment">//先解锁mutex，使其他线程可以使用mutex</span></span><br><span class="line">	<span class="function">emit <span class="title">newValue</span><span class="params">(seq,diceValue)</span></span>;</span><br><span class="line">	mutex.<span class="built_in">unlock</span>();</span><br><span class="line">	</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>QThreadProducer::run()函数负责每隔500毫秒掷筛子产生一次数据，新数据产生后通过等待条件唤醒所有等待的线程，即：</p>
<figure class="highlight haxe"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span><span class="type">dataAvailable</span>.wakeAll();</span><br></pre></td></tr></table></figure>
<p>QThreadConsumer::run()函数中的while循环，首先要将互斥量锁定，再执行下面的一条语句：</p>
<figure class="highlight haxe"><table><tr><td class="code"><pre><span class="line"><span class="keyword">new</span><span class="type">dataAvailable</span>.wait(&amp;mutex);</span><br></pre></td></tr></table></figure>
<p>这条语句以mutex作为输入参数，内部会首先解锁mutex，使其它线程可以使用mutex，newdataAvailable进行等待状态。当QThreadProducer产生新数据使用newdataAvailable.wakAll()唤醒所有线程后，newdataAvailable.wait(&amp;mutex)会再次锁定mutex，然后退出阻塞状态，以执行后面的语句。</p>
<h2 id="基于信号量的线程同步"><a href="#基于信号量的线程同步" class="headerlink" title="基于信号量的线程同步"></a>基于信号量的线程同步</h2><p>信号量是另一种限制对共享资源进行访问的线程同步机制。一个互斥量只能被锁定一次，而信号量可以多次使用。信号量通常用来保护一定数量的相同的资源，如数据采集时的双缓冲区。</p>
<p>QSemaphore是实现信号量功能的类，它可以提供以下几个基本的函数。</p>
<p>acquire(int n)尝试获得n个资源。如果没有这么多资源，线程将阻塞直到有n个资源可用。</p>
<p>release(int n)释放n个资源，如果信号量的资源已全部可用之后再release()，就可以创建更多的资源，增加可用资源的个数。</p>
<p>int available()返回当前信号量可用的资源个数，这个数永远不可能为负数，如果为0，就说明当前没有资源可用。</p>
<p>bool tryAcquire(int n=1)，尝试获取n个资源，不成功时不阻塞线程。</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">QSemaphore</span> wc(<span class="number">5</span>);	//WC.available()==<span class="number">5</span>，初始化资源为<span class="number">5</span></span><br><span class="line"><span class="attribute">wc</span>.acquire(<span class="number">4</span>);	//wc.available()==<span class="number">1</span>，用了四个资源，剩<span class="number">1</span>个可用</span><br><span class="line"><span class="attribute">wc</span>.release(<span class="number">2</span>);	//wc.available()==<span class="number">3</span>，释放<span class="number">2</span>个资源，剩<span class="number">3</span>个资源可用</span><br><span class="line"><span class="attribute">wc</span>.tryAcquire(<span class="number">1</span>);	//因为wc.available()==<span class="number">0</span>，返回false</span><br><span class="line"><span class="attribute">wc</span>.acquire();	//因为wc.available()==<span class="number">0</span>，没有资源可用，阻塞</span><br></pre></td></tr></table></figure>
<p>双缓冲区数据采集和读取线程类设计</p>
<p>信号量通常用来保护一定数量的相同的资源，如数据采集时的双缓冲区，适用于Producer/Consumer模型。</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">const int BufferSize=<span class="number">8</span>;</span><br><span class="line">int buffer1[BufferSize];</span><br><span class="line">int buffer2[BufferSize];</span><br><span class="line"></span><br><span class="line">int curBuf=<span class="number">1</span>;	<span class="regexp">//</span>当前正在写入的Buffer</span><br><span class="line">int bufNo=<span class="number">1</span>;	<span class="regexp">//</span>采集的缓冲区序号</span><br><span class="line">quint8 counter=<span class="number">0</span>;	<span class="regexp">//</span>数据生成器</span><br><span class="line"></span><br><span class="line">QSemaphore	emptyBufs(<span class="number">2</span>);	<span class="regexp">//</span>信号量，空的缓冲区个数，初始资源个数为<span class="number">2</span></span><br><span class="line">QSemaphore	fullBufs;	<span class="regexp">//</span>满的缓冲区个数，初始资源为<span class="number">0</span></span><br><span class="line"></span><br><span class="line">void QThreadDAQ::run()&#123;</span><br><span class="line"></span><br><span class="line">	m_stop=false;	<span class="regexp">//</span>启动线程时令m_stop=false</span><br><span class="line">	bufNo=<span class="number">0</span>;	<span class="regexp">//</span>缓冲区序号</span><br><span class="line">	curBuf=<span class="number">1</span>;	<span class="regexp">//</span>当前写入使用的缓冲区</span><br><span class="line">	counter=<span class="number">0</span>;	<span class="regexp">//</span>数据生成器</span><br><span class="line">	int n=emptyBufs.available();</span><br><span class="line">	<span class="keyword">if</span>(n&lt;<span class="number">2</span>)		<span class="regexp">//</span>保证线程启动时，emptyBufs.available==<span class="number">2</span></span><br><span class="line">	emptyBufs.release(<span class="number">2</span>-n);</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">while</span>(!m_stop)&#123;<span class="regexp">//</span>循环主体</span><br><span class="line">	</span><br><span class="line">	emptyBufs.acquire();<span class="regexp">//</span>获取一个空的缓冲区	</span><br><span class="line">	<span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;BufferSize;i++)&#123;	<span class="regexp">//</span>产生一个缓冲区的数据</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span>(curBuf==<span class="number">1</span>)</span><br><span class="line">		buffer1[i]=counter;	<span class="regexp">//</span>向缓冲区写入数据</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		buffer2[i]=counter;</span><br><span class="line">	counter++;	<span class="regexp">//</span>模拟数据采集卡产生数据</span><br><span class="line">	msleep(<span class="number">50</span>)</span><br><span class="line">	</span><br><span class="line">	&#125;</span><br><span class="line">	bufNo++;<span class="regexp">//</span>缓冲区序号</span><br><span class="line">	<span class="keyword">if</span>(curBuf==<span class="number">1</span>)	<span class="regexp">//</span>切换当前写入缓冲区</span><br><span class="line">		curBuf=<span class="number">2</span>;</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">		curBuf=<span class="number">1</span>;</span><br><span class="line">	fullBufs.release();<span class="regexp">//</span>有了一个满的缓冲区，available==<span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">void QThreadShow::run()&#123;</span><br><span class="line">	</span><br><span class="line">	m_stop=false;</span><br><span class="line">	int n=fullBufs.available();</span><br><span class="line">	<span class="keyword">if</span>(n&gt;<span class="number">0</span>)</span><br><span class="line">		fullBufs.acquire(n);	<span class="regexp">//</span>将fullbufs可用资源个数初始化为<span class="number">0</span></span><br><span class="line">	<span class="keyword">while</span>(!m_stop)&#123;</span><br><span class="line">	fullBufs.acquire();<span class="regexp">//</span>等待有缓冲区满，当buffBufs.available==<span class="number">0</span>阻塞</span><br><span class="line">	int bufferData[BufferSize];</span><br><span class="line">	int seq=bufNo;</span><br><span class="line">	</span><br><span class="line">	<span class="keyword">if</span>(curBuf==<span class="number">1</span>)	<span class="regexp">//</span>当前在写入的缓冲区是<span class="number">1</span>，那么满的缓冲区是<span class="number">2</span></span><br><span class="line">	<span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;BufferSize;i++)</span><br><span class="line">	bufferData[i]=buffer2[i];<span class="regexp">//</span>复制数据</span><br><span class="line">	<span class="keyword">else</span></span><br><span class="line">	<span class="keyword">for</span>(int i=<span class="number">0</span>;i&lt;BufferSize;i++)</span><br><span class="line">	bufferData[i]=buffer1[i];</span><br><span class="line">	</span><br><span class="line">	emptyBufs.release();<span class="regexp">//</span>缓冲一个空的缓冲区</span><br><span class="line">	emit newValue(bufferData,BufferSize,seq);<span class="regexp">//</span>给主线程传递数据</span><br><span class="line">	</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>信号量emptyBufs初始资源个数为2，表示有2个空的缓冲区可用。</p>
<p>信号量fullBufs初始资源个数为0，表示写满数据的缓冲区个数为零。</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>应用开发</category>
      </categories>
      <tags>
        <tag>QT</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>er图基本概念整理</title>
    <url>/posts/417c8dba/</url>
    <content><![CDATA[<p>本文为ER图基本概念笔记的整理。</p>
<span id="more"></span>
<p>er图的图形所代表的含义如下图所示</p>
<p><img src="/posts/417c8dba/20210507233721682.png" alt="20210507233721682"></p>
<h2 id="实体"><a href="#实体" class="headerlink" title="实体"></a>实体</h2><p>实体表示一个具体的类，如用户、订单等，即一个实体对象。</p>
<p><img src="/posts/417c8dba/v2-d135de9a82ab5c99826cb63e0609ba51_r.png" alt="v2-d135de9a82ab5c99826cb63e0609ba51_r"></p>
<h2 id="弱实体"><a href="#弱实体" class="headerlink" title="弱实体"></a>弱实体</h2><p>它衍生出来的为弱实体，即若B是依赖于A而存在的，则B为弱实体，如学生与成绩单的关系。</p>
<p><img src="/posts/417c8dba/v2-3809fb1447f349737f0d4ca60ddb4128_1440w.png" alt="v2-3809fb1447f349737f0d4ca60ddb4128_1440w"></p>
<h2 id="联合实体"><a href="#联合实体" class="headerlink" title="联合实体"></a>联合实体</h2><p>联合实体常用于实现两个或多个实体间的M:N的联系。</p>
<p>下图为一个联合实体，因为用户与订单之间1：N的关系，又订单对于用户来讲是一个弱实体，同理订单对于商品而言也是一个弱实体，他们都是如何自身不存在，那么订单本身将无法创建。</p>
<p><img src="/posts/417c8dba/v2-9ccb315f567c43d2d88e45481f11ba7b_r.png" alt="v2-9ccb315f567c43d2d88e45481f11ba7b_r"></p>
<h2 id="属性"><a href="#属性" class="headerlink" title="属性"></a>属性</h2><p>属性表示一个具体类所拥有的特性或者特征，具体而言，如用户有账号、用户名、密码等特性。</p>
<h2 id="多值属性"><a href="#多值属性" class="headerlink" title="多值属性"></a>多值属性</h2><p>其衍生出来的多值属性，表示一个类可能具有多个这样的属性，如用户里面的电话号码，一个用户可以有多个电话号码。</p>
<p>在下图，对于书本来讲分类就是一个多值属性。</p>
<p><img src="/posts/417c8dba/v2-ae1112c97b59789dda7b35e22d877daf_1440w.png" alt="v2-ae1112c97b59789dda7b35e22d877daf_1440w"></p>
<h2 id="派生属性"><a href="#派生属性" class="headerlink" title="派生属性"></a>派生属性</h2><p>派生属性表示是由其它属性进行推理出来的，需要经过一系列的计算才能出来的属性，在其表中不存在，如由身份证号码推测出用户的年龄。</p>
<h2 id="关系"><a href="#关系" class="headerlink" title="关系"></a>关系</h2><p>关系表示两个实体之间的联系，弱关系通常配合着弱实体一起使用。</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>er图</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql必知必会</title>
    <url>/posts/7740304/</url>
    <content><![CDATA[<p>本文将整理mysql必知必会-知识点整理。</p>
<span id="more"></span>
<h1 id="常用函数与操作符"><a href="#常用函数与操作符" class="headerlink" title="常用函数与操作符"></a>常用函数与操作符</h1><h2 id="UUID-函数"><a href="#UUID-函数" class="headerlink" title="UUID()函数"></a>UUID()函数</h2><p>UUID是由32位小写的16进制数字组成，如下</p>
<p>aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee</p>
<p>在mysql中UUID函数，前三组数字由时间戳生成。第四组数字暂时保持时间戳的唯一性，第五组数字是一个IEEE 802节点标点值，保证空间唯一。使用UUID()函数可以生成时间、空间都独一无二的值。</p>
<p>在mysql中的使用语法</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">select</span> <span class="title">uuid</span>()</span>;</span><br></pre></td></tr></table></figure>
<p>结果如下</p>
<p><img src="/posts/7740304/image-20220325112747319-16482888974852.png" alt="image-20220325112747319"></p>
<p>我们可以使用替换函数把中间的-去掉，语句如下 </p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">replace</span>(<span class="built_in">uuid</span>(),<span class="string">&quot;-&quot;</span>,<span class="string">&quot;&quot;</span>);</span><br></pre></td></tr></table></figure>
<p>结果如下</p>
<p><img src="/posts/7740304/image-20220325112915766-16482888896501.png" alt="image-20220325112915766"></p>
<h2 id="limit"><a href="#limit" class="headerlink" title="limit"></a>limit</h2><p>SQL语句</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">limit</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure>
<p>表示检索出的数据小于等于5行，而SQL语句</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">limit</span> <span class="number">5</span>,<span class="number">5</span>;</span><br></pre></td></tr></table></figure>
<p>指示SQL语句返回从第5行开始的5行，所以limit arg1,arg2,arg1表示从第几行开始，arg2表示返回几行。</p>
<h2 id="order"><a href="#order" class="headerlink" title="order"></a>order</h2><p>order语句如下</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> <span class="built_in">table_name</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> <span class="built_in">column_name</span> [<span class="keyword">desc</span>|<span class="keyword">asc</span>]</span><br></pre></td></tr></table></figure>
<p>order语句默认按升序排序，desc为升序，asc为降序，order可以对多个列进行排序。</p>
<h2 id="where"><a href="#where" class="headerlink" title="where"></a>where</h2><p>where子句操作符</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">操作符</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">=</td>
<td style="text-align:center">等于</td>
</tr>
<tr>
<td style="text-align:center">&lt;&gt;</td>
<td style="text-align:center">不等于</td>
</tr>
<tr>
<td style="text-align:center">!=</td>
<td style="text-align:center">不等于</td>
</tr>
<tr>
<td style="text-align:center">&lt;</td>
<td style="text-align:center">小于</td>
</tr>
<tr>
<td style="text-align:center">&lt;=</td>
<td style="text-align:center">小于等于</td>
</tr>
<tr>
<td style="text-align:center">&gt;</td>
<td style="text-align:center">大于</td>
</tr>
<tr>
<td style="text-align:center">&gt;=</td>
<td style="text-align:center">大于等于</td>
</tr>
<tr>
<td style="text-align:center">between a and b</td>
<td style="text-align:center">ab之间，包括ab</td>
</tr>
</tbody>
</table>
</div>
<h2 id="IN操作符"><a href="#IN操作符" class="headerlink" title="IN操作符"></a>IN操作符</h2><p>有SQL语句</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> vend_id <span class="keyword">in</span> (<span class="number">1002</span>,<span class="number">1003</span>)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p>此select语句检索供应商1002和1003制造的所有产品</p>
<font color="red">IN操作符一般比OR操作符执行更快</font>

<h2 id="NOT操作符"><a href="#NOT操作符" class="headerlink" title="NOT操作符"></a>NOT操作符</h2><p>有SQL语句</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> vend_id not <span class="keyword">in</span> (<span class="number">1002</span>,<span class="number">1003</span>)</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p>匹配1002,1003之外的vend_id制造的所有产品。</p>
<h2 id="LIKE操作符"><a href="#LIKE操作符" class="headerlink" title="LIKE操作符"></a>LIKE操作符</h2><h3 id="百分号-通配符"><a href="#百分号-通配符" class="headerlink" title="百分号%通配符"></a>百分号%通配符</h3><p>%表示任何字符出现的次数</p>
<p>有SQL语句</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name <span class="keyword">like</span> <span class="string">&#x27;jet%&#x27;</span></span><br></pre></td></tr></table></figure>
<p>匹配产品名以jet开始的产品名。</p>
<h3 id="下划线-通配符"><a href="#下划线-通配符" class="headerlink" title="下划线_通配符"></a>下划线_通配符</h3><p>下划线_只匹配单个字符 </p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name <span class="keyword">like</span> <span class="string">&#x27;_ton anvil&#x27;</span></span><br></pre></td></tr></table></figure>
<p>匹配产品名为aton anvil ，bton anvil 诸如此类，</p>
<h1 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h1><h2 id="基本字符匹配"><a href="#基本字符匹配" class="headerlink" title="基本字符匹配"></a>基本字符匹配</h2><figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name REGEXP <span class="string">&#x27;1000&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p>REGEXP表示告诉mysql作为正则表达式进行处理。该SQL语句表示检索列prod_name包含文本1000的所有行。</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> <span class="keyword">from</span> produts</span><br><span class="line"><span class="keyword">where</span> prod_name REGEXP <span class="string">&#x27;.000&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p>.是正则表达语句的一个特殊字符，它表示匹配任意一个字符，因此该SQL语句表示匹配，诸如01000,2000,..诸如此类的文本。</p>
<font color="red">REGEXP与like的区别</font>

<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="type">name</span> <span class="keyword">from</span> test </span><br><span class="line"><span class="keyword">where</span> <span class="type">name</span> <span class="keyword">like</span> <span class="string">&#x27;_w&#x27;</span></span><br></pre></td></tr></table></figure>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span>  <span class="type">name</span> <span class="keyword">from</span> test</span><br><span class="line"><span class="keyword">where</span> <span class="type">name</span> REGEXP <span class="string">&#x27;.w&#x27;</span></span><br></pre></td></tr></table></figure>
<p>后一条语句会返回</p>
<p>lwq</p>
<p>lwq</p>
<p>这是因为正则表达式是在文本内匹配任意位置</p>
<p>前一条语句什么都不返回</p>
<p>因为LIke操作符位置固定，除非有数据ww或者lw诸如此类的数据。</p>
<h2 id="进行OR匹配"><a href="#进行OR匹配" class="headerlink" title="进行OR匹配"></a>进行OR匹配</h2><figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name REGEXP <span class="string">&#x27;1000|2000&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p>|为正则表达式的OR操作符。它表示匹配其中之一。</p>
<p><img src="/posts/7740304/image-20220326180048541.png" alt="image-20220326180048541"></p>
<h2 id="匹配几个字符之一"><a href="#匹配几个字符之一" class="headerlink" title="匹配几个字符之一"></a>匹配几个字符之一</h2><figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> prod_name <span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name REGEXP <span class="string">&#x27;[123] Ton&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326180402863.png" alt="image-20220326180402863"></p>
<p>用了正则表达式[123] Ton。 [123]定义一组字符，它的意思是匹配1或2或3，因此， 1 ton和2 ton都匹配且返回（没有3 ton）  </p>
<h2 id="匹配范围"><a href="#匹配范围" class="headerlink" title="匹配范围"></a>匹配范围</h2><p>集合可用来定义要匹配的一个或多个字符，其中匹配数字0-9可简写成[0-9]，匹配字母可简写成[a-z]</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> prod_name <span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name REGEXP <span class="string">&#x27;[1-5] Ton&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326180719178.png" alt="image-20220326180719178"></p>
<h2 id="匹配特殊字符"><a href="#匹配特殊字符" class="headerlink" title="匹配特殊字符"></a>匹配特殊字符</h2><p>为了匹配特殊字符，必须用\\为前导。 \-表示查找-， \.表示查找.  。</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> vend_name <span class="keyword">from</span> vendors</span><br><span class="line"><span class="keyword">where</span> vend_name REGEXP <span class="string">&#x27;\\.&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> vend_name</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326180933032.png" alt="image-20220326180933032"></p>
<h2 id="匹配字符类"><a href="#匹配字符类" class="headerlink" title="匹配字符类"></a>匹配字符类</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">类</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">[:alnum:]</td>
<td style="text-align:center">任意字母和数字（同[a-zA-Z0-9]）</td>
</tr>
<tr>
<td style="text-align:center">[:alpha:]</td>
<td style="text-align:center">任意字符（同[a-zA-Z]）</td>
</tr>
<tr>
<td style="text-align:center">[:blank:]</td>
<td style="text-align:center">空格和制表（同[\\t]）</td>
</tr>
<tr>
<td style="text-align:center">[:cntrl:]</td>
<td style="text-align:center">ASCII控制字符（ ASCII 0到31和127）</td>
</tr>
<tr>
<td style="text-align:center">[:digit:]</td>
<td style="text-align:center">任意数字（同[0-9]）</td>
</tr>
<tr>
<td style="text-align:center">[:graph:]</td>
<td style="text-align:center">与[:print:]相同，但不包括空格</td>
</tr>
<tr>
<td style="text-align:center">[:lower:]</td>
<td style="text-align:center">任意小写字母（同[a-z]）</td>
</tr>
<tr>
<td style="text-align:center">[:print:]</td>
<td style="text-align:center">任意可打印字符</td>
</tr>
<tr>
<td style="text-align:center">[:punct:]</td>
<td style="text-align:center">既不在[:alnum:]又不在[:cntrl:]中的任意字符</td>
</tr>
<tr>
<td style="text-align:center">[:space:]</td>
<td style="text-align:center">包括空格在内的任意空白字符（同[\\f\\n\\r\\t\\v]）</td>
</tr>
<tr>
<td style="text-align:center">[:upper:]</td>
<td style="text-align:center">任意大写字母（同[A-Z]）</td>
</tr>
<tr>
<td style="text-align:center">[:xdigit:]</td>
<td style="text-align:center">任意十六进制数字（同[a-fA-F0-9]）</td>
</tr>
</tbody>
</table>
</div>
<h2 id="匹配多个例子"><a href="#匹配多个例子" class="headerlink" title="匹配多个例子"></a>匹配多个例子</h2><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">元字符</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">*</td>
<td style="text-align:center">0个或多个匹配</td>
</tr>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">1个或多个匹配（等于{1,}）</td>
</tr>
<tr>
<td style="text-align:center">?</td>
<td style="text-align:center">0个或1个匹配（等于{0,1}）</td>
</tr>
<tr>
<td style="text-align:center">{n}</td>
<td style="text-align:center">指定数目的匹配</td>
</tr>
<tr>
<td style="text-align:center">{n,}</td>
<td style="text-align:center">不少于指定数目的匹配</td>
</tr>
<tr>
<td style="text-align:center">{n,m}</td>
<td style="text-align:center">匹配数目的范围（ m不超过255）</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> prod_name <span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name REGEXP <span class="string">&#x27;\\([0-9] sticks?\\)&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326181845451.png" alt="image-20220326181845451"></p>
<p>双斜杠(表示匹配左括号(,[0-9]表示匹配任意数字,sticks(s后的?使得s可选)</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> prod_name <span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name REGEXP <span class="string">&#x27;[[:digit:]]&#123;4&#125;&#x27;</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> prod_name</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326182258136.png" alt="image-20220326182258136"></p>
<p>[:digit:]匹配任意数字，因而它为数字的一个集合。 {4}确切地要求它前面的字符（任意数字）出现4次，所以[[:digit:]]{4}匹配连在一起的任意4位数字。  </p>
<h2 id="定位符"><a href="#定位符" class="headerlink" title="定位符"></a>定位符</h2><p>目前为止的所有例子都是匹配一个串中任意位置的文本。为了匹配特定位置的文本，需要使用定位符。  </p>
<p>定位元字符</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">元字符</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">^</td>
<td style="text-align:center">文本开始</td>
</tr>
<tr>
<td style="text-align:center">$</td>
<td style="text-align:center">文本结尾</td>
</tr>
<tr>
<td style="text-align:center">[[:&lt;:]]</td>
<td style="text-align:center">词的开始</td>
</tr>
<tr>
<td style="text-align:center">[[:&gt;:]]</td>
<td style="text-align:center">词的结尾</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> prod_name</span><br><span class="line"><span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_name REGEXP <span class="string">&#x27;^[0-9\\.]&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326182647819.png" alt="image-20220326182647819"></p>
<p>^匹配串的开始 ，^[0-9\.]只在.或任意数字为串中第一个字符时才匹配它们。</p>
<h1 id="拼接字段"><a href="#拼接字段" class="headerlink" title="拼接字段"></a>拼接字段</h1><h2 id="concat函数"><a href="#concat函数" class="headerlink" title="concat函数"></a>concat函数</h2><figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">concat</span>(vend_name,<span class="string">&#x27;(&#x27;</span>,vend_country,<span class="string">&#x27;)&#x27;</span>)</span><br><span class="line"><span class="keyword">from</span> vendors</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> vend_name</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326191058867.png" alt="image-20220326191058867"></p>
<p>Concat()拼接串，即把多个串连接起来形成一个较长的串。  </p>
<p>Concat()需要一个或多个指定的串，各个串之间用逗号分隔。  </p>
<h2 id="Rtrim函数"><a href="#Rtrim函数" class="headerlink" title="Rtrim函数"></a>Rtrim函数</h2><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> concat(Rtrim(vend_name),<span class="string">&#x27;(&#x27;</span>,Rtrim(vend_country),<span class="string">&#x27;)&#x27;</span>) <span class="keyword">from</span> vendors</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> vend_name</span><br></pre></td></tr></table></figure>
<p>RTrim()函数去掉值右边的所有空格。  LTrim()（去掉串左边的空格）以及Trim()（去掉串左右两边的空格）。  </p>
<h1 id="数据处理函数"><a href="#数据处理函数" class="headerlink" title="数据处理函数"></a>数据处理函数</h1><h2 id="文本处理函数"><a href="#文本处理函数" class="headerlink" title="文本处理函数"></a>文本处理函数</h2><p>使用upper函数</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> vend_name,<span class="built_in">upper</span>(vend_name) <span class="keyword">as</span> vend_name_upcase <span class="keyword">from</span> vendors</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> vend_name</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326191901387.png" alt="image-20220326191901387"></p>
<p>upper函数将文本转换成大写。</p>
<p>常用的文本处理函数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">left(str,length)</td>
<td style="text-align:center">返回串左边的length字符</td>
</tr>
<tr>
<td style="text-align:center">length()</td>
<td style="text-align:center">返回串的长度</td>
</tr>
<tr>
<td style="text-align:center">locate(substr,str)</td>
<td style="text-align:center">返回substr在字符串str 的第一个出现的位置</td>
</tr>
<tr>
<td style="text-align:center">locate(substr,str,pos)</td>
<td style="text-align:center">返回字符串substr在字符串str，从pos处开始的第一次出现的位置，如果substr不在str中，则返回值为0</td>
</tr>
<tr>
<td style="text-align:center">Lower(str)</td>
<td style="text-align:center">将串str转换为小写</td>
</tr>
<tr>
<td style="text-align:center">LTrim(str)</td>
<td style="text-align:center">去掉串str左边的空格</td>
</tr>
<tr>
<td style="text-align:center">Right(str,legth)</td>
<td style="text-align:center">返回串右边的legth字符</td>
</tr>
<tr>
<td style="text-align:center">RTrim(str)</td>
<td style="text-align:center">去掉串str右边的空格</td>
</tr>
<tr>
<td style="text-align:center">Soundex(str)</td>
<td style="text-align:center">返回串str的SOUNDEX值</td>
</tr>
<tr>
<td style="text-align:center">SubString(str,posi)</td>
<td style="text-align:center">返回从串str的位置posi开始的子字符 串</td>
</tr>
<tr>
<td style="text-align:center">Upper(str)</td>
<td style="text-align:center">将串str转换为大写</td>
</tr>
</tbody>
</table>
</div>
<p>假设customers表中有一个顾客Coyote Inc.,其联系名为Y.LEE但如果这是输入错误，此联系名实际应该是Y.Lie，怎么办?，我们将使用Soundex()函数进行搜索，它匹配所有发音类似于Y.Lie的联系名：  </p>
<figure class="highlight sas"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> cust_name,cust_contact <span class="keyword">from</span> customers</span><br><span class="line"><span class="keyword">where</span> <span class="meta">Soundex</span>(cust_contact)=<span class="meta">Soundex</span>(<span class="string">&#x27;Y Lie&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220326194005184.png" alt="image-20220326194005184"></p>
<p>WHERE子句使用Soundex()函数来转换cust_contact列值和搜索串为它们的SOUNDEX值。因为Y.Lee和Y.Lie发音相似，所以它们的SOUNDEX值匹配，因此WHERE子句正确地过滤出了所需的数据。  </p>
<h2 id="日期和时间处理函数"><a href="#日期和时间处理函数" class="headerlink" title="日期和时间处理函数"></a>日期和时间处理函数</h2><p>常用日期和时间处理函数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">AddDate()</td>
<td style="text-align:center">增加一个日期（天、周等）</td>
</tr>
<tr>
<td style="text-align:center">AddTime()</td>
<td style="text-align:center">增加一个时间（时、分等）</td>
</tr>
<tr>
<td style="text-align:center">CurDate()</td>
<td style="text-align:center">返回当前日期</td>
</tr>
<tr>
<td style="text-align:center">CurTime()</td>
<td style="text-align:center">返回当前时间</td>
</tr>
<tr>
<td style="text-align:center">Date()</td>
<td style="text-align:center">返回日期时间的日期部分</td>
</tr>
<tr>
<td style="text-align:center">DateDiff()</td>
<td style="text-align:center">计算两个日期之差</td>
</tr>
<tr>
<td style="text-align:center">Date_Add()</td>
<td style="text-align:center">高度灵活的日期运算函数</td>
</tr>
<tr>
<td style="text-align:center">Date_Format()</td>
<td style="text-align:center">返回一个格式化的日期或时间串</td>
</tr>
<tr>
<td style="text-align:center">Day()</td>
<td style="text-align:center">返回一个日期的天数部分</td>
</tr>
<tr>
<td style="text-align:center">DayOfWeek()</td>
<td style="text-align:center">对于一个日期，返回对应的星期几</td>
</tr>
<tr>
<td style="text-align:center">Hour()</td>
<td style="text-align:center">返回一个时间的小时部分</td>
</tr>
<tr>
<td style="text-align:center">Minute()</td>
<td style="text-align:center">返回一个时间的分钟部分</td>
</tr>
<tr>
<td style="text-align:center">Month()</td>
<td style="text-align:center">返回一个日期的月份部分</td>
</tr>
<tr>
<td style="text-align:center">Now()</td>
<td style="text-align:center">返回当前日期和时间</td>
</tr>
<tr>
<td style="text-align:center">Second()</td>
<td style="text-align:center">返回一个时间的秒部分</td>
</tr>
<tr>
<td style="text-align:center">Time()</td>
<td style="text-align:center">返回一个日期时间的时间部分</td>
</tr>
<tr>
<td style="text-align:center">Year()</td>
<td style="text-align:center">返回一个日期的年份部分</td>
</tr>
</tbody>
</table>
</div>
<h2 id="数值处理函数"><a href="#数值处理函数" class="headerlink" title="数值处理函数"></a>数值处理函数</h2><p>常用数据处理函数</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">函数</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Abs()</td>
<td style="text-align:center">返回一个数的绝对值</td>
</tr>
<tr>
<td style="text-align:center">Cos()</td>
<td style="text-align:center">返回一个角度的余弦</td>
</tr>
<tr>
<td style="text-align:center">Exp()</td>
<td style="text-align:center">返回一个数的指数值</td>
</tr>
<tr>
<td style="text-align:center">Mod()</td>
<td style="text-align:center">返回除操作的余数</td>
</tr>
<tr>
<td style="text-align:center">Pi()</td>
<td style="text-align:center">返回圆周率</td>
</tr>
<tr>
<td style="text-align:center">Rand()</td>
<td style="text-align:center">返回一个随机数</td>
</tr>
<tr>
<td style="text-align:center">Sin()</td>
<td style="text-align:center">返回一个角度的正弦</td>
</tr>
<tr>
<td style="text-align:center">Sqrt()</td>
<td style="text-align:center">返回一个数的平方根</td>
</tr>
<tr>
<td style="text-align:center">Tan()</td>
<td style="text-align:center">返回一个角度的正切</td>
</tr>
</tbody>
</table>
</div>
<h1 id="分组数据"><a href="#分组数据" class="headerlink" title="分组数据"></a>分组数据</h1><h2 id="group-by"><a href="#group-by" class="headerlink" title="group by"></a>group by</h2><figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> vend_id ,<span class="keyword">count</span>(*) <span class="keyword">as</span> num_prods</span><br><span class="line"><span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> vend_id</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327091235617.png" alt="image-20220327091235617"></p>
<p>上面SQL语句，近按vend_id进行分组，并求它的产品数量。</p>
<h2 id="过滤分组"><a href="#过滤分组" class="headerlink" title="过滤分组"></a>过滤分组</h2><figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> cust_id,count<span class="comment">(*) as orders</span></span><br><span class="line"><span class="comment">from orders</span></span><br><span class="line"><span class="comment">group by cust_id</span></span><br><span class="line"><span class="comment">having count(*)</span>&gt;=<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327091623570.png" alt="image-20220327091623570"></p>
<font color="red">Having与where差别：where在数据分组前过滤，having在数据分组后进行过滤</font>

<figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> vend_id,count<span class="comment">(*) as num_prods</span></span><br><span class="line"><span class="comment">from products</span></span><br><span class="line"><span class="comment">where prod_price&gt;=10</span></span><br><span class="line"><span class="comment">group by vend_id</span></span><br><span class="line"><span class="comment">having count(*)</span>&gt;=<span class="number">2</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327091927047.png" alt="image-20220327091927047"></p>
<p>WHERE子句过滤所有prod_price至少为10的行。然后按vend_id分组数据， HAVING子句过滤计数为2或2以上的分组 。</p>
<h2 id="分组与排序"><a href="#分组与排序" class="headerlink" title="分组与排序"></a>分组与排序</h2><p>order by 与group by</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">order by</th>
<th style="text-align:center">group by</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">排序产生的输出</td>
<td style="text-align:center">分组行，但输出可能不是分组的顺序</td>
</tr>
<tr>
<td style="text-align:center">任意列都可以使用</td>
<td style="text-align:center">只可能使用选择列或表达式列，而且必须使用每个选择列表达式</td>
</tr>
<tr>
<td style="text-align:center">不一定需要</td>
<td style="text-align:center">如果与聚集函数一起使用列（或表达式），则必须使用</td>
</tr>
</tbody>
</table>
</div>
<p>我们发现用group by分组的数据是以分组顺序进行输出的，但情况并不总是这样，此外，用户也可能会要求以不同于分组的顺序排序。仅因为你以某种方式分组数据（获得特定的分组聚集值），并不表示你需要以相同的方式排序输出。应该提供明确的ORDER BY子句。 </p>
<font color="red">一般在使用GROUP BY子句时，应该也给
出ORDER BY子句。这是保证数据正确排序的唯一方法。千万
不要仅依赖GROUP BY排序数据。  </font>

<h1 id="子查询"><a href="#子查询" class="headerlink" title="子查询"></a>子查询</h1><p>假如需要列出订购物品TNT2的所有客户，应该怎样检索？  </p>
<p>(1) 检索包含物品TNT2的所有订单的编号。<br>(2) 检索具有前一步骤列出的订单编号的所有客户的ID。<br>(3) 检索前一步骤返回的所有客户ID的客户信息。  </p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> cust_name,cust_contact　<span class="comment">//根据客户ID检索出客户信息</span></span><br><span class="line"><span class="keyword">from</span> customers</span><br><span class="line"><span class="keyword">where</span> cust_id <span class="keyword">in</span> (</span><br><span class="line"><span class="keyword">select</span> cust_id <span class="keyword">from</span> orders <span class="comment">//检索订购TNT２的客户ID</span></span><br><span class="line"><span class="keyword">where</span> order_num <span class="keyword">in</span>(</span><br><span class="line"><span class="keyword">select</span> order_num <span class="keyword">from</span> orderitems <span class="comment">//检索物品ＴＮＴ２的所有客户</span></span><br><span class="line"><span class="keyword">where</span> prod_id=<span class="string">&#x27;TNT2&#x27;</span>))</span><br></pre></td></tr></table></figure>
<p>假如需要显示customers表中每个客户的订单总数。订单与相应的客户ID存储在orders表中。  </p>
<p>为了执行这个操作，遵循下面的步骤。<br>(1) 从customers表中检索客户列表。<br>(2) 对于检索出的每个客户，统计其在orders表中的订单数目。  </p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> cust_name,cust_state,(</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(*) <span class="keyword">from</span> orders</span><br><span class="line"><span class="keyword">where</span> orders.cust_id=customers.cust_id) <span class="keyword">as</span> orders</span><br><span class="line"><span class="keyword">from</span> customers</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> cust_name</span><br></pre></td></tr></table></figure>
<h1 id="联结表"><a href="#联结表" class="headerlink" title="联结表"></a>联结表</h1><figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> vend_name,prod_name,prod_price</span><br><span class="line"><span class="keyword">from</span> vendors,products</span><br><span class="line"><span class="keyword">where</span> vendors.vend_id=products.vend_id</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> vend_name,prod_name</span><br></pre></td></tr></table></figure>
<p>该语句为两个表的联结。</p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> vend_name,prod_name,prod_price</span><br><span class="line"><span class="keyword">from</span> vendors,products</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> vend_name,prod_name</span><br></pre></td></tr></table></figure>
<p>该语句因为没有过滤条件，所以会执行<font color="red">笛卡尔积</font>。</p>
<figure class="highlight mipsasm"><table><tr><td class="code"><pre><span class="line">select prod_name,vend_name,prod_price,quantity</span><br><span class="line">from <span class="keyword">orderitems,products,vendors</span></span><br><span class="line"><span class="keyword"></span>where products.vend_id =vendors.vend_id</span><br><span class="line"><span class="keyword">and </span><span class="keyword">orderitems.prod_id=products.prod_id </span><span class="keyword">and </span></span><br><span class="line"><span class="keyword">order_num </span>=<span class="number">2005</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327103926728.png" alt="image-20220327103926728"></p>
<p>使用and操作符对多表联合查询进行条件过滤。</p>
<font color="red">AS的优势</font>

<font color="red">1)缩短SQL语句</font>

<font color="red">2)允许在单条select语句中多次使用相同的表</font>

<h2 id="自连接"><a href="#自连接" class="headerlink" title="自连接"></a>自连接</h2><p>假如你发现某物品（其ID为DTNTR）存在问题，因此想知道生产该物品的供应商生产的其他物品是否也存在这些问题。此查询要求首先找到生产ID为DTNTR的物品的供应商，然后找出这个供应商生产的其他物品。  </p>
<figure class="highlight smali"><table><tr><td class="code"><pre><span class="line">select p1.prod_id,p1.prod_name</span><br><span class="line">from products as p1,products as p2</span><br><span class="line">where p1.vend_id=p2.vend_id<span class="built_in"> and</span></span><br><span class="line"><span class="built_in"></span>      p2.prod_id=&#x27;DTNTR&#x27;</span><br></pre></td></tr></table></figure>
<p>该语句为表自身的连接。</p>
<p><img src="/posts/7740304/image-20220327104725852.png" alt="image-20220327104725852"></p>
<h2 id="内连接"><a href="#内连接" class="headerlink" title="内连接"></a>内连接</h2><center>表A</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">a_id</th>
<th style="text-align:center">a_name</th>
<th style="text-align:center">a_part</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">老潘</td>
<td style="text-align:center">技术部</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">老王</td>
<td style="text-align:center">秘书部</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">老张</td>
<td style="text-align:center">设计部</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">老李</td>
<td style="text-align:center">运营部</td>
</tr>
</tbody>
</table>
</div>
<center>表B</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">b_id</th>
<th style="text-align:center">b_name</th>
<th style="text-align:center">b_part</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">老王</td>
<td style="text-align:center">秘书部</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">老张</td>
<td style="text-align:center">设计部</td>
</tr>
<tr>
<td style="text-align:center">5</td>
<td style="text-align:center">老刘</td>
<td style="text-align:center">人事部</td>
</tr>
<tr>
<td style="text-align:center">6</td>
<td style="text-align:center">老黄</td>
<td style="text-align:center">生产部</td>
</tr>
</tbody>
</table>
</div>
<p>对表A与表B进行字段Id内连接就有</p>
<center>表C</center>

<div class="table-container">
<table>
<thead>
<tr>
<th>a_id</th>
<th>a_name</th>
<th>a_part</th>
<th>b_id</th>
<th>b_name</th>
<th>b_part</th>
</tr>
</thead>
<tbody>
<tr>
<td>2</td>
<td>老王</td>
<td>秘书部</td>
<td>2</td>
<td>老王</td>
<td>秘书部</td>
</tr>
<tr>
<td>3</td>
<td>老张</td>
<td>设计部</td>
<td>3</td>
<td>老张</td>
<td>设计部</td>
</tr>
</tbody>
</table>
</div>
<p>sql语句有</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> </span><br><span class="line"> A <span class="keyword">inner</span> <span class="keyword">join</span> B <span class="keyword">on</span> A.a_id=B.b_id</span><br></pre></td></tr></table></figure>
<h2 id="左连接"><a href="#左连接" class="headerlink" title="左连接"></a>左连接</h2><p>我们表A和表B，我们对表A进行左连接于表B，则有</p>
<center>表D</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">a_id</th>
<th style="text-align:center">a_name</th>
<th style="text-align:center">a_part</th>
<th style="text-align:center">b_id</th>
<th style="text-align:center">b_name</th>
<th style="text-align:center">b_part</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">老王</td>
<td style="text-align:center">秘书部</td>
<td style="text-align:center">2</td>
<td style="text-align:center">老王</td>
<td style="text-align:center">秘书部</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">老张</td>
<td style="text-align:center">设计部</td>
<td style="text-align:center">3</td>
<td style="text-align:center">老张</td>
<td style="text-align:center">设计部</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">老潘</td>
<td style="text-align:center">总裁部</td>
<td style="text-align:center">null</td>
<td style="text-align:center">null</td>
<td style="text-align:center">null</td>
</tr>
<tr>
<td style="text-align:center">4</td>
<td style="text-align:center">老李</td>
<td style="text-align:center">运营部</td>
<td style="text-align:center">null</td>
<td style="text-align:center">null</td>
<td style="text-align:center">null</td>
</tr>
</tbody>
</table>
</div>
<p>有SQL语句</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> A <span class="keyword">left</span> <span class="keyword">join</span> B <span class="keyword">on</span> A.a_id=B.b_id</span><br></pre></td></tr></table></figure>
<p>左(外)连接，左表A的记录将会全部表示出来，而右B只会显示符合搜索条件的记录。右表记录不足的地方均为NULL。</p>
<h2 id="右连接"><a href="#右连接" class="headerlink" title="右连接"></a>右连接</h2><p>我们表A和表B，我们对表A进行右连接于表B，则有</p>
<center>表E</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">a_id</th>
<th style="text-align:center">a_name</th>
<th style="text-align:center">a_part</th>
<th style="text-align:center">b_id</th>
<th style="text-align:center">b_name</th>
<th style="text-align:center">b_par</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">老王</td>
<td style="text-align:center">秘书部</td>
<td style="text-align:center">2</td>
<td style="text-align:center">老王</td>
<td style="text-align:center">秘书部</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">老张</td>
<td style="text-align:center">设计部</td>
<td style="text-align:center">3</td>
<td style="text-align:center">老张</td>
<td style="text-align:center">设计部</td>
</tr>
<tr>
<td style="text-align:center">null</td>
<td style="text-align:center">null</td>
<td style="text-align:center">null</td>
<td style="text-align:center">5</td>
<td style="text-align:center">老刘</td>
<td style="text-align:center">人事部</td>
</tr>
<tr>
<td style="text-align:center">null</td>
<td style="text-align:center">null</td>
<td style="text-align:center">null</td>
<td style="text-align:center">6</td>
<td style="text-align:center">老黄</td>
<td style="text-align:center">生产部</td>
</tr>
</tbody>
</table>
</div>
<h2 id="自然连接"><a href="#自然连接" class="headerlink" title="自然连接"></a>自然连接</h2><p>无论何时对表进行联结，应该至少有一个列出现在不止一个表中（被联结的列)。</p>
<font color="red">自然联结排除多次出现，使每个列只返回一次 ，也就是说，自然连接是去掉重复列的等值连接.</font>

<center>关系R</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">A</th>
<th style="text-align:center">B</th>
<th style="text-align:center">C</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">a1</td>
<td style="text-align:center">b1</td>
<td style="text-align:center">5</td>
</tr>
<tr>
<td style="text-align:center">a1</td>
<td style="text-align:center">b1</td>
<td style="text-align:center">6</td>
</tr>
<tr>
<td style="text-align:center">a2</td>
<td style="text-align:center">b2</td>
<td style="text-align:center">8</td>
</tr>
<tr>
<td style="text-align:center">a2</td>
<td style="text-align:center">b2</td>
<td style="text-align:center">12</td>
</tr>
</tbody>
</table>
</div>
<center>关系S</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">B</th>
<th style="text-align:center">D</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">b1</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">b2</td>
<td style="text-align:center">7</td>
</tr>
<tr>
<td style="text-align:center">b3</td>
<td style="text-align:center">10</td>
</tr>
<tr>
<td style="text-align:center">b3</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">b4</td>
<td style="text-align:center">5</td>
</tr>
</tbody>
</table>
</div>
<p>下面自然连接关系R$\infty$S</p>
<p> R$\infty$S </p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">A</th>
<th style="text-align:center">B</th>
<th style="text-align:center">C</th>
<th style="text-align:center">D</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">a1</td>
<td style="text-align:center">b1</td>
<td style="text-align:center">5</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">a1</td>
<td style="text-align:center">b1</td>
<td style="text-align:center">6</td>
<td style="text-align:center">3</td>
</tr>
<tr>
<td style="text-align:center">a2</td>
<td style="text-align:center">b2</td>
<td style="text-align:center">8</td>
<td style="text-align:center">7</td>
</tr>
<tr>
<td style="text-align:center">a2</td>
<td style="text-align:center">b2</td>
<td style="text-align:center">12</td>
<td style="text-align:center">7</td>
</tr>
</tbody>
</table>
</div>
<p>有SQL语法</p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">select c.*,o<span class="selector-class">.order_num</span>,o<span class="selector-class">.order_date</span>,oi<span class="selector-class">.prod_id</span>,oi<span class="selector-class">.quantity</span>,oi<span class="selector-class">.item_price</span></span><br><span class="line">from customers as c,orders as o,orderitems as oi</span><br><span class="line">where c<span class="selector-class">.cust_id</span> =o<span class="selector-class">.cust_id</span> and oi<span class="selector-class">.order_num</span> =o<span class="selector-class">.order_num</span> and prod_id=<span class="string">&#x27;FB&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="组合查询"><a href="#组合查询" class="headerlink" title="组合查询"></a>组合查询</h1><h2 id="union"><a href="#union" class="headerlink" title="union"></a>union</h2><p>假如需要价格小于等于5的所有物品的一个列表，而且还想包括供应商1001和1002生产的所有物品（不考虑价格）。  </p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> vend_id,prod_id,prod_price</span><br><span class="line"><span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> prod_price &lt;=<span class="number">5</span></span><br><span class="line"><span class="keyword">union</span> </span><br><span class="line"><span class="keyword">select</span> vend_id,prod_id,prod_price</span><br><span class="line"><span class="keyword">from</span> products</span><br><span class="line"><span class="keyword">where</span> vend_id <span class="keyword">in</span> (<span class="number">1001</span>,<span class="number">1002</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327155055270.png" alt="image-20220327155055270"></p>
<p>我们这里也有使用OR来进行连接，而产生的查询语句</p>
<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">select</span> vend_id,prod_id,prod_price</span><br><span class="line"><span class="attribute">from</span> products</span><br><span class="line"><span class="attribute">where</span> prod_price &lt;=<span class="number">5</span> or vend_id in (<span class="number">1001</span>,<span class="number">1002</span>)</span><br></pre></td></tr></table></figure>
<font color="red">注意</font>

<p>我们需要对比是使用union与or、and等操作符进行分析，看再场景下哪个操作更加高效。</p>
<p>UNION中的每个查询必须包含相同的列、表达式或聚集函数  。</p>
<h2 id="union-all"><a href="#union-all" class="headerlink" title="union all"></a>union all</h2><p>union操作符会默认，取消重复的行，而使用union all 都返回所有匹配的行，包括重复的行。</p>
<h1 id="全文本搜索"><a href="#全文本搜索" class="headerlink" title="全文本搜索"></a>全文本搜索</h1><p>全文本搜索相较于like关键字与正则表达式有都以下三点优势</p>
<p>1)性能更加高效</p>
<p>2)能明确控制匹配什么词，不匹配什么词</p>
<p>3)智能化结果。</p>
<h2 id="启用全文本搜索支持"><a href="#启用全文本搜索支持" class="headerlink" title="启用全文本搜索支持"></a>启用全文本搜索支持</h2><p>全文本搜索支持需要在创建表时启用，有SQL语句</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> productnotes</span><br><span class="line">(</span><br><span class="line">note_id <span class="type">int</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span> AUTO_INCREMENT,</span><br><span class="line">prod_id <span class="type">char</span>(<span class="number">10</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">note_date datetime <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">note_text <span class="type">text</span> <span class="keyword">NULL</span> ,</span><br><span class="line"><span class="keyword">PRIMARY KEY</span> (note_id),</span><br><span class="line">FULLTEXT(note_text)//note_text列启用全文本搜索支持</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>为了进行全文本搜索，MySQL根据子句FULLTEXT(note_text)的指示对它进行索引。这里的FULLTEXT索引单个列，如果需要也可以指定多个列。  </p>
<p>在定义之后， MySQL自动维护该索引。在增加、更新或删除行时，索引随之自动更新  。</p>
<h2 id="进行全文本搜索"><a href="#进行全文本搜索" class="headerlink" title="进行全文本搜索"></a>进行全文本搜索</h2><p>在索引之后，使用两个函数Match()和Against()执行全文本搜索，其中Match()指定被搜索的列， Against()指定要使用的搜索表达式 。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text</span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">Match</span>(note_text) Against(<span class="string">&#x27;rabbit&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327163015273.png" alt="image-20220327163015273"></p>
<p>此SELECT语句检索单个列note_text。由于WHERE子句，一个全文本搜索被执行。 Match(note_text)指示MySQL针对指定的列进行搜索， Against(‘rabbit’)指定词rabbit作为搜索文本。由于有两行包含词rabbit，这两个行被返回。  </p>
<figure class="highlight axapta"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text</span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> note_text <span class="keyword">like</span> <span class="string">&#x27;%rabbit&#x27;</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327163409474.png" alt="image-20220327163409474"></p>
<font color="red">注意</font>

<p>上两条SQL语句检索的内容相同，但是其有本质的区别，</p>
<p>后者（使用LIKE）以不特别有用的顺序返回数据。前者（使用全文本搜索）返回以文本匹配  的良好程度排序的数据。<font color="red">两个行都包含词rabbit，但包含词rabbit作为第3个词的行的等级比作为第20个词的行高</font>。  全文本搜索的一<br>个重要部分就是对结果排序。具有较高等级的行先返回（因为这些行很可能是你真正想要的行）。  </p>
<h2 id="使用查询扩展"><a href="#使用查询扩展" class="headerlink" title="使用查询扩展"></a>使用查询扩展</h2><p>查询扩展用来设法放宽所返回的全文本搜索结果的范围。 </p>
<p>在使用查询扩展时， MySQL对数据和索引进行两遍扫描来完成搜索  ：</p>
<p>1)首先，进行一个基本的全文本搜索，找出与搜索条件匹配的所有行。</p>
<p>2)其次，MySql检查这些匹配行并选择所有有用的词。</p>
<p>3)再其次，MySQL再次进行全文本搜索，这次不仅使用原来的条件，而且还使用所有有用的词。  </p>
<p>下面语句，首先进行一个简单的全文本搜索，没有查询扩展。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text</span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) Against(<span class="string">&#x27;anvils&#x27;</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327164606338.png" alt="image-20220327164606338"></p>
<p>只有一行包含词anvils，因此只返回一行。</p>
<p>使用查询扩展的SQL语句</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text</span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) against(<span class="string">&#x27;anvils&#x27;</span> <span class="keyword">with</span> query expension)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327164817158.png" alt="image-20220327164817158"></p>
<p>这次返回了7行。第一行包含词anvils，因此等级最高。第二行与anvils无关，但因为它包含第一行中的两个词（ customer和recommend），所以也被检索出来。第3行也包含这两个相同的词，但它们在文本中的位置更靠后且分开得更远，因此也包含这一行，但等级为第三。第三行确实也没有涉及anvils（按它们的产品名。  </p>
<p>表中的行越多（这些行中的文本就越多），使用查询扩展返回的结果越好。  </p>
<h2 id="布尔文本搜索"><a href="#布尔文本搜索" class="headerlink" title="布尔文本搜索"></a>布尔文本搜索</h2><p>以布尔方式，可以提供关于如下内容  </p>
<p>1、要匹配的词；<br>2、要排斥的词（如果某行包含这个词，则不返回该行，即使它包含其他指定的词也是如此）；<br>3、排列提示（指定某些词比其他词更重要，更重要的词等级更高）；<br>4、表达式分组；<br>5、另外一些内容。  </p>
<font color="red">布尔文本搜索，即 使 没 有 定 义
FULLTEXT索引，也可以使用它。  </font>

<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text</span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) against(<span class="string">&#x27;heavy&#x27;</span> <span class="keyword">in</span> <span class="type">boolean</span> mode )</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327170414219.png" alt="image-20220327170414219"></p>
<p>此全文本搜索检索包含词heavy的所有行（有两行）。其中使用了关键字IN BOOLEAN MODE。</p>
<p>为了匹配包含heavy但不包含任意以rope开始的词的行，可以使用以下查询 </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text </span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) against(<span class="string">&#x27;heavy -rope*&#x27;</span> <span class="keyword">in</span> <span class="type">boolean</span> mode)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220327181137210.png" alt="image-20220327181137210"></p>
<center>全文本布尔操作符</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">布尔操作符</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">+</td>
<td style="text-align:center">包含，词必须存在</td>
</tr>
<tr>
<td style="text-align:center">-</td>
<td style="text-align:center">排除，词必须不出现</td>
</tr>
<tr>
<td style="text-align:center">&gt;</td>
<td style="text-align:center">包含，而且增加等级值</td>
</tr>
<tr>
<td style="text-align:center">&lt;</td>
<td style="text-align:center">包含，且减少等级值</td>
</tr>
<tr>
<td style="text-align:center">()</td>
<td style="text-align:center">把词组成子表达式（允许这些子表达式作为一个组被包含、 排除、排列等）</td>
</tr>
<tr>
<td style="text-align:center">~</td>
<td style="text-align:center">取消一个词的排序值</td>
</tr>
<tr>
<td style="text-align:center">*</td>
<td style="text-align:center">词尾的通配符</td>
</tr>
<tr>
<td style="text-align:center">“ “</td>
<td style="text-align:center">定义一个短语（与单个词的列表不一样，它匹配整个短语以 便包含或排除这个短语）</td>
</tr>
</tbody>
</table>
</div>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text</span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) against(<span class="string">&#x27;+rabbit +bait&#x27;</span> <span class="keyword">in</span> <span class="type">boolean</span> mode)</span><br></pre></td></tr></table></figure>
<p>搜索匹配包含词rabbit和bait的行。  </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text </span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) against(<span class="string">&#x27;rabbit bait&#x27;</span> <span class="keyword">in</span> <span class="type">boolean</span> mode)</span><br></pre></td></tr></table></figure>
<p>搜索匹配包含rabbit和bait中的至少一个词的行 。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text</span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) against(<span class="string">&#x27;&quot;rabbit bait&quot;&#x27;</span> <span class="keyword">in</span> <span class="type">boolean</span> mode)</span><br></pre></td></tr></table></figure>
<p>搜索匹配短语rabbit bait而不是匹配两个词rabbit和bait。  </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text </span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) against(<span class="string">&#x27;&gt;rabbit &lt;carrot&#x27;</span> <span class="keyword">in</span> <span class="type">boolean</span> mode)</span><br></pre></td></tr></table></figure>
<p>匹配rabbit和carrot，增加前者的等级，降低后者的等级。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> note_text</span><br><span class="line"><span class="keyword">from</span> productnotes</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">match</span>(note_text) against(<span class="string">&#x27;+safe +(&lt;combination)&#x27;</span> <span class="keyword">in</span> <span class="type">boolean</span> mode)</span><br></pre></td></tr></table></figure>
<p>搜索匹配词safe和combination，降低后者的等级 。</p>
<h1 id="插入数据"><a href="#插入数据" class="headerlink" title="插入数据"></a>插入数据</h1><font color="red">省略列</font>

<p>省略列必须满足以下某个条件</p>
<p>1)该列定义为允许NULL值(无值或空值)</p>
<p>2)在表定义中给出默认值，表示如果不给出值，则将使用默认值。</p>
<font color="red">提高整体性能</font>

<p>数据库经常被多个客户访问，对处理什么请求以及用什么次序处理进行管理是MySQL的任务。 INSERT操作可能很耗时（特别是有很多索引需要更新时），而且它可能降低等待处理的SELECT语句的性能。  </p>
<p>如果数据检索是最重要的，则你可以通过在INSERT和INTO之间添加关键字LOW_PRIORITY，指示MySQL降低INSERT语句的优先级，有SQL代码</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> LOW_PRIORITY <span class="keyword">INTO</span></span><br></pre></td></tr></table></figure>
<p>该代码表示降低插入数据的级别，update与delete语句也适用。</p>
<p>若每条的insert语句中的列名(和次序)相同，可如下组合各语句</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> customers(cust_name,cust_address,cust_city,cust_state,cust_zip,cust_country) <span class="keyword">values</span>(<span class="string">&#x27;Pep E.LawPew&#x27;</span>,<span class="string">&#x27;100 Main Street&#x27;</span>,<span class="string">&#x27;Los Angeles&#x27;</span>,<span class="string">&#x27;CA&#x27;</span>,<span class="string">&#x27;90046&#x27;</span>,<span class="string">&#x27;USA&#x27;</span>),(<span class="string">&#x27;M. Martian&#x27;</span>,<span class="string">&#x27;42 Galaxy Way&#x27;</span>,<span class="string">&#x27;New York&#x27;</span>,<span class="string">&#x27;NY&#x27;</span>,<span class="string">&#x27;112213&#x27;</span>,<span class="string">&#x27;USA&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="更新与删除数据"><a href="#更新与删除数据" class="headerlink" title="更新与删除数据"></a>更新与删除数据</h1><font color="red">IGNORE关键字</font>

<p>如果用UPDATE语句更新多行，并且在更新这些行中的一行或多行时出一个现错误，则整个UPDATE操作被取消<br>（错误发生前更新的所有行被恢复到它们原来的值）。为了即使是发生错误，也继续进行更新，可使用IGNORE关键字。</p>
<figure class="highlight gams"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">update</span></span> ignore customers ....</span><br></pre></td></tr></table></figure>
<p>为了删除某列的值，可以设置它为NULL(若表定义允许设置为NULL值)</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">update customers</span><br><span class="line"><span class="built_in">set</span> <span class="attribute">cust_email</span>=<span class="literal">NULL</span></span><br><span class="line">where <span class="attribute">cust_id</span>=1005</span><br></pre></td></tr></table></figure>
<p>其中NULL用来去除cust_email列中的值。  </p>
<font color="red">删除表的内容而不是表</font>

<p>DELETE语句从表中删除行，甚至是删除表中所有行。但是， DELETE不删除表本身。  </p>
<font color="red">更快的删除</font>

<p>如果想从表中删除所有行，不要使用DELETE。可使用TRUNCATE TABLE语句，它完成相同的工作，但速度更快（ TRUNCATE实际是删除原来的表并重新创建一个表，而不是逐行删除表中的数据）。  </p>
<h1 id="创建与操纵表"><a href="#创建与操纵表" class="headerlink" title="创建与操纵表"></a>创建与操纵表</h1><font color="red">理解NULL</font>

<p>不要把NULL值与空串相混淆。 NULL值是没有值，它不是空串。如果指定’’（两个单引号，其间没有字符），这在NOT NULL列中是允许的。空串是一个有效的值，它不是无值。 NULL值用关键字NULL而不是空串指定。  </p>
<font color="red">覆盖Auto_increment</font>

<p>如果一个列被指定为AUTO_INCREMENT，则它需要使用特殊的值吗？  </p>
<p>可以简单地在INSERT语句中指定一个值，只要它是唯一的（至今尚未使用过）即可，该值将被用来替代自动生成的值。后续的增量将开始使用该手工插入的值。</p>
<font color="red">确定auto_increment值</font>

<p>让MySQL生成（通过自动增量）主键的一个缺点是你不知道这些值都是谁 。</p>
<p>考虑这个场景：你正在增加一个新订单。这要求在orders表中创建一行， 然后在orderitms表中对订购的每项物品创建一行。 order_num在orderitems表中与订单细节一起存储。这就是为什么orders表和orderitems表为相互关联的表的原因。这显然要求你在插入orders行之后，插入orderitems行之前知道生成的order_num。  </p>
<p>可使用last_insert_id()函数获得这个值。</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">select</span> <span class="title">last_insert_id</span>()</span></span><br></pre></td></tr></table></figure>
<p>此语句返回最后一个AUTO_INCREMENT值。</p>
<h2 id="更新表"><a href="#更新表" class="headerlink" title="更新表"></a>更新表</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> vendors</span><br><span class="line"><span class="keyword">add</span> vend_phone <span class="type">char</span>(<span class="number">20</span>)</span><br></pre></td></tr></table></figure>
<p>这条语句给vendors表增加一个名为vend_phone的列，必须明确其数据类型。  </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> vendors</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">column</span> vend_phone</span><br></pre></td></tr></table></figure>
<p>删除刚刚添加的列。</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> orderitems</span><br><span class="line"><span class="keyword">add</span> <span class="keyword">constraint</span> fk_orderitems_orders</span><br><span class="line"><span class="keyword">foreign key</span> (order_num) <span class="keyword">references</span> orders(order_num)</span><br></pre></td></tr></table></figure>
<p>添加外键。</p>
<h2 id="删除表"><a href="#删除表" class="headerlink" title="删除表"></a>删除表</h2><figure class="highlight pf"><table><tr><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="built_in">table</span> customers2</span><br></pre></td></tr></table></figure>
<p>删除表customers_2表</p>
<h2 id="重命名表"><a href="#重命名表" class="headerlink" title="重命名表"></a>重命名表</h2><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">rename</span> <span class="keyword">table</span> customers2 <span class="keyword">to</span> customers</span><br></pre></td></tr></table></figure>
<p>重命名表</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">rename</span> <span class="keyword">table</span> backup_customers <span class="keyword">to</span> customers,</span><br><span class="line">			 backup_vendors <span class="keyword">to</span> vendors,</span><br><span class="line">			 backup_products <span class="keyword">to</span> products</span><br></pre></td></tr></table></figure>
<p>多个表重命名。</p>
<h1 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h1><font color="red">注意</font>

<p>1)视图用create view 语句来创建</p>
<p>2)使用show create view viewname，来查看创建视图</p>
<p>3)用drop删除视图，语法为drop view viewname</p>
<p>4)更新视图时，可以先使用drop再使用create，也可以直接使用create or replace view ，如果要更新的视图不存在，则第2条更新语句会创建一个视图；如果要更新的视图存在，则第2条更新语句会替换原有视图。  </p>
<p>创建视图语法</p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">view</span> productcustomers <span class="keyword">as</span> </span><br><span class="line"><span class="keyword">select</span> cust_name,cust_contact,prod_id</span><br><span class="line"><span class="keyword">from</span> customers,orders,orderitems</span><br><span class="line"><span class="keyword">where</span> customers.cust_id=orders.cust_id <span class="keyword">and</span> orderitems.order_num=<span class="keyword">order</span>.order_num</span><br></pre></td></tr></table></figure>
<p>这条语句创建一个名为productcustomers的视图， 它联结三个表，以返回已订购了任意产品的所有客户的列表。  </p>
<h2 id="更新视图"><a href="#更新视图" class="headerlink" title="更新视图"></a>更新视图</h2><p>通常，视图是可更新的（即，可以对它们使用INSERT、 UPDATE和DELETE）。更新一个视图将更新其基表，如果你对视图增加或删除行，实际上是对其基表增加或删除行。</p>
<p>但是，并非所有视图都是可更新的。基本上可以说，如果MySQL不能正确地确定被更新的基数据，则不允许更新（包括插入和删除）。这实际上意味着，如果视图定义中有以下操作，则不能进行视图的更新：</p>
<p>1)分组（使用GROUP BY和HAVING）；<br>2)联结；<br>3)子查询；<br>4)并；<br>5)聚集函数（ Min()、 Count()、 Sum()等）；  </p>
<p>6)DISTINCT；<br>7)导出（计算）列。  </p>
<h1 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h1><h2 id="执行存储过程"><a href="#执行存储过程" class="headerlink" title="执行存储过程"></a>执行存储过程</h2><figure class="highlight less"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">call</span> <span class="selector-tag">productpricing</span>(<span class="variable">@pricelow</span>,<span class="variable">@pricehigh</span>,<span class="variable">@priceaverage</span>)</span><br></pre></td></tr></table></figure>
<p>执行名为productpricing的存储过程。</p>
<h2 id="创建存储过程"><a href="#创建存储过程" class="headerlink" title="创建存储过程"></a>创建存储过程</h2><figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> <span class="title function_">productpricing</span><span class="params">()</span></span><br><span class="line"><span class="title function_">begin</span></span><br><span class="line"><span class="title function_">select</span> <span class="title function_">avg</span><span class="params">(prod_price)</span> <span class="title function_">as</span> <span class="title function_">priceaverage</span></span><br><span class="line"><span class="title function_">from</span> <span class="title function_">products</span>;</span><br><span class="line"><span class="keyword">end</span><span class="punctuation">;</span></span><br></pre></td></tr></table></figure>
<p>一个返回产品平均价格的存储过程。  </p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="function">call <span class="title">productpricing</span>()</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220328120319499.png" alt="image-20220328120319499"></p>
<h2 id="删除存储过程"><a href="#删除存储过程" class="headerlink" title="删除存储过程"></a>删除存储过程</h2><figure class="highlight delphi"><table><tr><td class="code"><pre><span class="line">drop <span class="function"><span class="keyword">procedure</span> <span class="title">productpricing</span>;</span></span><br></pre></td></tr></table></figure>
<p>这条语句删除刚创建的存储过程。请注意没有使用后面的()，只给出存储过程名。  </p>
<h2 id="使用参数"><a href="#使用参数" class="headerlink" title="使用参数"></a>使用参数</h2><figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> <span class="title function_">productpricing</span><span class="params">(</span></span><br><span class="line"><span class="params"><span class="keyword">out</span> p1 decimal(8,2)</span>,</span><br><span class="line"><span class="title function_">out</span> <span class="title function_">ph</span> <span class="title function_">decimal</span><span class="params">(8,2)</span>,</span><br><span class="line"><span class="title function_">out</span> <span class="title function_">pa</span> <span class="title function_">decimal</span><span class="params">(8,2)</span></span><br><span class="line">)</span><br><span class="line"><span class="title function_">begin</span></span><br><span class="line"> <span class="title function_">select</span> <span class="title function_">min</span><span class="params">(prod_price)</span> <span class="title function_">into</span> <span class="title function_">p1</span></span><br><span class="line"> <span class="title function_">from</span> <span class="title function_">products</span>;</span><br><span class="line"> <span class="keyword">select</span> max(prod_price) <span class="keyword">into</span> ph</span><br><span class="line"> <span class="keyword">from</span> products<span class="punctuation">;</span></span><br><span class="line"> <span class="keyword">select</span> avg(prod_price) <span class="keyword">into</span> pa</span><br><span class="line"> <span class="keyword">from</span> products<span class="punctuation">;</span></span><br><span class="line"> <span class="keyword">end</span><span class="punctuation">;</span></span><br></pre></td></tr></table></figure>
<p>此存储过程接受3个参数： pl存储产品最低价格， ph存储产品最高价格， pa存储产品平均价格。每个参数必须具有指定的类型，这里使用十进制值。关键字OUT指出相应的参数用来从存储过程传出一个值（返回给调用者）。   MySQL支持IN（传递给存储过程）、 OUT（从存储过程传出，如这里所用）和INOUT（对存储过程传入和传出）类型的参数。  </p>
<p>考虑这个场景。你需要获得与以前一样的订单合计，但需要对合计增加营业税，不过只针对某些顾客（或许是你所在州中那些顾客）。那么，你需要做下面几件事情：  </p>
<ol>
<li>获得合计</li>
<li>把营业税有条件地添加到合计</li>
<li>返回合计</li>
</ol>
<figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> <span class="title function_">ordertotal</span><span class="params">(</span></span><br><span class="line"><span class="params"><span class="keyword">in</span> onumber int,</span></span><br><span class="line"><span class="params"><span class="keyword">in</span> taxable boolean,</span></span><br><span class="line"><span class="params"><span class="keyword">out</span> ototal decimal(8,2)</span></span><br><span class="line">)</span><br><span class="line"><span class="title function_">begin</span></span><br><span class="line">-- <span class="title function_">declare</span> <span class="title function_">variable</span> <span class="title function_">for</span> <span class="title function_">total</span> </span><br><span class="line"><span class="title function_">declare</span> <span class="title function_">total</span> <span class="title function_">decimal</span><span class="params">(8,2)</span>;</span><br><span class="line">-- declare tax percentage</span><br><span class="line">declare taxrate int <span class="keyword">default</span> <span class="number">6</span><span class="punctuation">;</span></span><br><span class="line"></span><br><span class="line">-- get the <span class="keyword">order</span> total </span><br><span class="line"><span class="keyword">select</span> sum(item_price*quantity)</span><br><span class="line"><span class="keyword">from</span> orderitems</span><br><span class="line"><span class="keyword">where</span> order_num=onumber</span><br><span class="line"><span class="keyword">into</span> total<span class="punctuation">;</span></span><br><span class="line"></span><br><span class="line">--<span class="keyword">is</span> this taxable?</span><br><span class="line"><span class="keyword">if</span> taxable <span class="keyword">then</span></span><br><span class="line">--yes,so <span class="keyword">add</span> taxrate <span class="keyword">to</span> the total</span><br><span class="line"><span class="keyword">select</span> total+(total/<span class="number">100</span>*taxrate) <span class="keyword">into</span> total<span class="punctuation">;</span></span><br><span class="line"><span class="keyword">end</span> <span class="keyword">if</span><span class="punctuation">;</span></span><br><span class="line">--<span class="keyword">and</span> <span class="keyword">finally</span>,save <span class="keyword">to</span> <span class="keyword">out</span> variable</span><br><span class="line"><span class="keyword">select</span> total <span class="keyword">into</span> ototal<span class="punctuation">;</span></span><br><span class="line"><span class="keyword">end</span><span class="punctuation">;</span></span><br></pre></td></tr></table></figure>
<h2 id="检查存储过程"><a href="#检查存储过程" class="headerlink" title="检查存储过程"></a>检查存储过程</h2><figure class="highlight oxygene"><table><tr><td class="code"><pre><span class="line">show <span class="keyword">create</span> <span class="keyword">procedure</span> <span class="title function_">ordertotal</span>;</span><br></pre></td></tr></table></figure>
<p>显示用来创建一个存储过程的CREATE语句，使用SHOW CREATEPROCEDURE语句 。</p>
<p>为了获得包括何时、由谁创建等详细信息的存储过程列表，使用show procedure status。</p>
<font color="red">限制过程状态结果</font>

<p>show procedure status列出所有存储过程，为限制其输出，可使用like指定一个过滤模式，例如：</p>
<figure class="highlight delphi"><table><tr><td class="code"><pre><span class="line">show <span class="function"><span class="keyword">procedure</span> <span class="title">status</span> <span class="title">like</span> &#x27;<span class="title">ordertotal</span>&#x27;</span></span><br></pre></td></tr></table></figure>
<h1 id="游标"><a href="#游标" class="headerlink" title="游标"></a>游标</h1><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> processorders()</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line"><span class="comment">--declare local variables</span></span><br><span class="line"><span class="keyword">DECLARE</span> o <span class="type">int</span>;</span><br><span class="line"><span class="keyword">DECLARE</span> done <span class="type">boolean</span> <span class="keyword">default</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--declare ordernumbers cursor</span></span><br><span class="line"><span class="keyword">declare</span> ordernumbers <span class="keyword">cursor</span></span><br><span class="line"><span class="keyword">for</span></span><br><span class="line"><span class="keyword">select</span> order_num <span class="keyword">from</span> orders;</span><br><span class="line"><span class="comment">--declare continue handler</span></span><br><span class="line"><span class="keyword">DECLARE</span> <span class="keyword">continue</span> <span class="keyword">handler</span> <span class="keyword">for</span></span><br><span class="line"><span class="built_in">sqlstate</span> <span class="string">&#x27;02000&#x27;</span> <span class="keyword">set</span> done=<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">--open the cursor</span></span><br><span class="line"><span class="keyword">open</span> ordernumbers;</span><br><span class="line"></span><br><span class="line"><span class="comment">--loop through all rows</span></span><br><span class="line">repeat</span><br><span class="line"></span><br><span class="line"><span class="comment">--get order number</span></span><br><span class="line"><span class="keyword">fetch</span> ordernumbers <span class="keyword">into</span> o;</span><br><span class="line"><span class="keyword">select</span> o;</span><br><span class="line"><span class="comment">--end of loop </span></span><br><span class="line"><span class="keyword">until</span> done <span class="keyword">end</span> repeat;</span><br><span class="line"><span class="comment">--close the cursor</span></span><br><span class="line"><span class="keyword">close</span> ordernumbers;</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220328165207698.png" alt="image-20220328165207698"></p>
<p>例子中的FETCH是在REPEAT内，因此它反复执行直到done为真（由UNTILdone END REPEAT;规定）。  </p>
<p>这条语句定义了一个CONTINUE HANDLER，它是在条件出现时被执行的代码。 这里， 它指出当SQLSTATE ‘02000’出现时， SET done=1。 SQLSTATE’02000’是一个未找到条件， 当REPEAT由于没有更多的行供循环而不能继续时，出现这个条件  。</p>
<h1 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h1><p>触发器是MySQL响应以下任意语句而自动执行的一条MySQL语句（或位于BEGIN和END语句之间的一组语句）：  </p>
<ol>
<li>delete</li>
<li>insert</li>
<li>update</li>
</ol>
<h2 id="创建触发器"><a href="#创建触发器" class="headerlink" title="创建触发器"></a>创建触发器</h2><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">trigger</span> newproduct <span class="keyword">after</span> <span class="keyword">insert</span> <span class="keyword">on</span> products</span><br><span class="line"><span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">row</span> <span class="keyword">select</span> <span class="string">&#x27;product added&#x27;</span>;</span><br></pre></td></tr></table></figure>
<p>CREATE TRIGGER用来创建名为newproduct的新触发器。触发器可在一个操作发生之前或之后执行，这里给出了AFTER INSERT，所以此触发器将在INSERT语句成功执行后执行。这个触发器还指定FOREACH ROW，因此代码对每个插入行执行。在这个例子中，文本Productadded将对每个插入的行显示一次。  </p>
<font color="red">触发器仅支持表</font>

<p>只有表才支持触发器，视图不支持(临时表也不支持)。</p>
<p>每个表最多支持6个触发器（每条INSERT、 UPDATE和DELETE的之前和之后）。  </p>
<font color="red">触发器失败</font>

<p>如果BEFORE触发器失败，则MySQL将不执行请求的操作。此外，如果BEFORE触发器或语句本身失败， MySQL将不执行AFTER触发器（如果有的话)。</p>
<h2 id="删除触发器"><a href="#删除触发器" class="headerlink" title="删除触发器"></a>删除触发器</h2><figure class="highlight haxe"><table><tr><td class="code"><pre><span class="line">drop trigger <span class="keyword">new</span><span class="type">product</span>;</span><br></pre></td></tr></table></figure>
<p>触发器不能更新或覆盖。为了修改一个触发器，必须先删除它，然后再重新创建。  </p>
<h2 id="insert触发器"><a href="#insert触发器" class="headerlink" title="insert触发器"></a>insert触发器</h2><p>INSERT触发器在INSERT语句执行之前或之后执行。需要知道以下几点：  </p>
<ol>
<li>在insert触发器代码内，可引用一个名为new的虚拟表，访问被 插入的行</li>
<li>在before insert触发器中，new中的值也可以被更新(允许更改被插入的值)；</li>
<li>对于auto_increment列，new在insert执行之前包含0,在insert执行之后包含新的自动生成值。</li>
</ol>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">declare</span> t <span class="type">decimal</span>(<span class="number">8</span>,<span class="number">2</span>);</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">trigger</span> neworder <span class="keyword">after</span> <span class="keyword">insert</span> <span class="keyword">on</span> orders</span><br><span class="line"><span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">row</span> <span class="keyword">set</span> @t=<span class="built_in">NEW</span>.order_num;</span><br></pre></td></tr></table></figure>
<p>此代码创建一个名为neworder的触发器，它按照AFTER INSERTON orders执行。在插入一个新订单到orders表时， MySQL生成一个新订单号并保存到order_num中。触发器从NEW. order_num取得这个值并赋值给t。此触发器必须按照AFTER INSERT执行，因为在BEFOREINSERT语句执行之前，新order_num还没有生成。  </p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line">insert into <span class="built_in">orders</span>(order_date,cust_id) <span class="built_in">values</span>(Now(),<span class="number">10001</span>);</span><br><span class="line">select <span class="keyword">@t</span>;</span><br></pre></td></tr></table></figure>
<p>进行插入而后触发触发器，我们再通过select把值给打印出来。</p>
<h2 id="delete触发器"><a href="#delete触发器" class="headerlink" title="delete触发器"></a>delete触发器</h2><p>DELETE触发器在DELETE语句执行之前或之后执行。需要知道以下两点：  </p>
<ol>
<li>在delete触发器代码内你可以引用一个名为OLD的虚拟表，访问被删除的行；  </li>
<li>OLD中的值全都是只读的，不能更新。</li>
</ol>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">trigger</span> deleteorder <span class="keyword">before</span> <span class="keyword">delete</span> <span class="keyword">on</span> orders</span><br><span class="line"><span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">row</span></span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> archive_orders(order_num,order_date,cust_id) <span class="keyword">values</span>(<span class="built_in">old</span>.order_num,<span class="built_in">old</span>.order_date,<span class="built_in">old</span>.cust_id);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure>
<p>在任意订单被删除前将执行此触发器。它使用一条INSERT语句将OLD中的值（要被删除的订单）保存到一个名为archive_orders的存档表中。</p>
<h2 id="update触发器"><a href="#update触发器" class="headerlink" title="update触发器"></a>update触发器</h2><p>UPDATE触发器在UPDATE语句执行之前或之后执行。需要知道以下几点：  </p>
<ol>
<li>在UPDATE触发器代码中，你可以引用一个名为OLD的虚拟表访问以前（ UPDATE语句前）的值，引用一个名为NEW的虚拟表访问新更新的值。</li>
<li>在BEFORE UPDATE触发器中， NEW中的值可能也被更新。</li>
<li>OLD中的值全都是只读的，不能更新 。</li>
</ol>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">trigger</span> updatevendor <span class="keyword">before</span> <span class="keyword">update</span> <span class="keyword">on</span> vendors</span><br><span class="line"><span class="keyword">for</span> <span class="keyword">each</span> <span class="keyword">row</span> </span><br><span class="line"><span class="keyword">set</span> <span class="built_in">NEW</span>.vend_state=Upper(<span class="built_in">NEW</span>.vend_state);</span><br></pre></td></tr></table></figure>
<h1 id="管理事务处理"><a href="#管理事务处理" class="headerlink" title="管理事务处理"></a>管理事务处理</h1><ol>
<li><strong>事务</strong>指一组SQL语句。</li>
<li><strong>回退</strong>指撤销指定SQL语句的过程。</li>
<li><strong>提交</strong>指将未存储的SQL语句结果写入数据库表。</li>
<li><strong>保留点</strong>指事务处理中设置的临时占位符，可以对它发布回退。</li>
</ol>
<h2 id="控制事务处理"><a href="#控制事务处理" class="headerlink" title="控制事务处理"></a>控制事务处理</h2><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span></span><br></pre></td></tr></table></figure>
<p>用来标识事务的开始。</p>
<h3 id="使用ROLLBACK"><a href="#使用ROLLBACK" class="headerlink" title="使用ROLLBACK"></a>使用ROLLBACK</h3><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> ordertotals;</span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> ordertotals;</span><br><span class="line"><span class="keyword">select</span>*<span class="keyword">from</span> ordertotals;</span><br><span class="line"><span class="keyword">rollback</span>;</span><br><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> ordertotals;</span><br></pre></td></tr></table></figure>
<p>删除ordertotals表中的内容，然后进行事务回退。</p>
<h3 id="使用commit"><a href="#使用commit" class="headerlink" title="使用commit"></a>使用commit</h3><p>一般的MySQL语句都是直接针对数据库表执行和编写的。这就是所谓的隐含提交（ implicit commit），即提交（写或保存）操作是自动进行的。但是，在事务处理块中，提交不会隐含地进行。为进行明确的提交，使用COMMIT语句，如下所示：  </p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> orderitems</span><br><span class="line"><span class="keyword">where</span> order_num=<span class="number">20010</span>;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> orders</span><br><span class="line"><span class="keyword">where</span> order_num=<span class="number">20010</span>;</span><br><span class="line"><span class="keyword">commit</span>;</span><br></pre></td></tr></table></figure>
<p>在这个例子中，从系统中完全删除订单20010。因为涉及更新两个数据库表orders和orderItems，所以使用事务处理块来保证订单不被部分删除。最后的COMMIT语句仅在不出错时写出更改。如果第一条DELETE起作用，但第二条失败，则DELETE不会提交（实际上，它是被自动撤销的）。  </p>
<h3 id="使用保留点"><a href="#使用保留点" class="headerlink" title="使用保留点"></a>使用保留点</h3><p>简单的ROLLBACK和COMMIT语句就可以写入或撤销整个事务处理。但是，只是对简单的事务处理才能这样做，更复杂的事务处理可能需要部分提交或回退。</p>
<p>例如，前面描述的添加订单的过程为一个事务处理。如果发生错误，只需要返回到添加orders行之前即可，不需要回退customers表（如果存在的话）。  </p>
<p>为了支持回退部分事务处理，必须能在事务处理块中合适的位置放<br>置占位符。这样，如果需要回退，可以回退到某个占位符。  </p>
<p>这些占位符称为保留点。为了创建占位符，可如下使用SAVEPOINT<br>语句：  </p>
<figure class="highlight abnf"><table><tr><td class="code"><pre><span class="line">savepoint delete1<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p>每个保留点都取标识它的唯一名字，以便在回退时， MySQL知道要<br>回退到何处。为了回退到本例给出的保留点，可如下进行  </p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">rollback</span> <span class="keyword">to</span> delete1;</span><br></pre></td></tr></table></figure>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- start transaction</span></span><br><span class="line"><span class="keyword">start</span> <span class="keyword">transaction</span>;</span><br><span class="line"><span class="comment">-- save point</span></span><br><span class="line"><span class="keyword">SAVEPOINT</span> point1;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> orders</span><br><span class="line"><span class="keyword">where</span> order_num=<span class="number">20007</span>;</span><br><span class="line"><span class="comment">-- rollback</span></span><br><span class="line"><span class="keyword">rollback</span> <span class="keyword">to</span> point1;</span><br><span class="line"><span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> orders;</span><br></pre></td></tr></table></figure>
<p>保留点与回滚到点须在事务开始与提交之间。</p>
<font color="red">保留点越多越好</font>

<p>保留点越多，你就越能按自己的意愿灵活地进行回退。</p>
<font color="red">释放保留点</font>

<p>保留点在事务处理完成（执行一条ROLLBACK或COMMIT）后自动释放。  用RELEASE<br>SAVEPOINT明确地释放保留点。  </p>
<h3 id="更改默认的提交行为"><a href="#更改默认的提交行为" class="headerlink" title="更改默认的提交行为"></a>更改默认的提交行为</h3><p>默认的MySQL行为是自动提交所有更改。换句话说，任何时候你执行一条MySQL语句，该语句实际上都是针对表执行的，而且所做的更改立即生效。为指示MySQL不自动提交更改，需要使用以下句：  </p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> <span class="attribute">autocommit</span>=0;</span><br></pre></td></tr></table></figure>
<p>autocommit标志决定是否自动提交更改，不管有没有COMMIT语句。设置autocommit为0（假）指示MySQL不自动提交更改（直到autocommit被设置为真为止）。  </p>
<font color="red">标志为连接专用</font>

<p>autocommit标志是针对每个连接而不是服务器的。</p>
<h1 id="全球化与本地化"><a href="#全球化与本地化" class="headerlink" title="全球化与本地化"></a>全球化与本地化</h1><p><script type="math/tex">校对</script>为规定字符如何比较指令。</p>
<font color="red">校对为什么重要</font>

<p>排序英文正文很容易，对吗？或许不。考虑词APE、 apex和Apple。它们处于正确的排序顺序吗？这有赖于你是否想区分大小写。使用区分大小写的校对顺序，这些词有一种排序方式，使用不区分大小写的校对顺序有另外一种排序方式。这不仅影响排序（如用ORDER BY排序数据），还影响搜索。</p>
<h2 id="使用字符集与校对顺序"><a href="#使用字符集与校对顺序" class="headerlink" title="使用字符集与校对顺序"></a>使用字符集与校对顺序</h2><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="type">character</span> <span class="keyword">set</span>;</span><br></pre></td></tr></table></figure>
<p>这条语句显示所有可用的字符集以及每个字符集的描述和默认校对。  </p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">collation</span>;</span><br></pre></td></tr></table></figure>
<p>此语句显示所有可用的校对，以及它们适用的字符集。  </p>
<p>在创建数据库时，指定默认的字符集和校对。为了确定所用的字符集和校对，可以使用以下语句：  </p>
<figure class="highlight gams"><table><tr><td class="code"><pre><span class="line">show <span class="keyword">variables</span> like <span class="comment">&#x27;character%&#x27;</span>;</span><br><span class="line">show <span class="keyword">variables</span> like <span class="comment">&#x27;collation%&#x27;</span>;</span><br></pre></td></tr></table></figure>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> mytable</span><br><span class="line">(</span><br><span class="line">column1 <span class="type">int</span>,</span><br><span class="line">column2 <span class="type">varchar</span>(<span class="number">10</span>)</span><br><span class="line">)<span class="keyword">default</span> <span class="type">character</span> <span class="keyword">set</span> hebrew <span class="keyword">collate</span> hebrew_general_ci;</span><br></pre></td></tr></table></figure>
<p>此语句创建一个包含两列的表，并且指定一个字符集和一个校对顺序。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> mytable</span><br><span class="line">(</span><br><span class="line">column1 <span class="type">int</span>,</span><br><span class="line">column2 <span class="type">varchar</span>(<span class="number">10</span>),</span><br><span class="line">column3 <span class="type">varchar</span>(<span class="number">10</span>) <span class="type">character</span> <span class="keyword">set</span> lain1 <span class="keyword">collate</span></span><br><span class="line">hebrew_general_ci;</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>这里对整个表以及一个特定的列指定了CHARACTER SET和COLLATE。  </p>
<p>如果你需要用与创建表时不同的校对顺序排序特定的SELECT语句，可以在SELECT语句自身中进行  </p>
<figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> *<span class="keyword">from</span> customers</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> lastname,firstname <span class="keyword">collate</span> laint1_general_cs;</span><br></pre></td></tr></table></figure>
<h1 id="安全管理"><a href="#安全管理" class="headerlink" title="安全管理"></a>安全管理</h1><h2 id="查看用户表"><a href="#查看用户表" class="headerlink" title="查看用户表"></a>查看用户表</h2><figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line">use mysql;</span><br><span class="line"><span class="keyword">select</span> <span class="keyword">user</span> <span class="keyword">from</span> <span class="keyword">user</span>;</span><br></pre></td></tr></table></figure>
<p>该语句会列出所有mysql中的所有用户。</p>
<h2 id="创建用户账号"><a href="#创建用户账号" class="headerlink" title="创建用户账号"></a>创建用户账号</h2><figure class="highlight n1ql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">user</span> ben identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span></span><br></pre></td></tr></table></figure>
<p>CREATE USER创建一个新用户账号。  </p>
<h2 id="重命名账号"><a href="#重命名账号" class="headerlink" title="重命名账号"></a>重命名账号</h2><figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line">rename <span class="keyword">user</span> <span class="title">ben</span> to bforta;</span><br></pre></td></tr></table></figure>
<h2 id="删除账号"><a href="#删除账号" class="headerlink" title="删除账号"></a>删除账号</h2><figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line">drop <span class="keyword">user</span> <span class="title">bforta</span>;</span><br></pre></td></tr></table></figure>
<h2 id="设置访问权限"><a href="#设置访问权限" class="headerlink" title="设置访问权限"></a>设置访问权限</h2><figure class="highlight abnf"><table><tr><td class="code"><pre><span class="line">show grants for bforta<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220329163543533.png" alt="image-20220329163543533"></p>
<p>输出结果显示用户bforta有一个权限USAGE ON <em>.</em>。 USAGE表示根本没有权限。</p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">grant</span> <span class="literal">select</span> <span class="literal">on</span> <span class="regexp">crashcourse.*</span> to bforta;</span><br></pre></td></tr></table></figure>
<p>此GRANT允许用户在crashcourse.*（ crashcourse数据库的所有表）上使用SELECT。 通过只授予SELECT访问权限，用户bforta对crashcourse数据库中的所有数据具有只读访问权限。  </p>
<p>show grants反映这个更改。</p>
<figure class="highlight abnf"><table><tr><td class="code"><pre><span class="line">show grants for bforta<span class="comment">;</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220329163636393.png" alt="image-20220329163636393"></p>
<p>每个GRANT添加（或更新）用户的一个权限。   </p>
<figure class="highlight nginx"><table><tr><td class="code"><pre><span class="line"><span class="attribute">revoke</span> <span class="literal">select</span> <span class="literal">on</span> <span class="regexp">crashcourse.*</span> from bforta;</span><br></pre></td></tr></table></figure>
<p>这条REVOKE语句取消刚赋予用户bforta的SELECT访问权限。 被撤销的访问权限必须存在，否则会出错  。</p>
<font color="red">grant和revoke可在几个层次上控制访问权限</font>

<ol>
<li>整个服务器，使用grant all 和revoke all；</li>
<li>整个数据库，使用on database.*；</li>
<li>特定的表，使用on database.table；</li>
<li>特定的列；</li>
<li>特定的存储过程；</li>
</ol>
<center>权限</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">权限</th>
<th style="text-align:center">说明</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">ALL</td>
<td style="text-align:center">除GRANT OPTION外的所有权限</td>
</tr>
<tr>
<td style="text-align:center">ALTER</td>
<td style="text-align:center">使用ALTER TABLE</td>
</tr>
<tr>
<td style="text-align:center">ALTER ROUTINE</td>
<td style="text-align:center">使用ALTER PROCEDURE和DROP PROCEDURE</td>
</tr>
<tr>
<td style="text-align:center">CREATE</td>
<td style="text-align:center">使用CREATE TABLE</td>
</tr>
<tr>
<td style="text-align:center">CREATE ROUTINE</td>
<td style="text-align:center">使用CREATE PROCEDURE</td>
</tr>
<tr>
<td style="text-align:center">CREATE TEMPORARY TABLES</td>
<td style="text-align:center">使用CREATE TEMPORARY TABLE</td>
</tr>
<tr>
<td style="text-align:center">CREATE USER</td>
<td style="text-align:center">使用CREATE USER、 DROP USER、 RENAME USER和REVOKE ALL PRIVILEGES</td>
</tr>
<tr>
<td style="text-align:center">CREATE VIEW</td>
<td style="text-align:center">使用CREATE VIEW</td>
</tr>
<tr>
<td style="text-align:center">DELETE</td>
<td style="text-align:center">使用DELETE</td>
</tr>
<tr>
<td style="text-align:center">DROP</td>
<td style="text-align:center">使用DROP TABLE</td>
</tr>
<tr>
<td style="text-align:center">EXECUTE</td>
<td style="text-align:center">使用CALL和存储过程</td>
</tr>
<tr>
<td style="text-align:center">FILE</td>
<td style="text-align:center">使用SELECT INTO OUTFILE和LOAD DATA INFILE</td>
</tr>
<tr>
<td style="text-align:center">GRANT OPTION</td>
<td style="text-align:center">使用GRANT和REVOKE</td>
</tr>
<tr>
<td style="text-align:center">INDEX</td>
<td style="text-align:center">使用CREATE INDEX和DROP INDEX</td>
</tr>
<tr>
<td style="text-align:center">INSERT</td>
<td style="text-align:center">使用INSERT</td>
</tr>
<tr>
<td style="text-align:center">LOCK TABLES</td>
<td style="text-align:center">使用LOCK TABLES</td>
</tr>
<tr>
<td style="text-align:center">PROCESS</td>
<td style="text-align:center">使用SHOW FULL PROCESSLIST</td>
</tr>
<tr>
<td style="text-align:center">RELOAD</td>
<td style="text-align:center">使用FLUSH</td>
</tr>
<tr>
<td style="text-align:center">REPLICATION CLIENT</td>
<td style="text-align:center">服务器位置的访问</td>
</tr>
<tr>
<td style="text-align:center">REPLICATION SLAVE</td>
<td style="text-align:center">由复制从属使用</td>
</tr>
<tr>
<td style="text-align:center">SELECT</td>
<td style="text-align:center">使用SELECT</td>
</tr>
<tr>
<td style="text-align:center">SHOW DATABASES</td>
<td style="text-align:center">使用SHOW DATABASES</td>
</tr>
<tr>
<td style="text-align:center">SHOW VIEW</td>
<td style="text-align:center">使用SHOW CREATE VIEW</td>
</tr>
<tr>
<td style="text-align:center">SHUTDOWN</td>
<td style="text-align:center">使用mysqladmin shutdown（用来关闭MySQL）</td>
</tr>
<tr>
<td style="text-align:center">SUPER</td>
<td style="text-align:center">使用CHANGE MASTER、 KILL、 LOGS、 PURGE、 MASTER 和SET GLOBAL。还允许mysqladmin调试登录</td>
</tr>
<tr>
<td style="text-align:center">UPDATE</td>
<td style="text-align:center">使用UPDATE</td>
</tr>
<tr>
<td style="text-align:center">USAGE</td>
<td style="text-align:center">无访问权限</td>
</tr>
</tbody>
</table>
</div>
<h2 id="更改口令"><a href="#更改口令" class="headerlink" title="更改口令"></a>更改口令</h2><figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">password</span> <span class="keyword">for</span> bforta=<span class="keyword">password</span>(<span class="string">&#x27;123456&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p>SET PASSWORD更新用户口令。新口令必须传递到Password()函数进行加密。  </p>
<p>SET PASSWORD还可以用来设置你自己的口令：  </p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> <span class="keyword">password</span>=<span class="keyword">Password</span>(<span class="string">&#x27;awertfg&#x27;</span>);</span><br></pre></td></tr></table></figure>
<p>在不指定用户名时， SET PASSWORD更新当前登录用户的口令。  </p>
<h1 id="数据库维护"><a href="#数据库维护" class="headerlink" title="数据库维护"></a>数据库维护</h1><h2 id="备份数据"><a href="#备份数据" class="headerlink" title="备份数据"></a>备份数据</h2><p>像所有数据一样， MySQL的数据也必须经常备份。由于MySQL数据库是基于磁盘的文件，普通的备份系统和例程就能备份MySQL的数据。但是，由于这些文件总是处于打开和使用状态，普通的文件副本备份不一定总是有效。  </p>
<p>下面列出这个问题的可能解决方案。  </p>
<ol>
<li>使用命令行实用程序mysqldump转储所有数据库内容到某个外部<br>文件。在进行常规备份前这个实用程序应该正常运行，以便能正<br>确地备份转储文件。 </li>
<li>可用命令行实用程序mysqlhotcopy从一个数据库复制所有数据<br>（并非所有数据库引擎都支持这个实用程序)。</li>
<li>可以使用MySQL的BACKUP TABLE或SELECT INTO OUTFILE转储所<br>有数据到某个外部文件。这两条语句都接受将要创建的系统文件<br>名，此系统文件必须不存在，否则会出错。数据可以用RESTORE<br>TABLE来复原  </li>
</ol>
<font color="red">首先刷新未写数据  </font>

<p>为了保证所有数据被写到磁盘（包括索引数据），可能需要在进行备份前使用FLUSH TABLES语句。  </p>
<h2 id="进行数据库维护"><a href="#进行数据库维护" class="headerlink" title="进行数据库维护"></a>进行数据库维护</h2><p>为了保证所有数据被写到磁盘（包括索引数据），可能需要在进行备份前使用FLUSH TABLES语句。  </p>
<ol>
<li><p>ANALYZE TABLE，用来检查表键是否正确。 ANALYZE TABLE返回如下所示的状态信息：  </p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">analyze</span> <span class="keyword">table</span> orders;</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220329171225225.png" alt="image-20220329171225225"></p>
</li>
<li><p>CHECK TABLE用来针对许多问题对表进行检查。 在MyISAM表上还对索引进行检查。 CHECK TABLE支持一系列的用于MyISAM表的方式。CHANGED检查自最后一次检查以来改动过的表。 EXTENDED执行最彻底的检查， FAST只检查未正常关闭的表， MEDIUM检查所有被删除的链接并进行键检验， QUICK只进行快速扫描。如下所示， CHECKTABLE发现和修复问题：</p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">check</span> <span class="keyword">table</span> orders,orderitems;</span><br></pre></td></tr></table></figure>
<p><img src="/posts/7740304/image-20220329171338109.png" alt="image-20220329171338109"></p>
</li>
<li><p>如果MyISAM表访问产生不正确和不一致的结果，可能需要用<br>REPAIR TABLE来修复相应的表。这条语句不应该经常使用，如果<br>需要经常使用，可能会有更大的问题要解决。  </p>
</li>
<li><p>如果从一个表中删除大量数据，应该使用OPTIMIZE TABLE来收回所用的空间，从而优化表的性能。  </p>
</li>
</ol>
<h2 id="诊断启动问题"><a href="#诊断启动问题" class="headerlink" title="诊断启动问题"></a>诊断启动问题</h2><p>服务器启动问题通常在对MySQL配置或服务器本身进行更改时出<br>现。 MySQL在这个问题发生时报告错误，但由于多数MySQL服务器是作为系统进程或服务自动启动的，这些消息可能看不到。</p>
<p> 在排除系统启动问题时，首先应该尽量用手动启动服务器。 MySQL<br>服务器自身通过在命令行上执行mysqld启动。下面是几个重要的mysqld命令行选项：  </p>
<ol>
<li>—help显示帮助—一个选项列表；</li>
<li>—safe-mode装载减去某些最佳配置的服务器；</li>
<li>—verbose显示全文本消息（为获得更详细的帮助消息与—help联合使用）；  </li>
<li>—version显示版本信息然后退出</li>
</ol>
<h2 id="查看日志文件"><a href="#查看日志文件" class="headerlink" title="查看日志文件"></a>查看日志文件</h2><p>MySQL维护管理员依赖的一系列日志文件。主要的日志文件有以下<br>几种。  </p>
<ol>
<li>错误日志。它包含启动和关闭问题以及任意关键错误的细节。此日志通常名为hostname.err，位于data目录中。此日志名可用—log-error命令行选项更改。  </li>
<li>查询日志。它记录所有MySQL活动，在诊断问题时非常有用。此日志文件可能会很快地变得非常大，因此不应该长期使用它。此日志通常名为hostname.log，位于data目录中。此名字可以用—log命令行选项更改 。</li>
<li>二进制日志。它记录更新过数据（或者可能更新过数据）的所有语句。此日志通常名为hostname-bin，位于data目录内。此名字可以用—log-bin命令行选项更改。注意， 这个日志文件是MySQL5中添加的，以前的MySQL版本中使用的是更新日志。</li>
<li>缓慢查询日志。顾名思义，此日志记录执行缓慢的任何查询。这个日志在确定数据库何处需要优化很有用。此日志通常名为hostname-slow.log ， 位 于 data 目 录 中 。 此 名 字 可 以 用—log-slow-queries命令行选项更改。  </li>
</ol>
<p>在使用日志时，可用FLUSH LOGS语句来刷新和重新开始所有日志文件。  </p>
<h1 id="改善性能"><a href="#改善性能" class="headerlink" title="改善性能"></a>改善性能</h1><ol>
<li>MySQL是用一系列的默认设置预先配置的，从这些设置开始通常是很好的。但过一段时间后你可能需要调整内存分配、缓冲区大小等。（为查看当前设置，可使用SHOW VARIABLES;和SHOWSTATUS;。）  </li>
<li>MySQL一个多用户多线程的DBMS，换言之，它经常同时执行多个任务。如果这些任务中的某一个执行缓慢，则所有请求都会执行缓慢。如果你遇到显著的性能不良，可使用SHOW PROCESSLIST显示所有活动进程（以及它们的线程ID和执行时间）。你还可以用  KILL命令终结某个特定的进程（使用这个命令需要作为管理员登录）。  </li>
<li>使用EXPLAIN语句让MySQL解释它将如何执行一条SELECT语句。  一般来说，存储过程执行得比一条一条地执行其中的各条MySQL语句快。</li>
<li>在导入数据时，应该关闭自动提交。你可能还想删除索引（包括FULLTEXT索引），然后在导入完成后再重建它们。  </li>
<li>你的SELECT语句中有一系列复杂的OR条件吗？通过使用多条SELECT语句和连接它们的UNION语句，你能看到极大的性能改进。  </li>
<li>LIKE很慢。一般来说，最好是使用FULLTEXT而不是LIKE。  </li>
</ol>
]]></content>
      <categories>
        <category>计算机</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>mysql必知必会</tag>
      </tags>
  </entry>
  <entry>
    <title>吴恩达深度学习</title>
    <url>/posts/89451192/</url>
    <content><![CDATA[<p>吴恩达机器学习与深度学习总结。</p>
<span id="more"></span>
<h1 id="梯度下降原理"><a href="#梯度下降原理" class="headerlink" title="梯度下降原理"></a>梯度下降原理</h1><h2 id="梯度下降算法"><a href="#梯度下降算法" class="headerlink" title="梯度下降算法"></a>梯度下降算法</h2><h3 id="代价函数定义"><a href="#代价函数定义" class="headerlink" title="代价函数定义"></a>代价函数定义</h3><p>梯度下降算法—求最小代价函数的一种算法，通过不断变幻参数使得找到最小代价函数，代价函数的定义</p>
<p><img src="/posts/89451192/梯度下降算法代价函数的定义.png" alt="梯度下降算法代价函数的定义"></p>
<h3 id="算法概要"><a href="#算法概要" class="headerlink" title="算法概要"></a>算法概要</h3><p><img src="/posts/89451192/梯度下降算法概要.png" alt="梯度下降算法概要"></p>
<p><strong>注：a表示学习率下降得多快</strong></p>
<h3 id="算法详解"><a href="#算法详解" class="headerlink" title="算法详解"></a>算法详解</h3><p><img src="/posts/89451192/梯度下降算法详解.png" alt="梯度下降算法详解"></p>
<h2 id><a href="#" class="headerlink" title=" "></a> </h2><p>当 <script type="math/tex">\theta_1</script>在右边时，由于导数为x轴的<script type="math/tex">\Delta x</script> 与y轴的 <script type="math/tex">\Delta y</script>的比值，又由于<script type="math/tex">\theta_1</script> 在右边，故 <script type="math/tex">\Delta x</script>与  <script type="math/tex">\Delta y</script>皆为正数所以其导数为正，通过公式<script type="math/tex">\theta_1=\theta_1-a\frac{d}{d\theta_1}J(\theta_1)</script>，参数<script type="math/tex">\theta_1</script>会向右趋近。<br>当  <script type="math/tex">\theta_1</script>在左边时，由于<script type="math/tex">\Delta x</script>  为正数与 <script type="math/tex">\Delta y</script>为负数，故其导数为负数，通过上面公式<script type="math/tex">\theta_1=\theta_1-a\frac{d}{d\theta_1}J(\theta_1)</script> ，参数<script type="math/tex">\theta_1</script>会向左进行趋近，无论<script type="math/tex">\theta_1</script> 在其极小值的左边或右边皆向极小值逼进，通过一系列的趋近，最终目的是使得参数<script type="math/tex">\theta_1</script>趋近于最小值。<br>此算法通过不断迭代直到极小值，当 <script type="math/tex">\theta_1</script> 为极小值点时其导数斜率趋向于某个值 ，此时斜率发生的变化极小  </p>
<p><strong>注：我们需要合理调整学习率</strong></p>
<p>当学习率a过大时，会导致<script type="math/tex">\frac{\Delta x}{\Delta y}</script>过大，就有</p>
<p><img src="/posts/89451192/学习率1.png" alt="学习率1" style="zoom:60%;"></p>
<p>当学习率过大时，会导致</p>
<p><img src="/posts/89451192/学习率2.png" alt="学习率2" style="zoom:60%;"></p>
<p>这样会导致从右边直接跨过极小点，直接到左边的位置，这是学习率过大导致的情况。</p>
<p>而若学习率过小会导致需要多次迭代。</p>
<h1 id="神经网络原理"><a href="#神经网络原理" class="headerlink" title="神经网络原理"></a>神经网络原理</h1><h2 id="神经网络梯度下降算法"><a href="#神经网络梯度下降算法" class="headerlink" title="神经网络梯度下降算法"></a>神经网络梯度下降算法</h2><p><img src="/posts/89451192/神经网络的梯度下降.png" alt="神经网络的梯度下降"></p>
<p>如图所示，我们有参数w,b，代价函数<script type="math/tex">J(w^{(1)},b^{1},w^{2},b^{2})=\frac{1}{m}\sum_{i=1}^{m}\iota(y^*,y)</script>，我们需要通过<script type="math/tex">dw^{(l)}</script>和<script type="math/tex">db^{(l)}</script>来更新梯度下降，使得参数<script type="math/tex">w^{(l)}</script>和<script type="math/tex">b^{(l)}</script>来更新。</p>
<h2 id="神经网络反向传播详解"><a href="#神经网络反向传播详解" class="headerlink" title="神经网络反向传播详解"></a>神经网络反向传播详解</h2><p>神经网络的计算是按照正向传播与反向传播过程来实现的，首先通过正向传播计算出神经网络的输出，紧接着进行一个反向传播，反向传播可以计算出对应的梯度或导数，即正向传播服务于反向传播，反向传播服务于导数的计算。依赖关系导数-&gt;反向传播-&gt;正向传播。</p>
<p>我们来通过计算图进行分析。</p>
<h3 id="计算图求导数"><a href="#计算图求导数" class="headerlink" title="计算图求导数"></a>计算图求导数</h3><p><img src="/posts/89451192/计算图1.png" alt="计算图1"></p>
<p>如图我们想要计算函数J(a,b,c)的值，其中</p>
<script type="math/tex; mode=display">
J(a,b,c)=3(a+bc)=3(5+3*2)=33</script><p>我们计算函数J，可以进行如下步骤的拆分 </p>
<script type="math/tex; mode=display">
U=bc \\
V=a+u \\
J=3V</script><p>我们可以得到如下计算图</p>
<p><img src="/posts/89451192/计算图2.png" alt="计算图2"></p>
<p>当结果有不同或者一些特殊的输出变量时，比如J也是我们想要优化，在logistic回归中，J也是想要最小化的成本函数，可以看出，一个从左到右的过程，可以计算出函数J的值。接下来我们将会看到为了计算导数数，从右到左的过程。</p>
<p><img src="/posts/89451192/使用计算图求导1.png" alt="使用计算图求导1"></p>
<p>假设我们要计算<script type="math/tex">\frac{dJ}{dV}</script>，我们应该怎么计算呢？</p>
<p>我们若改变V的值，那么J将如何改变呢？</p>
<p>我们有<script type="math/tex">J=3V</script>，我们将<script type="math/tex">V=11</script>，变化为V=11.001，而J=33变化为33.003，<br>即我们可以得到我们在x轴变动1，而<script type="math/tex">\frac{dJ}{dV}=3</script> ，我们y轴将变化的动静为x轴的3倍，又当我们令a=5变化成a=5.001， v=11.001， J=33.003，即a的波动影响v， 而影响J，又我们有链式法则<script type="math/tex">\frac{dJ}{da}=\frac{dJ}{dv}\frac{dv}{da}=3</script> ，即若我们要计算<script type="math/tex">\frac{dJ}{da}</script>， 需要先计算出<script type="math/tex">\frac{dJ}{dv}</script> 和<script type="math/tex">\frac{dv}{da}</script>  。</p>
<p><img src="/posts/89451192/使用计算图求导2.png" alt="使用计算图求导2"></p>
<p>可以通过上面的方法求出各导数，反向传播原理与上面过程相同。</p>
<h3 id="神经模型"><a href="#神经模型" class="headerlink" title="神经模型"></a>神经模型</h3><p><img src="/posts/89451192/神经模型1.png" alt="神经模型1"></p>
<p>神经模型中的单个逻辑单元如上图所示，通过输入x0,x1,…,xn得到一个逻辑激活函数，这称之为逻辑激活函数 。<br>逻辑激活函数即  </p>
<script type="math/tex; mode=display">
g(z)=\frac{1}{1+e^{-z}}</script><p><img src="/posts/89451192/神经模型2.png" alt="神经模型2"></p>
<p>在上图中是由一组神经元组合而成的神经网络，其中最左边的为输入层，中间为隐藏层，最右边的为输出层。</p>
<p><img src="/posts/89451192/神经模型3.png" alt="神经模型3"></p>
<p>在上图中 示神经网络共有三层，第一层 示输入，中间层 示隐藏层，第三层示输出层。其中<script type="math/tex">a_i^{(j)}</script> 示第j层的第i个激活单元，<script type="math/tex">\theta^{j}</script>表示权重控制的矩阵函数从第j 层映射到第 j+1层。如果神经网络在第j层有<script type="math/tex">s_j</script>单元在第j+1层有<script type="math/tex">s_j+1</script>单元然后<script type="math/tex">\theta^{(j)}</script>有维度  </p>
<script type="math/tex; mode=display">
s_{j+1}\times(s_j+1)</script><p>向前传播是指从第一层传播到第三层，以这样的方式进行传播，即从输入层传播到输出层  </p>
<p><img src="/posts/89451192/神经模型4.png" alt="神经模型4"></p>
<p>我们对向前传播进行向量化，我们变化为</p>
<script type="math/tex; mode=display">
z_1^{(2)}=\theta_{10}^{(1)}x_0+\theta_{11}^{(1)}x_1+\theta_{12}^{(1)}x_2+\theta_{13}^{(1)}x_3 \\
z_2^{(2)}=\theta_{20}^{(1)}x_0+\theta_{21}^{(1)}x_1+\theta_{22}^{(1)}x_2+\theta_{23}^{(1)}x_3 \\
z_3^{(2)}=\theta_{30}^{(1)}x_0+\theta_{31}^{(1)}x_1+\theta_{32}^{(1)}x_2+\theta_{33}^{(1)}x_3</script><p>其中</p>
<script type="math/tex; mode=display">
z^{(2)}=\theta^{(1)}x=\theta^{(1)}a^{(1)}\\
a^{(2)}=g(z^{(2)})\\
z^{(3)}=\theta^{(2)}x=\theta^{(2)}a^{(2)}\\
h_{\theta}(x)=a^{(3)}=g(z^{(3)})</script><h3 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h3><p><img src="/posts/89451192/代价函数1.png" alt="代价函数1"></p>
<p>在神经网络中，我们若要实行多元分类，我们需要通过输入样本数据<script type="math/tex">x^{(i)}</script>，然后输出数据<script type="math/tex">y^{(i)}</script>分类的矩阵，我们需要使得函数<script type="math/tex">h_\theta(x)</script>来近似于<script type="math/tex">y^{(i)}</script>来对不同的数据进行分类。</p>
<p><img src="/posts/89451192/代价函数2.png" alt="代价函数2"></p>
<p>神经网络的代价函数如下：  </p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}[\sum_{i=1}^{m}\sum _{k=1}^{k} y^{(i)}_{k}log(h_{\theta}(x^{(i)})_k)+(1-y^{(i)}_k)log(1-h_{\theta}(x^{(i)})_k)]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\theta_{ji}^{(l)})^2</script><p>其中<script type="math/tex">(h_{\theta}(x))_i=i^{th} output</script> 表示第i个输出函数，因为在神经网络中有多个输入项，所以有多个代价函数，在这个式子中代价函数数为k,即k个输出函数(项),所以第一项为上式所 示,故有第 一项为  </p>
<script type="math/tex; mode=display">
J(\theta)=-\frac{1}{m}[\sum_{i=1}^{m}\sum _{k=1}^{k} y^{(i)}_{k}log(h_{\theta}(x^{(i)})_k)+(1-y^{(i)}_k)log(1-h_{\theta}(x^{(i)})_k)]</script><p>在第二项中， 示有L层，而因为 由上一层与连接下一层而形成的，所以个数也为两层的乘积，而后有从1到L-1，故有第二项为  </p>
<script type="math/tex; mode=display">
\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\theta_{ji}^{(l)})^2</script><h3 id="反向传播过程"><a href="#反向传播过程" class="headerlink" title="反向传播过程"></a>反向传播过程</h3><p>反向传播可以计算出在神经网络中各个节点的导数，我们再通过梯度下降算法来进行目标函数的最小化。</p>
<p><img src="/posts/89451192/反向传播1.png" alt="反向传播1"></p>
<p>在上图中我们的最终目的是寻找最小化 <script type="math/tex">J(\theta)</script></p>
<p><img src="/posts/89451192/反向传播2.png" alt="反向传播2"></p>
<p>反射传播从直观上来说就是计算每一层的每个节点的误差，在上图中就是计算第三层的各个节点的误差，而后计算第二层的各个节点的误差，最后计算第一层的各个节点的误差，其中</p>
<script type="math/tex; mode=display">
\frac{\partial}{\partial \theta^{(l)}_{ij}}J(\theta)=a_j^{(l)}\delta_i^{(l+1)}</script><p>我们有梯度下降的正向传播过程以及涉及的参数如下  </p>
<p><img src="/posts/89451192/反向传播3.png" alt="反向传播3"></p>
<p>梯度下降算法中的反射传播算法过程如下  </p>
<p><img src="/posts/89451192/反向传播4.png" alt="反向传播4"></p>
<p>其中反向传播算法如下：</p>
<p><img src="/posts/89451192/反向传播5.png" alt="反向传播5"></p>
<p>其中符号<script type="math/tex">\Delta</script>，其中l表示神经网络的层数，i表示样本数，j表示神经网络中某层的第几个节点</p>
<p>算法步骤解释如下：</p>
<p>一、设置所有符号<script type="math/tex">\Delta_{ij}^{(l)}</script>均为0.</p>
<p>二、设置从样本1到m的循环，即遍历所有样本.</p>
<p>三、在循环中设置输入参数为<script type="math/tex">a^{(1)}</script>.</p>
<p>四、执行正向传播算法，求得各层的<script type="math/tex">a^{(l)}</script>.</p>
<p>五、使用结果<script type="math/tex">y^{(i)}</script>来计算输出层的差值<script type="math/tex">\delta^{(l)}</script>.</p>
<p>六、计算隐藏层的差值<script type="math/tex">\delta^{(l-1)},\delta^{(l-2)}...</script></p>
<p>七、计算每个样本的各层的各个节点的<script type="math/tex">\Delta_{ij}^{(l)}</script>,其中式子</p>
<script type="math/tex; mode=display">
\Delta_{ij}^{(l)}=\Delta_{ij}^{(l)}+a_{j}^{(l)}\delta_i^{(l+1)}</script><p>当<script type="math/tex">J(\theta)</script>的m为1，且无正则项时，</p>
<script type="math/tex; mode=display">
\frac {\partial}{\partial \theta_{ij}^{(l)}}J(\theta)=a_j^{(l)}\delta_i^{(l+1)}</script><p>八、 我们计算各样本各层节点的<script type="math/tex">D_{ij}^{(l)}</script>，<script type="math/tex">\frac{\partial}{\partial \theta_{ij}^{(l)}}=D_{ij}^{(l)}</script>，此处的函数<script type="math/tex">J(\theta)</script>m不等于1且有正则项的分两种情况，当</p>
<script type="math/tex; mode=display">
D_{ij}^{(l)}=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda \theta_{ij}^{(l)} \:\:\: if\:\: j\neq0(非偏置项) \\
D_{ij}^{(l)}=\frac{1}{m}\Delta_{ij}^{(l)}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ if\:\: j\neq0(偏置项)</script><p>为什么此时是<script type="math/tex">\lambda \theta_{ij}^{(l)}</script>不是<script type="math/tex">\frac{\lambda }{m}\theta_{ij}^{(l)}</script>暂时不清楚</p>
<h3 id="反向传播算法的理解与推导"><a href="#反向传播算法的理解与推导" class="headerlink" title="反向传播算法的理解与推导"></a>反向传播算法的理解与推导</h3><p><img src="/posts/89451192/image-20220619151637481-16556230021951.png" alt="image-20220619151637481"></p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>机器学习</tag>
        <tag>吴恩达</tag>
      </tags>
  </entry>
  <entry>
    <title>基于CNN的代码查重模型</title>
    <url>/posts/8ed5f4b9/</url>
    <content><![CDATA[<p>本文为基于CNN的代码查重模型所涉及的知识点的总结。该论文主是通过利用卷积神经网络来进行模型的构建，经过一定数量的数据训练使得模型在进行代码查重具有一定的准确性。该流程主要分成三个部分，一、代码文本预处理，二、数据输入进模型并得到向量，三、文本的相似度计算。</p>
<span id="more"></span>
<h1 id="相关技术介绍"><a href="#相关技术介绍" class="headerlink" title="相关技术介绍"></a>相关技术介绍</h1><h2 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h2><p>词嵌入原本是用来表示自然语言的向量表示方法，词嵌入可以使得自然语言文本映射成为一个向量矩阵，这会使得词通过词嵌入映射变得更加有含义，可以通过词嵌入映射的向量进行词的类比。其中词进行映射成嵌入矩阵的流程如下图所示</p>
<p><img src="/posts/8ed5f4b9/image-20220619100227736-16556041504651.png" alt="image-20220619100227736"></p>
<center>图1 文本映射成嵌入矩阵 </center>

<p>其中词典为文本中的词的类别映射成的(词,数字)元组，使得已经包括进行词典的词，可以再次映射到相同的数字。如有文本，周杰伦/唱/本草纲目，我们可以得构建的词典为{“周杰伦”:1, “和”:2, “本草纲目”:3}，这样我们就构建起了词典，下次再次出现“周杰伦”，我们就把这个词映射成为1，出现“本草纲目”，会把这个映射成3，若出现新的不在词典中的词会接着进行映射。当然，上面对于文本是进行了分词操作，如果没有使用分词操作，会进行单个字符的映射来构建词典，总得来说，词典把文本映射成数字，再把数字送入嵌入层来得到嵌入矩阵。对于嵌入层来说，运算过程图2所示</p>
<p><img src="/posts/8ed5f4b9/image-20220619100251717-16556041746852.png" alt="image-20220619100251717"></p>
<center>图2 嵌入层的运算过程</center>

<p>我们可以从上图可以看出两个矩阵进行相乘可以得到“周杰伦”的嵌入层权重向量，且权重向量为词维度，故整个嵌入层的权重矩阵为词典大小*词维度。</p>
<h2 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h2><h3 id="神经网络结构"><a href="#神经网络结构" class="headerlink" title="神经网络结构"></a>神经网络结构</h3><p>我们把多个神经元相互进行连接，即可得到一个神经网络。如图3如示。</p>
<p>​                                                         <img src="/posts/8ed5f4b9/image-20220619100323940-16556042052743.png" alt="image-20220619100323940"></p>
<center>图3 神经网络结构图</center>

<p>这是一个简易的神经网络图，由三层组成，分别为输入层、隐藏层、输出层组成，其中隐藏层得到输入经由激活函数再进行输出，在这个神经网络中，隐藏层的每一个神经元都由全部输入数据进行输入，再由隐藏中的W，B权重相乘与相加，最后经由激活函数进行输出，我们也可以通过加入多个隐藏层来构建一个更加深的神经网络。</p>
<h3 id="神经网络模型的训练"><a href="#神经网络模型的训练" class="headerlink" title="神经网络模型的训练"></a>神经网络模型的训练</h3><p>神经网络的模型训练，通常由以下几步来完成。</p>
<h4 id="定义和初始化模型"><a href="#定义和初始化模型" class="headerlink" title="定义和初始化模型"></a>定义和初始化模型</h4><p>首先需要对于神经网络的结构进行定义，几层隐藏层，中间使用什么层作为隐藏层，初始化模型即需要对于模型的参数权重进行初始化。</p>
<h4 id="定义损失函数"><a href="#定义损失函数" class="headerlink" title="定义损失函数"></a>定义损失函数</h4><p>损失函数通常为模型的预测值Y与实际的Y之间的差值。预测值Y为对于输入X，通过模型的运算得出的值称之为预测Y。比较常见的损失函数为</p>
<script type="math/tex; mode=display">
J(\theta)=\frac{1}{2m}[\sum_{i=1}^{m}(h_{\theta(x^{(i)})}-y^{(i)})^{2}+\lambda\sum_{j=1}^{n}\theta_{j}^{2}]</script><p>其中<script type="math/tex">h_{\theta}(x^{(i)})</script>为模型预测后得出的值，通过优化损失函数使得模型预测的值与实际的值进行最小化。</p>
<h4 id="定义优化函数"><a href="#定义优化函数" class="headerlink" title="定义优化函数"></a>定义优化函数</h4><p>优化函数是指定义算法使得最小化损失函数，比较常见的优化算法有梯度下降算法、批量梯度下降算法、动量梯度下降算法、Adam算法等。以梯度下降为例，有算法如下：</p>
<p>我们有在神经网络中的权重参数矩阵有 <script type="math/tex">\theta_{1},\theta_{2}</script>。</p>
<p>Repeat until convergence{</p>
<script type="math/tex; mode=display">
\theta_{1}:=\theta_{1}-a\frac{\partial}{\partial\theta_{1}}J(\theta_{1},\theta_{2}) \\
\theta_{2}:=\theta_{2}-a\frac{\partial}{\partial\theta_{2}}J(\theta_{1},\theta_{2})</script><p>}</p>
<p>其中a为学习率，这是一个超参数，通过调节来指定在梯度下降的过程中进行下降的速度，而 <script type="math/tex">\frac{\partial}{\partial\theta_{1}}J(\theta_{1},\theta_{2})</script>为上式中损失函数的求导数。</p>
<p>假设我们设在神经网络中只有一个权重参数<script type="math/tex">\theta</script>，则我们有梯度下降算法</p>
<p>Repeat until convergence{</p>
<script type="math/tex; mode=display">
\theta:=\theta-a\frac{\partial}{\partial \theta}J(\theta)</script><p>}</p>
<p>则上面的下降算法则在作如下变化，如图4如示：</p>
<p><img src="/posts/8ed5f4b9/image-20220619100410578-16556042516654.png" alt="image-20220619100410578"></p>
<center>图4 下降过程-右</center>

<p>当我们的参数<script type="math/tex">\theta</script>在最小值的右边时，由于其导数为<script type="math/tex">\theta</script>的<script type="math/tex">d\theta</script>与<script type="math/tex">dJ(\theta)</script>的比值，当增量足够小时，则有<script type="math/tex">\frac{dJ(\theta)}{d\theta}\approx\frac{\triangle J(\theta)}{ \triangle \theta}</script>，所以当执行算法</p>
<script type="math/tex; mode=display">
\theta:=\theta-a\frac{\partial }{\partial \theta}J(\theta)</script><p>时参数<script type="math/tex">\theta</script>会逐渐减小，并沿着横坐标往左向着最小值的方向进行逼进，如图5所示。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100429806-16556042712155.png" alt="image-20220619100429806"></p>
<center>图5 下降过程-左</center>

<p>当我们的参数<script type="math/tex">\theta</script>在最小值的左边时，由于其 <script type="math/tex">\frac{\partial}{\partial \theta}J(\theta)</script>为负，会导致整体算法在运算时时会逐渐变大，并沿着横坐标往右向着最小值的方向进行逼进。</p>
<p>而当我们把超参数a设置过大时，会导致参数<script type="math/tex">\theta</script>在最小值的最大距离之间来回振荡，而当超参数<script type="math/tex">\theta</script>设置过小会导致迭代次数的增加。以上为神经网络中只有一个权重参数<script type="math/tex">\theta</script>进行下降算法时的过程，而把这个参数从一维推广到多维可以得到类似的下降过程。</p>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>我们在对模型进行计算与训练时，会进行两次传播，即正向传播与反向传播，其中正向传播为整个模型的计算过程。如图6所示。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100446065-16556042873666.png" alt="image-20220619100446065"></p>
<center>图6 正向传播</center>

<p>正向传播通过从左往后进行计算，得到各个节点的值并逐步向右进行传播，最终到输出层得到最后的预测值Y，而反向传播的过程与正向传播正相反，是从右往左进行传播并逐步计算出各个节点的值，最终到输入层。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100459505-16556043007987.png" alt="image-20220619100459505"></p>
<center>图7 反向传播</center>

<p>如图7所示，其中<script type="math/tex">\delta</script>为节点之间的误差，而执行反向传播的意义是为计算各参数的导数所需的数据，即优化算法需要各参数的导数，而导数需要进行正向传播与反向传播计算出各个节点的值与误差。</p>
<p>模型训练需要用到损失函数的定义，优化算法的定义，以及模型和迭代次数等参数。其中模型训练的流程如图8所示。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100515320-16556043163118.png" alt="image-20220619100515320"></p>
<center>图8 模型训练流程图</center>

<p>我们在进行模型训练时经过模型的计算得到的预测Y，并通过损失函数来进行两者数据之间的差值，再执行反向传播，并利用优化算法对参数进行更新，如此反复，直到整个迭代过程结束。</p>
<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><h4 id="卷积神经网络简介"><a href="#卷积神经网络简介" class="headerlink" title="卷积神经网络简介"></a>卷积神经网络简介</h4><p>卷积神经网络开始主要是应用于图像，因为卷积神经网络在图像中具有高效、快速提取的特性，故而在图像处理中有着较为广泛的应用。这是因为卷积神经网络主要有两个特性，一为局部感知，二为权重共享。对于神经元来说，无需全部感知，只需要局部感知即可。其中局部感知主要是进行局部区域信息的提取，通过一个个的过滤器进行参数的计算，权重共享是指过滤器在进行信息的提取时，会共用一个参数来进行计算值，这使得参数大大减少，会让训练更加高效。</p>
<p>其中卷积神经网络的计算过程如下图9所示。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100542906-165560434428810.png" alt="image-20220619100542906"></p>
<center>图9 卷积层提取信息</center>

<p>上图为一个过滤器通过卷积操作进行局部信息的提取并进行计算的过程，在这过程之后，窗口会继续进行滑动去提取未进行过滤器提取的信息。</p>
<p>池化层，对于不同的特征进行简化操作，一为可以进行神经元大小的缩减，二为可以提取主要特征，而忽略其次要特征，这使得我们的神经网络可以减少参数，使得训练的速度更快，也可以大幅度减少欠拟合的情况。</p>
<p>其中池化层有两种操作，一为最大池化层，二为平均池化层，其中最大池层是指所在的窗口范围内进行最大特征的提取，而忽略其它特征，而平均池化层是指在窗口范围内进行信息的值相加并取平均化。其中最大池化进行信息提取的过程如图10所示。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100600392-165560436154311.png" alt="image-20220619100600392"></p>
<center>图10 最大池化信息提取</center>

<p>而平均池化层进行信息提取，如图11所示。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100612750-165560437374712.png" alt="image-20220619100612750"></p>
<center>图11 平均池化信息提取</center>

<h4 id="卷积神经网络在文本上的应用"><a href="#卷积神经网络在文本上的应用" class="headerlink" title="卷积神经网络在文本上的应用"></a>卷积神经网络在文本上的应用</h4><p>卷积神经网络在开始是用于处理图像信息，如对图像进行特征的提取。其中把卷积神经网络应用于文本的开创性的模型是textCNN模型，该模型主要是通过卷积神经网络进行文本信息的提取，进行得到一个文本感情分类的模型。后面卷积神经网络也开始应用于文本的处理中，其中由Baotian Hu等人提出的自然语言句子匹配的卷积神经网络架构，他们提出了一个自然语言的文本匹配模型。如图12所示</p>
<p><img src="/posts/8ed5f4b9/image-20220619100624925-165560438627913.png" alt="image-20220619100624925"></p>
<center>图12 自然语言文本匹配模型</center>

<p>该模型是把文本输入进嵌入层，获取各个词的嵌入向量，再通过滑动窗口进行卷积，再通过池化层进行大小与的缩减与有效信息特征的提取，如此往复直到模型把文本缩减到一个只有固定大小的向量。</p>
<p>其中BaotianHu等人在论文中提到的一个关于在滑动窗口中卷积的例子是如图13如示。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100642035-165560440300314.png" alt="image-20220619100642035"></p>
<center>图13 卷积在滑动窗口中的例子分析</center>

<p>例子中通过卷积单元在三字窗口中关注不同的片段，给出了不同的组合，同时对于可信度较低的组合标记为灰色，然后用池化在两个窗口之间进行滑动，选取可信度较高的组合，排除可信度最低的组合，使得信息可以更加有效的提取。</p>
<p>BaotianHu等人在论文中提出一种用于两个句子之间进行匹配的模型如图14所示</p>
<p><img src="/posts/8ed5f4b9/image-20220619100652715-165560441393515.png" alt="image-20220619100652715"></p>
<center>图14 两个句子之间进行匹配的模型</center>

<p>可以看出这个模型是两个句子通过反复卷积、池化之后，把向量缩减到一个固定大小再通过多层感知器进行匹配程序的输出。</p>
<h1 id="基于CNN的代码查重系统的模型设计"><a href="#基于CNN的代码查重系统的模型设计" class="headerlink" title="基于CNN的代码查重系统的模型设计"></a>基于CNN的代码查重系统的模型设计</h1><h2 id="代码文本预处理"><a href="#代码文本预处理" class="headerlink" title="代码文本预处理"></a>代码文本预处理</h2><p>我们对于表1中所列出的代码抄袭手段进行相应的处理，即对于代码文本加注释、给代码文本加空格、数据类型变化，以及对变量名进行更改进行如下处理。</p>
<p>1) 我们对代码文本进行注释的删除，空格和头文件的去除。</p>
<p>2) 对于变量名进行风格的统一，统一用var字符来进行表示。</p>
<p>3) 为了应对数据类型的变化，我们对于数据类型我们也进行统一的数据类型字符来进行代替，用data_type来进行数据类的替代。</p>
<p>4) 为了应对循环结构的变化，我们对于循环结构进行风格的统一，对于所有的循环类型，统一用while来进行代替。</p>
<p>5) 为了应对变量声明的变化，我们对于变量的声明也作统一的变化，如把 int a,b=10; 变化为 int a;int b;b=10;</p>
<p>6) 我们把数字与字符也作变化，把数字变成统一的文本NUMS,把字符变作WORDS。</p>
<p>对于上述内容的总结，我们有如下表</p>
<center>表1 文本代码预处理规则</center>

<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>代码内容</th>
<th>替换标记</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>变量名</td>
<td>var</td>
</tr>
<tr>
<td>2</td>
<td>数据类型</td>
<td>data_type</td>
</tr>
<tr>
<td>3</td>
<td>循环</td>
<td>while</td>
</tr>
<tr>
<td>4</td>
<td>数字</td>
<td>nums</td>
</tr>
<tr>
<td>5</td>
<td>字符</td>
<td>words</td>
</tr>
<tr>
<td>6</td>
<td>函数声明的函数名</td>
<td>func_declar</td>
</tr>
<tr>
<td>7</td>
<td>函数调用</td>
<td>call_func</td>
</tr>
</tbody>
</table>
</div>
<h2 id="数据的处理与采样"><a href="#数据的处理与采样" class="headerlink" title="数据的处理与采样"></a>数据的处理与采样</h2><p>首先通过把相关的数据进行保留其有效数据，并把数据源划分为三类，分别为源代码、正例、负例。其中正例为对于源代码进行位置交换、变量名的更改、循环语句关键字的改变、函数的合并与切割，变量声明的改变等一系仿抄袭手段的变换。负例则是与源代码完全不同的代码文本，如我们有源代码1_source，通过抄袭手段变换得到正例1_positive，我们把2_source设置为1_negative表示为源代码1_source的负例。即我们得到三个不同的数据集，分别为源代码数据集、正例数据集和负例数据集。接下来我们通过应用表1中的规则对三个数据集都进行代码预处理操作。</p>
<h2 id="基于CNN的代码查重模型"><a href="#基于CNN的代码查重模型" class="headerlink" title="基于CNN的代码查重模型"></a>基于CNN的代码查重模型</h2><p>我们对于代码文本进行预处理之后，我们需要对于预处理之后的代码文本进行查重操作。考虑利用卷积神经网络的高效提取手段来进行代码文本的处理。我们利用Baotian Hu等人提出的自然语言句子匹配的卷积神经网络架构的模型来进行代码查重操作。</p>
<p>我们有基于CNN的代码查重训练模型如图15所示</p>
<p><img src="/posts/8ed5f4b9/image-20220619100707934-165560442902116.png" alt="image-20220619100707934"></p>
<center>图15 基于CNN的代码查重的训练模型</center>

<p>我们对于文本代码采用(正例，源代码，负例)的形式进行输入，首先经由嵌入层由文字变化为向量，再经过三层卷积和池化操作，使得有效信息被提取，以及减少了无效信息，最后进行一层全连接层的语义连接，再对得到的向量进行cos函数的比较，再对目标函数进行相应的优化。</p>
<p><strong>卷积层：</strong>对于卷积我们在经过嵌入层的代码转换的矩阵中，进行窗口的滑动操作并进行卷积操作，我们对于代码文本的卷积有如下定义。</p>
<script type="math/tex; mode=display">
z_{i}^{(a,f)}=z_{i}^{(a,f)}(x)=g(\hat{z}_{i}^{(a-1)})\sigma(W^{(a,f)}\hat{z}_{i}^{(a-1)}+b^{(a,f)}), f=1,2,...</script><p>其中表a层数，而f则为特征映射的卷积单元。</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.682ex;" xmlns="http://www.w3.org/2000/svg" width="7.71ex" height="3.082ex" role="img" focusable="false" viewbox="0 -1060.7 3407.7 1362.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="TeXAtom" transform="translate(498,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(918,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(1196,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(1746,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(498,-293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="mo" transform="translate(2057.7,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(2446.7,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(3018.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>表示对于a层中位置i的过滤f的卷积映射。</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.05ex;" xmlns="http://www.w3.org/2000/svg" width="6.099ex" height="2.071ex" role="img" focusable="false" viewbox="0 -893.3 2695.9 915.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="TeXAtom" transform="translate(1136.2,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(918,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(1196,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(1746,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></g></g></svg></mjx-container>则为a层中对于过滤器f的参数</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="4.058ex" height="2.262ex" role="img" focusable="false" viewbox="0 -750 1793.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70E" d="M184 -11Q116 -11 74 34T31 147Q31 247 104 333T274 430Q275 431 414 431H552Q553 430 555 429T559 427T562 425T565 422T567 420T569 416T570 412T571 407T572 401Q572 357 507 357Q500 357 490 357T476 358H416L421 348Q439 310 439 263Q439 153 359 71T184 -11ZM361 278Q361 358 276 358Q152 358 115 184Q114 180 114 178Q106 141 106 117Q106 67 131 47T188 26Q242 26 287 73Q316 103 334 153T356 233T361 278Z"/></g><g data-mml-node="mo" transform="translate(571,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mo" transform="translate(960,0)"><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"/></g><g data-mml-node="mo" transform="translate(1404.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>表示激活函数</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.682ex;" xmlns="http://www.w3.org/2000/svg" width="8.215ex" height="3.082ex" role="img" focusable="false" viewbox="0 -1060.7 3630.9 1362.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msubsup" transform="translate(866,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(288.1,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(498,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(918,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1696,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(2196,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(498,-293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="mo" transform="translate(3241.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> 表示当向量 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.682ex;" xmlns="http://www.w3.org/2000/svg" width="5.375ex" height="3.082ex" role="img" focusable="false" viewbox="0 -1060.7 2375.9 1362.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(288.1,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(498,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(918,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1696,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(2196,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(498,-293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></g></svg></mjx-container> 全为零时，函数    <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.682ex;" xmlns="http://www.w3.org/2000/svg" width="8.215ex" height="3.082ex" role="img" focusable="false" viewbox="0 -1060.7 3630.9 1362.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msubsup" transform="translate(866,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(288.1,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(498,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(918,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1696,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(2196,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(498,-293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="mo" transform="translate(3241.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>  也为零，当向量不为零时，则函数 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.682ex;" xmlns="http://www.w3.org/2000/svg" width="8.215ex" height="3.082ex" role="img" focusable="false" viewbox="0 -1060.7 3630.9 1362.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msubsup" transform="translate(866,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(288.1,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(498,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(918,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1696,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(2196,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(498,-293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="mo" transform="translate(3241.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>  为1。这是因为函数代码长度最有较大的可变性，通过添加函数 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.682ex;" xmlns="http://www.w3.org/2000/svg" width="8.215ex" height="3.082ex" role="img" focusable="false" viewbox="0 -1060.7 3630.9 1362.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D454" d="M311 43Q296 30 267 15T206 0Q143 0 105 45T66 160Q66 265 143 353T314 442Q361 442 401 394L404 398Q406 401 409 404T418 412T431 419T447 422Q461 422 470 413T480 394Q480 379 423 152T363 -80Q345 -134 286 -169T151 -205Q10 -205 10 -137Q10 -111 28 -91T74 -71Q89 -71 102 -80T116 -111Q116 -121 114 -130T107 -144T99 -154T92 -162L90 -164H91Q101 -167 151 -167Q189 -167 211 -155Q234 -144 254 -122T282 -75Q288 -56 298 -13Q311 35 311 43ZM384 328L380 339Q377 350 375 354T369 368T359 382T346 393T328 402T306 405Q262 405 221 352Q191 313 171 233T151 117Q151 38 213 38Q269 38 323 108L331 118L384 328Z"/></g><g data-mml-node="mo" transform="translate(477,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="msubsup" transform="translate(866,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mover"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="mo" transform="translate(288.1,16) translate(-250 0)"><path data-c="5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"/></g></g></g><g data-mml-node="TeXAtom" transform="translate(498,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(918,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1696,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(2196,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(498,-293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="mo" transform="translate(3241.9,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container> 可以消除因为长度引起的边际效应。</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.682ex;" xmlns="http://www.w3.org/2000/svg" width="9.754ex" height="3.082ex" role="img" focusable="false" viewbox="0 -1060.7 4311.4 1362.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D467" d="M347 338Q337 338 294 349T231 360Q211 360 197 356T174 346T162 335T155 324L153 320Q150 317 138 317Q117 317 117 325Q117 330 120 339Q133 378 163 406T229 440Q241 442 246 442Q271 442 291 425T329 392T367 375Q389 375 411 408T434 441Q435 442 449 442H462Q468 436 468 434Q468 430 463 420T449 399T432 377T418 358L411 349Q368 298 275 214T160 106L148 94L163 93Q185 93 227 82T290 71Q328 71 360 90T402 140Q406 149 409 151T424 153Q443 153 443 143Q443 138 442 134Q425 72 376 31T278 -11Q252 -11 232 6T193 40T155 57Q111 57 76 -3Q70 -11 59 -11H54H41Q35 -5 35 -2Q35 13 93 84Q132 129 225 214T340 322Q352 338 347 338Z"/></g><g data-mml-node="TeXAtom" transform="translate(498,530.4) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(389,0)"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"/></g><g data-mml-node="mo" transform="translate(918,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(1696,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g><g data-mml-node="mo" transform="translate(2196,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mi" transform="translate(2474,0)"><path data-c="1D453" d="M118 -162Q120 -162 124 -164T135 -167T147 -168Q160 -168 171 -155T187 -126Q197 -99 221 27T267 267T289 382V385H242Q195 385 192 387Q188 390 188 397L195 425Q197 430 203 430T250 431Q298 431 298 432Q298 434 307 482T319 540Q356 705 465 705Q502 703 526 683T550 630Q550 594 529 578T487 561Q443 561 443 603Q443 622 454 636T478 657L487 662Q471 668 457 668Q445 668 434 658T419 630Q412 601 403 552T387 469T380 433Q380 431 435 431Q480 431 487 430T498 424Q499 420 496 407T491 391Q489 386 482 386T428 385H372L349 263Q301 15 282 -47Q255 -132 212 -173Q175 -205 139 -205Q107 -205 81 -186T55 -132Q55 -95 76 -78T118 -61Q162 -61 162 -103Q162 -122 151 -136T127 -157L118 -162Z"/></g><g data-mml-node="mo" transform="translate(3024,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g><g data-mml-node="TeXAtom" transform="translate(498,-293.8) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g><g data-mml-node="mo" transform="translate(2961.4,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"/></g><g data-mml-node="mi" transform="translate(3350.4,0)"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mo" transform="translate(3922.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"/></g></g></g></svg></mjx-container>为a-1卷积层中位置i的节点分段，其中</p>
<script type="math/tex; mode=display">
\hat{z_{i } }^{(0 )}=X_{i:i+k_1-1}=[X_{i}^{T},...,X_ {i+k_1-1 }^{T}]^T</script><p>这部分为连接来自句子输入x的k1(滑动窗口的宽度)单词的向量。</p>
<p><strong>最大池化层</strong>:在每次卷积之后，我们在每两个单元窗口中为之间过滤器取一个最大池化操作。这里的池化操作有两重效果：一则可以将前层的特性大小缩小一半，从而可以快速对句子进行差异化吸收。二则可以过滤掉一些可信度较低的句子组合。</p>
<p>其中cos函数为</p>
<script type="math/tex; mode=display">
cos(A,B)=\frac{X^Ty } {||X||||Y||}</script><p>我们有最大优化目标为</p>
<script type="math/tex; mode=display">
L=max (0,cos(源代码,负例)-cos(源代码,正例)+1W)</script><p>通过上述一系列的特征提取与相似函数计算，到最后得到模型预测的值，再经过反向传播与梯度下降操作，使得变得逐渐收敛。</p>
<p>我们通过已经训练完毕的模型进行代码的查重与对比，对于此我们有代码对比查重模型，如图16所示。</p>
<p><img src="/posts/8ed5f4b9/image-20220619100720700-165560444171917.png" alt="image-20220619100720700"></p>
<center>图16 基于CNN的代码查重模型</center>

<p>通过在训练模型中已经更新完成的各层参数来进行代码对比的运算，源代码1和源代码2通过查重模型得到各自的向量，并通过cos来进行代码查重的概率计算。</p>
<p>通过该模型进行数据的训练以及超参数的调整可以使得模型去学习抄袭手段，通过数据的学习去对抗新的作弊方式。</p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>MXNet</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>动手学深度学习总结</title>
    <url>/posts/b96b4738/</url>
    <content><![CDATA[<p>本章节为&lt;&lt;动手学深度学习&gt;&gt;的知识点整理。</p>
<span id="more"></span>
<h1 id="深度学习基础"><a href="#深度学习基础" class="headerlink" title="深度学习基础"></a>深度学习基础</h1><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p><img src="/posts/b96b4738/image-20220404205915668.png" alt="image-20220404205915668"></p>
<p><strong>注</strong>：线性回归为一个单层的神经网络</p>
<h3 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h3><h4 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h4><p>设房屋面积为x1，房龄为x2，售出价格为y，我们需要建立基于输入x1和x2来计算输出y的表达式，即模型，线性回归假设输出与各个输入之间是线性关系，则我们有公式</p>
<script type="math/tex; mode=display">
y^*=x_1w_1+x_2w_2+b</script><p>其中w1,w2是权重，b是偏差。模型输出 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="2.096ex" height="2.029ex" role="img" focusable="false" viewbox="0 -691.8 926.6 896.8"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(523,363) scale(0.707)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"/></g></g></g></g></svg></mjx-container> 是线性回归对于真实价格y的预测与估计。</p>
<h4 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h4><p>我们需要寻找特定的模型的参数值，使得模型在数据上的误差尽可能的小。</p>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>我们收集一系列的数据，在这个数据上面寻找模型参数来使得模型的预测价格与真实价格的误差最小。这个数据集被称之为训练集。</p>
<p>假设我们采集的样本数为n，索引为i的样本的特征为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.669ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="2.548ex" role="img" focusable="false" viewbox="0 -830.4 1008.6 1126.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mi" transform="translate(605,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(605,-295.7) scale(0.707)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/></g></g></g></g></svg></mjx-container>和<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.669ex;" xmlns="http://www.w3.org/2000/svg" width="2.282ex" height="2.548ex" role="img" focusable="false" viewbox="0 -830.4 1008.6 1126.1"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msubsup"><g data-mml-node="mi"><path data-c="1D465" d="M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"/></g><g data-mml-node="mi" transform="translate(605,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mn" transform="translate(605,-295.7) scale(0.707)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g></g></svg></mjx-container> ，标签为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="1.848ex" height="2.343ex" role="img" focusable="false" viewbox="0 -830.4 817 1035.4"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mi" transform="translate(523,363) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"/></g></g></g></g></svg></mjx-container> ，对于索引为i的房屋，线性回归模型的房屋价格预测表达式为</p>
<script type="math/tex; mode=display">
y^{*(i)}=x^{i}_1w_1+x^{i}_2w_2+b</script><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>在模型训练中，我们需要衡量价格预测与真实值之间的误差，通常会选取一个非负作为误差，且数值越小表示误差越小，一个常用的式子为</p>
<script type="math/tex; mode=display">
\iota^i(w_1,w_2,b)=\frac{1}{2}(y^{*(i)}-y^{(i)})^2</script><p>其中常数1/2使对平方项求导后的常数系数为1，这样在形式上稍微简单，其中这个式子的误差越小表示预测价格与真实价格越相近。</p>
<p>通常我们用训练数据集中所有样本误差的平均来衡量模型预测的质量，即</p>
<script type="math/tex; mode=display">
\iota(w_1,w_2,b)=\frac{1}{n}\sum_{i=1}^{n}\iota^i(w_1,w_2,b)=\frac{1}{n}\sum_{i=1}^{n}\frac{1}{2}(x^i_1w_1+x^i_2w_2+b-y^i)^2</script><p>在模型的训练中，我们希望找出一组模型参数，记为$ w^<em>_1<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="0.629ex" height="0.713ex" role="img" focusable="false" viewbox="0 -121 278 315"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g></g></svg></mjx-container>w^</em>_2 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="0.629ex" height="0.713ex" role="img" focusable="false" viewbox="0 -121 278 315"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g></g></svg></mjx-container>b^*$，来使得训练样本平均损失最小：</p>
<script type="math/tex; mode=display">
w^*_1,w*_2,b^*=argmin_{(w_1,w_2,b)}\iota(w_1,w_2,b)</script><h4 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h4><p>我们通过小批量随机梯度下降法，来进行求出最优的$ w^<em>_1<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="0.629ex" height="0.713ex" role="img" focusable="false" viewbox="0 -121 278 315"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g></g></svg></mjx-container>w^</em>_2 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.439ex;" xmlns="http://www.w3.org/2000/svg" width="0.629ex" height="0.713ex" role="img" focusable="false" viewbox="0 -121 278 315"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g></g></g></svg></mjx-container>b^*$,使得目标函数最小，小批量随机梯度下降法具有特征在每次迭代中使用b个样本。Mini-Batch梯度下降算法与批量梯度下降算法类似，只不过它会选取B个样本来进行迭代，介于批量梯度下降算法与随机梯度下降算法之间，对于Mini-Batch而言B在
</p><p>批量梯度下降算法：在每次迭代中使用所有样本</p>
<p>随机梯度下降算法：在每次迭代中使用一个样本</p>
<p>Mini-Batch梯度下降算法：在每次迭代中使用b个样本</p>
<p><img src="/posts/b96b4738/小批量随机梯度下降.png" alt="小批量随机梯度下降"></p>
<p>图中是B为10的一个例子，Mini-batch在数据存取和求导的过程中使用向量化，进行并行计算，这样可以加快计算速度，而Mini-batch的一个缺点是需要确定参数B的大小。</p>
<p>对于上例中我们有算法</p>
<figure class="highlight maxima"><table><tr><td class="code"><pre><span class="line">Repeat {</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i=<span class="number">1</span>,i=b+<span class="number">1</span>,i=<span class="number">2b+1</span>,...n{</span><br></pre></td></tr></table></figure>
<script type="math/tex; mode=display">
w_1\leftarrow w_1-\frac{n}{|\beta|}(\sum_{i\in\beta}{}\frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial w_1})=w_1-\frac{n}{|\beta|}[\sum_{i\in\beta}{}x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})]</script><script type="math/tex; mode=display">
w_2\leftarrow w_2-\frac{n}{|\beta|}(\sum_{i\in\beta}{}\frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial w_2})=w_2-\frac{n}{|\beta|}[\sum_{i\in\beta}{}x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})]</script><script type="math/tex; mode=display">
b\leftarrow b-\frac{n}{|\beta|}(\sum_{i\in\beta}{}\frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial b})=b-\frac{n}{|\beta|}[\sum_{i\in\beta}{}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)})]</script><figure class="highlight"><table><tr><td class="code"><pre><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></table></figure>
<h2 id="线性回归的表示方法"><a href="#线性回归的表示方法" class="headerlink" title="线性回归的表示方法"></a>线性回归的表示方法</h2><p>我们对训练数据集里的3个房屋样本(索引分别为1、2和3)逐一预测价格，将得到</p>
<script type="math/tex; mode=display">
y^{*{(1)}}=x_1^{(1)}w_1+x_2^{(1)}w_2+b \\
y^{*{(2)}}=x_1^{(2)}w_1+x_2^{(2)}w_2+b \\
y^{*{(3)}}=x_1^{(3)}w_1+x_2^{(3)}w_2+b</script><p>将上面3个等式转化成矢量计算，则有</p>
<script type="math/tex; mode=display">
y^*=\left|\begin{matrix}
    y^{*(1)} \\
    y^{*(2)}\\
    y^{*(3)} \\
    \end{matrix} \right| ,
X=\left|\begin{matrix}
    x_1^{(1)}  \:\:x_2^{(1)}\\
    x_1^{(2)}  \:\:x_2^{(2)}\\
    x_1^{(3)}  \:\:x_2^{(3)} \\
    \end{matrix} \right| ,
W=\left|\begin{matrix}
    w_1 \\
    w_2\\
    \end{matrix} \right|</script><p>当数据样本数为n，特征数为d时，线性回归的矢量计算表达式为</p>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="13.148ex" height="2.034ex" role="img" focusable="false" viewbox="0 -694 5811.6 899"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"/></g><g data-mml-node="mo" transform="translate(523,363) scale(0.707)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"/></g></g><g data-mml-node="mo" transform="translate(1204.3,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mi" transform="translate(2260.1,0)"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"/></g><g data-mml-node="mi" transform="translate(3112.1,0)"><path data-c="1D44A" d="M436 683Q450 683 486 682T553 680Q604 680 638 681T677 682Q695 682 695 674Q695 670 692 659Q687 641 683 639T661 637Q636 636 621 632T600 624T597 615Q597 603 613 377T629 138L631 141Q633 144 637 151T649 170T666 200T690 241T720 295T759 362Q863 546 877 572T892 604Q892 619 873 628T831 637Q817 637 817 647Q817 650 819 660Q823 676 825 679T839 682Q842 682 856 682T895 682T949 681Q1015 681 1034 683Q1048 683 1048 672Q1048 666 1045 655T1038 640T1028 637Q1006 637 988 631T958 617T939 600T927 584L923 578L754 282Q586 -14 585 -15Q579 -22 561 -22Q546 -22 542 -17Q539 -14 523 229T506 480L494 462Q472 425 366 239Q222 -13 220 -15T215 -19Q210 -22 197 -22Q178 -22 176 -15Q176 -12 154 304T131 622Q129 631 121 633T82 637H58Q51 644 51 648Q52 671 64 683H76Q118 680 176 680Q301 680 313 683H323Q329 677 329 674T327 656Q322 641 318 637H297Q236 634 232 620Q262 160 266 136L501 550L499 587Q496 629 489 632Q483 636 447 637Q428 637 422 639T416 648Q416 650 418 660Q419 664 420 669T421 676T424 680T428 682T436 683Z"/></g><g data-mml-node="mo" transform="translate(4382.3,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"/></g><g data-mml-node="mi" transform="translate(5382.6,0)"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g></g></g></svg></mjx-container></p>
<p>其中模型输出$y^<em>\in R^{n</em>1}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="20.362ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 9000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">批</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">量</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">据</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">样</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">本</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">特</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">征</text></g></g></g></svg></mjx-container>X \in R^{n<em>d}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="6.787ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 3000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">权</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">重</text></g></g></g></svg></mjx-container>W \in R^{d</em>1}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="6.787ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 3000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">偏</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">差</text></g></g></g></svg></mjx-container>b \in R<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="20.362ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 9000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">批</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">量</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">数</text></g><g data-mml-node="mi" transform="translate(4000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">据</text></g><g data-mml-node="mi" transform="translate(5000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">样</text></g><g data-mml-node="mi" transform="translate(6000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">本</text></g><g data-mml-node="mi" transform="translate(7000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">标</text></g><g data-mml-node="mi" transform="translate(8000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">签</text></g></g></g></svg></mjx-container>y\in R^{n*1}<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.452ex;" xmlns="http://www.w3.org/2000/svg" width="9.05ex" height="2.149ex" role="img" focusable="false" viewbox="0 -750 4000 950"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><text data-variant="italic" transform="scale(1,-1)" font-size="884px" font-family="serif" font-style="italic">，</text></g><g data-mml-node="mi" transform="translate(1000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">设</text></g><g data-mml-node="mi" transform="translate(2000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">模</text></g><g data-mml-node="mi" transform="translate(3000,0)"><text data-variant="normal" transform="scale(1,-1)" font-size="884px" font-family="serif">型</text></g></g></g></svg></mjx-container>\theta=[w_1,w_2,b]^T$，我 们重写损失函数为</p>
<script type="math/tex; mode=display">
\iota(\theta)=\frac{1}{2n}(y^*-y)^T(y^*-y)</script><p>小批量随机梯度下降可以改写为</p>
<script type="math/tex; mode=display">
\theta\leftarrow \theta -\frac{n}{|\beta|}\sum_{i \in \beta}\bigtriangledown_{\theta}\iota^{(i)}(\theta)</script><script type="math/tex; mode=display">
\bigtriangledown_{\theta}\iota^{(i)}(\theta)=\left|\begin{matrix}
    \frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial w_1} \\
    \frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial w_2} \\
    \frac{\partial\iota^{(i)}(w_1,w_2,b)}{\partial b}  \\
    \end{matrix} \right|= \left|\begin{matrix}
    x_1^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)} \\
    x_2^{(i)}(x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)} \\
    x_1^{(i)}w_1+x_2^{(i)}w_2+b-y^{(i)}  \\
    \end{matrix} \right|= \left|\begin{matrix}
    x_1^{(i)} \\
    x_2^{(i)} \\
    1 \\
    \end{matrix} \right|(y^{*(i)}-y^{(i)})</script><h2 id="线性回归从零开始实现"><a href="#线性回归从零开始实现" class="headerlink" title="线性回归从零开始实现"></a>线性回归从零开始实现</h2><p>设我们训练数据集样本数为1000，输入个数(特征数)为2。给定随机生成的批量样本特征<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.09ex;" xmlns="http://www.w3.org/2000/svg" width="11.398ex" height="1.977ex" role="img" focusable="false" viewbox="0 -833.9 5037.9 873.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44B" d="M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"/></g><g data-mml-node="mo" transform="translate(1129.8,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"/></g><g data-mml-node="msup" transform="translate(2074.6,0)"><g data-mml-node="mi"><path data-c="1D445" d="M230 637Q203 637 198 638T193 649Q193 676 204 682Q206 683 378 683Q550 682 564 680Q620 672 658 652T712 606T733 563T739 529Q739 484 710 445T643 385T576 351T538 338L545 333Q612 295 612 223Q612 212 607 162T602 80V71Q602 53 603 43T614 25T640 16Q668 16 686 38T712 85Q717 99 720 102T735 105Q755 105 755 93Q755 75 731 36Q693 -21 641 -21H632Q571 -21 531 4T487 82Q487 109 502 166T517 239Q517 290 474 313Q459 320 449 321T378 323H309L277 193Q244 61 244 59Q244 55 245 54T252 50T269 48T302 46H333Q339 38 339 37T336 19Q332 6 326 0H311Q275 2 180 2Q146 2 117 2T71 2T50 1Q33 1 33 10Q33 12 36 24Q41 43 46 45Q50 46 61 46H67Q94 46 127 49Q141 52 146 61Q149 65 218 339T287 628Q287 635 230 637ZM630 554Q630 586 609 608T523 636Q521 636 500 636T462 637H440Q393 637 386 627Q385 624 352 494T319 361Q319 360 388 360Q466 361 492 367Q556 377 592 426Q608 449 619 486T630 554Z"/></g><g data-mml-node="TeXAtom" transform="translate(792,363) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mn"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(500,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1000,0)"/><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z" transform="translate(1500,0)"/></g><g data-mml-node="mo" transform="translate(2000,0)"><path data-c="2217" d="M229 286Q216 420 216 436Q216 454 240 464Q241 464 245 464T251 465Q263 464 273 456T283 436Q283 419 277 356T270 286L328 328Q384 369 389 372T399 375Q412 375 423 365T435 338Q435 325 425 315Q420 312 357 282T289 250L355 219L425 184Q434 175 434 161Q434 146 425 136T401 125Q393 125 383 131T328 171L270 213Q283 79 283 63Q283 53 276 44T250 35Q231 35 224 44T216 63Q216 80 222 143T229 213L171 171Q115 130 110 127Q106 124 100 124Q87 124 76 134T64 161Q64 166 64 169T67 175T72 181T81 188T94 195T113 204T138 215T170 230T210 250L74 315Q65 324 65 338Q65 353 74 363T98 374Q106 374 116 368T171 328L229 286Z"/></g><g data-mml-node="mn" transform="translate(2500,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g></g></g></g></g></svg></mjx-container>,我们使用线性回归模型真实权重<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="13.998ex" height="2.47ex" role="img" focusable="false" viewbox="0 -841.7 6187 1091.7"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D464" d="M580 385Q580 406 599 424T641 443Q659 443 674 425T690 368Q690 339 671 253Q656 197 644 161T609 80T554 12T482 -11Q438 -11 404 5T355 48Q354 47 352 44Q311 -11 252 -11Q226 -11 202 -5T155 14T118 53T104 116Q104 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Q21 293 29 315T52 366T96 418T161 441Q204 441 227 416T250 358Q250 340 217 250T184 111Q184 65 205 46T258 26Q301 26 334 87L339 96V119Q339 122 339 128T340 136T341 143T342 152T345 165T348 182T354 206T362 238T373 281Q402 395 406 404Q419 431 449 431Q468 431 475 421T483 402Q483 389 454 274T422 142Q420 131 420 107V100Q420 85 423 71T442 42T487 26Q558 26 600 148Q609 171 620 213T632 273Q632 306 619 325T593 357T580 385Z"/></g><g data-mml-node="mo" transform="translate(993.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mo" transform="translate(2049.6,0)"><path data-c="5B" d="M118 -250V750H255V710H158V-210H255V-250H118Z"/></g><g data-mml-node="mn" transform="translate(2327.6,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"/></g><g data-mml-node="mo" transform="translate(2827.6,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"/></g><g data-mml-node="mo" transform="translate(3272.2,0)"><path data-c="2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"/></g><g data-mml-node="mn" transform="translate(4050.2,0)"><path data-c="33" d="M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"/><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"/><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z" transform="translate(778,0)"/></g><g data-mml-node="msup" transform="translate(5328.2,0)"><g data-mml-node="mo"><path data-c="5D" d="M22 710V750H159V-250H22V-210H119V710H22Z"/></g><g data-mml-node="mi" transform="translate(311,363) scale(0.707)"><path data-c="1D447" d="M40 437Q21 437 21 445Q21 450 37 501T71 602L88 651Q93 669 101 677H569H659Q691 677 697 676T704 667Q704 661 687 553T668 444Q668 437 649 437Q640 437 637 437T631 442L629 445Q629 451 635 490T641 551Q641 586 628 604T573 629Q568 630 515 631Q469 631 457 630T439 622Q438 621 368 343T298 60Q298 48 386 46Q418 46 427 45T436 36Q436 31 433 22Q429 4 424 1L422 0Q419 0 415 0Q410 0 363 1T228 2Q99 2 64 0H49Q43 6 43 9T45 27Q49 40 55 46H83H94Q174 46 189 55Q190 56 191 56Q196 59 201 76T241 233Q258 301 269 344Q339 619 339 625Q339 630 310 630H279Q212 630 191 624Q146 614 121 583T67 467Q60 445 57 441T43 437H40Z"/></g></g></g></g></svg></mjx-container>和偏差<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.186ex;" xmlns="http://www.w3.org/2000/svg" width="6.879ex" height="1.756ex" role="img" focusable="false" viewbox="0 -694 3040.6 776"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D44F" d="M73 647Q73 657 77 670T89 683Q90 683 161 688T234 694Q246 694 246 685T212 542Q204 508 195 472T180 418L176 399Q176 396 182 402Q231 442 283 442Q345 442 383 396T422 280Q422 169 343 79T173 -11Q123 -11 82 27T40 150V159Q40 180 48 217T97 414Q147 611 147 623T109 637Q104 637 101 637H96Q86 637 83 637T76 640T73 647ZM336 325V331Q336 405 275 405Q258 405 240 397T207 376T181 352T163 330L157 322L136 236Q114 150 114 114Q114 66 138 42Q154 26 178 26Q211 26 245 58Q270 81 285 114T318 219Q336 291 336 325Z"/></g><g data-mml-node="mo" transform="translate(706.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"/></g><g data-mml-node="mn" transform="translate(1762.6,0)"><path data-c="34" d="M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"/><path data-c="2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z" transform="translate(500,0)"/><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" transform="translate(778,0)"/></g></g></g></svg></mjx-container>以及一个随机噪声项<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewbox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g></svg></mjx-container>来生成标签</p>
<script type="math/tex; mode=display">
y=Xw+b+\epsilon</script><p>其中噪声项<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.025ex;" xmlns="http://www.w3.org/2000/svg" width="0.919ex" height="1ex" role="img" focusable="false" viewbox="0 -431 406 442"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D716" d="M227 -11Q149 -11 95 41T40 174Q40 262 87 322Q121 367 173 396T287 430Q289 431 329 431H367Q382 426 382 411Q382 385 341 385H325H312Q191 385 154 277L150 265H327Q340 256 340 246Q340 228 320 219H138V217Q128 187 128 143Q128 77 160 52T231 26Q258 26 284 36T326 57T343 68Q350 68 354 58T358 39Q358 36 357 35Q354 31 337 21T289 0T227 -11Z"/></g></g></g></svg></mjx-container>服从均值为0、标准差为0.01的正态分布。噪声代表了数据集中无意义的干扰。</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="attribute">num_inputs</span>=2 #特征项数</span><br><span class="line"><span class="attribute">num_examples</span>=1000 #样本数</span><br><span class="line">true_w=[2,-3.4]</span><br><span class="line"><span class="attribute">true_b</span>=4.2</span><br><span class="line"><span class="attribute">features</span>=nd.random.normal(scale=1,shape(num_examples,num_inputs)) #生成1000<span class="number">*2</span>个为1的标准差</span><br><span class="line"><span class="attribute">labels</span>=true_w[0]<span class="number">*fea</span>tures[:,0]+true_w[1]<span class="number">*fea</span>tures[;，1]+true_b</span><br><span class="line">lables+=nd.random.normal(<span class="attribute">scale</span>=0.01,shape=labels.shape) #</span><br><span class="line"></span><br><span class="line"><span class="comment"># 该函数返回batch_size(批量大小)个随机样本的特征和标签</span></span><br><span class="line"></span><br><span class="line">def data_iter(batch_size,features,labels):</span><br><span class="line">	<span class="attribute">num_examples</span>=len(features) </span><br><span class="line">	<span class="attribute">indices</span>=list(range(num_examples)) #生成样本数量个随机数</span><br><span class="line">	random.shuffle(indices) #样本的读取顺序是随机的</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(0,num_examples,batch_size):</span><br><span class="line">		<span class="attribute">j</span>=nd.array(indices[i:min(i+batch_size,num_examples)])</span><br><span class="line">		yield features.take(j),labels.take(j) #take函数根据索引返回对应元素</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line"><span class="comment">#我们将权重初始化成均值为0、标准差为0.01的正态随机数，偏差则初始化成0</span></span><br><span class="line"><span class="attribute">w</span>=nd.random.normal(scale=0.01,shape=(num_inputs,1))</span><br><span class="line"><span class="attribute">b</span>=nd.zeros(shape=(1,))</span><br><span class="line"></span><br><span class="line"><span class="comment">#申请梯度</span></span><br><span class="line">w.attach_grad()</span><br><span class="line">b.attach_grad()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义函数 使用dot函数做矩阵乘法</span></span><br><span class="line">def linreg(X,w,b):	</span><br><span class="line">	return nd.dot(X,w)+b</span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="comment">#在实现中，我们需要把真实值y变成预测值y_hat的形状</span></span><br><span class="line">def squared_loss(y_hat,y):</span><br><span class="line">	return (y_hat-y.reshape(y.hat.shape))*<span class="number">*2</span>/2</span><br><span class="line"></span><br><span class="line"><span class="comment">#sgd函数实现了小批量随机梯度下降算法，通过不断迭代模型参数来优化损失函数。使用自动求梯度模块计算得来的#梯度是一个批量样本的梯度和，我们将它除以批量大小来得到平均值 </span></span><br><span class="line"><span class="comment"># params参数，lr学习率，batch_size批量大小</span></span><br><span class="line">def sgd(params,lr,batch_size):</span><br><span class="line">	<span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">		param[:]=param-lr*param.grad/batch_size</span><br><span class="line">		</span><br><span class="line"><span class="attribute">lr</span>=0.03	#学习率</span><br><span class="line"><span class="attribute">num_epochs</span>=3 #迭代周期</span><br><span class="line"><span class="attribute">net</span>=linreg	</span><br><span class="line"><span class="attribute">loss</span>=squared_loss</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):#训练模型一共需要num_epochs个迭代周期</span><br><span class="line">    #在每一个迭代周期中，会使用训练数据集中所有样本一次(假设样本数能够被批量大小整除)。</span><br><span class="line">    #x和y分别是小批量样本的特征和标签</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter(batch_size,features,labels):</span><br><span class="line">        with autograd.record():</span><br><span class="line">            <span class="attribute">l</span>=loss(net(X,w,b),y) #l是有关小批量x和y的损失</span><br><span class="line">        l.backward() #小批量的损失对模型参数求梯度</span><br><span class="line">        sgd([w,b],lr,batch_size) #使用小批量随机梯度下降迭代模型参数</span><br><span class="line">    <span class="attribute">train_l</span>=loss(net(features,w,b),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'epoch %d,loss %f'</span> % (epoch+1,train_l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220402214312255.png" alt="image-20220402214312255"></p>
<p>通过上面的计算，我们得到结果(true_w为刚开始定义的值,w为我们通过计算得出的值)</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="literal">true</span>_w,w</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220402214736512.png" alt="image-20220402214736512"></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="literal">true</span>_b,b</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220402215308326.png" alt="image-20220402215308326"></p>
<p>通过以上结果我们可以知道，以上算法是通过优化目标模型来反向求得参数，使得求得的参数最接近原参数。</p>
<h2 id="线性回归的简洁实现"><a href="#线性回归的简洁实现" class="headerlink" title="线性回归的简洁实现"></a>线性回归的简洁实现</h2><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet import autograd,nd</span><br><span class="line"></span><br><span class="line"><span class="attribute">num_inputs</span>=2</span><br><span class="line"><span class="attribute">num_examples</span>=1000</span><br><span class="line">true_w=[2,3.4]</span><br><span class="line"><span class="attribute">true_b</span>=4.2</span><br><span class="line"><span class="attribute">features</span>=nd.random.normal(scale=1,shape=(num_examples,num_inputs))</span><br><span class="line"><span class="attribute">labels</span>=true_w[0]<span class="number">*fea</span>tures[:,0]+true_w[1]<span class="number">*fea</span>tures[:,1]+true_b</span><br><span class="line">labels+=nd.random.normal(<span class="attribute">scale</span>=0.01,shape=labels.shape)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import data as gdata</span><br><span class="line"></span><br><span class="line"><span class="attribute">batch_size</span>=10</span><br><span class="line"><span class="comment">#将训练数据的特征和标签组合</span></span><br><span class="line"><span class="attribute">dataset</span>=gdata.ArrayDataset(features,labels)</span><br><span class="line"><span class="comment">#随机读取小批量</span></span><br><span class="line"><span class="attribute">data_iter</span>=gdata.DataLoader(dataset,batch_size,shuffle=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#定义模型</span></span><br><span class="line"><span class="comment">#Sequential为一个串联各个层的容器，可以看作一个画板</span></span><br><span class="line"><span class="comment">#Dense为全连接层，定义该层的输出为1</span></span><br><span class="line"><span class="comment">#Gluon中我们无须指定每一层输入的形状，如线性回归的输入个数，当模型得到数据时，如后面的执行net(X)时</span></span><br><span class="line"><span class="comment">#模型将自动推断出每一层的输入个数，如若是全相连的，通过层n推断n+1层的输入也是比较合理，且能实现的</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import nn</span><br><span class="line"><span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(1))</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line"><span class="comment">#从mxnet导入init模块，该模块提供了模型参数初始化的各种方法，通过init.Normal(sigma=0.01)指权重</span></span><br><span class="line"><span class="comment">#参数每个元素在初始化时随机采样于均值为0，标准差为0.01的正态分布，偏差参数默认会为0</span></span><br><span class="line"><span class="keyword">from</span> mxnet import init </span><br><span class="line">net.initialize(init.Normal(<span class="attribute">sigma</span>=0.01))</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数</span></span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import loss as gloss</span><br><span class="line"><span class="attribute">loss</span>=gloss.L2Loss() #平方损失又称L2范数损失</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义优化函数</span></span><br><span class="line"><span class="comment">#在导入Gluon之后，我们创建一个Trainer实例，并指定学习率为0.03的小批量随机梯度下降(sgd)为优化算法</span></span><br><span class="line"><span class="comment">#该优化算法用来迭代net实例所有通过add函数嵌套的层所包含的全部参数，这些参数可以通过collect_params</span></span><br><span class="line"><span class="comment">#函数获取</span></span><br><span class="line"><span class="keyword">from</span> mxnet import gluon</span><br><span class="line"><span class="attribute">trainer</span>=gluon.Trainer(net.collect_params(),'sgd',{<span class="string">'learning_rate'</span>:0.03})</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="comment">#在使用Gluon训练模型时，我们通过调用Trainer实例的step函数来迭代模型参数，由于变量l是长度为</span></span><br><span class="line"><span class="comment">#batch_size的一维NDArray，执行l.backward()等价于执行l.sum().backward()。按照小批量随机</span></span><br><span class="line"><span class="comment">#梯度下降的定义，我们在step函数中指明批量大小，从而对批量中样本梯度求平均</span></span><br><span class="line"><span class="attribute">num_epochs</span>=3;</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(1,num_epochs+1):</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        with autograd.record():</span><br><span class="line">             <span class="attribute">l</span>=loss(net(X),y)</span><br><span class="line">        l.backward()</span><br><span class="line">        trainer.<span class="keyword">step</span>(batch_size)</span><br><span class="line">    <span class="attribute">l</span>=loss(net(features),labels)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'epoch %d, loss:%f'</span> % (epoch,l.mean().asnumpy()))</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220404212518086.png" alt="image-20220404212518086"></p>
<p>下面我们分别比较学到的模型参数与真实的模型参数，从而从net获得需要的层，并访问其权重和偏差。</p>
<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line"><span class="title">dense</span>=net[<span class="number">0</span>]</span><br><span class="line"><span class="title">true_w</span>,dense.weight.<span class="class"><span class="keyword">data</span>()</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220404212656630.png" alt="image-20220404212656630"></p>
<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line"><span class="title">true_b</span>,dense.bias.<span class="class"><span class="keyword">data</span>()</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220404212748893.png" alt="image-20220404212748893"></p>
<h2 id="图像分类数据集-Fashion-MNIST"><a href="#图像分类数据集-Fashion-MNIST" class="headerlink" title="图像分类数据集(Fashion-MNIST)"></a>图像分类数据集(Fashion-MNIST)</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#导包</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取数据集，从网上获取数据集，通过train来指定获取训练数据集或测试数据集</span></span><br><span class="line">mnist_train=gdata.vision.FashionMNIST(train=<span class="literal">True</span>)</span><br><span class="line">mnist_test=gdata.vision.FashionMNIST(train=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mnist_train[0]同时返回feature与label,变量feature对应⾼和宽均为28像素的图像。每个像素的数值为0</span></span><br><span class="line"><span class="comment">#到255之间8位⽆符号整数（uint8)</span></span><br><span class="line">feature,label=mnist_train[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出 ((28,28,1),numpy.uint8)</span></span><br><span class="line">feature.shape,feature.dtype</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出 (2,numpy.int32,dtype('int32')) label为2，后面通过函数把标签的数字转成文字标签</span></span><br><span class="line">label,<span class="built_in">type</span>(label),label.dtype</span><br><span class="line"></span><br><span class="line"><span class="comment">#将标签中的数字转换成文字标签</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_fashion_mnist_labels</span>(<span class="params">labels</span>):</span><br><span class="line">    text_labels=[<span class="string">'t-shirt'</span>,<span class="string">'trouser'</span>,<span class="string">'pullover'</span>,<span class="string">'dress'</span>,<span class="string">'coat'</span>,</span><br><span class="line">                 <span class="string">'sandal'</span>,<span class="string">'shirt'</span>,<span class="string">'sneaker'</span>,<span class="string">'bag'</span>,<span class="string">'ankle boot'</span>]</span><br><span class="line">    <span class="keyword">return</span> [text_labels[<span class="built_in">int</span>(i)] <span class="keyword">for</span> i <span class="keyword">in</span> labels]</span><br><span class="line">    </span><br><span class="line"><span class="comment">#下⾯定义⼀个可以在⼀⾏⾥画出多张图像和对应标签的函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_fashion_mnist</span>(<span class="params">images,labels</span>):</span><br><span class="line">    d2l.use_svg_display()</span><br><span class="line">    _,figs=d2l.plt.subplots(<span class="number">1</span>,<span class="built_in">len</span>(images),figsize=(<span class="number">12</span>,<span class="number">12</span>))</span><br><span class="line">    <span class="keyword">for</span> f,img,lbl <span class="keyword">in</span> <span class="built_in">zip</span>(figs,images,labels):</span><br><span class="line">        f.imshow(img.reshape((<span class="number">28</span>,<span class="number">28</span>)).asnumpy())</span><br><span class="line">        f.set_title(lbl)</span><br><span class="line">        f.axes.get_xaxis().set_visible(<span class="literal">True</span>)</span><br><span class="line">        f.axes.get_yaxis().set_visible(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#我们看⼀下训练数据集中前9个样本的图像内容和⽂本标签</span></span><br><span class="line">X,y=mnist_train[<span class="number">0</span>:<span class="number">9</span>]</span><br><span class="line">show_fashion_mnist(X,get_fashion_mnist_labels(y))</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409155515489.png" alt="image-20220409155515489"></p>
<h2 id="softmax回归从零开始实现"><a href="#softmax回归从零开始实现" class="headerlink" title="softmax回归从零开始实现"></a>softmax回归从零开始实现</h2><figure class="highlight clean"><table><tr><td class="code"><pre><span class="line">%matplotlib <span class="keyword">inline</span></span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd,nd</span><br><span class="line"></span><br><span class="line">#设置批处理大小，且导入数据到训练集与测试集中</span><br><span class="line">batch_size=<span class="number">256</span></span><br><span class="line">train_iter,test_iter=d2l.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#因为softmax回归模型是一个单层神经网络</span></span><br><span class="line"><span class="comment">#又由于输入向量的长度为28*28=784，该向量的每个元素对应图像中的每个像素，由于图像有十个类别，所以单层</span></span><br><span class="line"><span class="comment">#神经网络的输出层的输出个数为10</span></span><br><span class="line"><span class="attribute">num_inputs</span>=784</span><br><span class="line"><span class="attribute">num_outputs</span>=10</span><br><span class="line"><span class="attribute">w</span>=nd.random.normal(scale=0.01,shape=(num_inputs,num_outputs))</span><br><span class="line"><span class="attribute">b</span>=nd.zeros(num_outputs)</span><br><span class="line">w.attach_grad()</span><br><span class="line">b.attach_grad()</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409181323633.png" alt="image-20220409181323633"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#在下⾯的函数中，矩阵X的⾏数是样本数，</span></span><br><span class="line"><span class="comment">#列数是输出个数。为了表达样本预测各个输出的概率， softmax运算会先通过exp函数对每个元素</span></span><br><span class="line"><span class="comment">#做指数运算，再对exp矩阵同⾏元素求和，最后令矩阵每⾏各元素与该⾏元素之和相除。这样⼀</span></span><br><span class="line"><span class="comment">#来，最终得到的矩阵每⾏元素和为1且⾮负。因此，该矩阵每⾏都是合法的概率分布。 softmax运</span></span><br><span class="line"><span class="comment">#算的输出矩阵中的任意⼀⾏元素代表了⼀个样本在各个输出类别上的预测概率</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">softmax</span>(<span class="params">X</span>):</span><br><span class="line">    X_exp=X.exp()</span><br><span class="line">    partition=X_exp.<span class="built_in">sum</span>(axis=<span class="number">1</span>,keepdims=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> X_exp/partition</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409181537778.png" alt="image-20220409181537778"></p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义了softmax的回归模型</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">net</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> softmax(nd.dot(X.reshape((-<span class="number">1</span>,num_inputs)),w)+b)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409181856448.png" alt="image-20220409181856448"></p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="comment">#损失函数</span></span><br><span class="line"><span class="comment">#对于样本i,使其他在正确标签的的时候值为1，其余的时候为0</span></span><br><span class="line"><span class="comment">#又因为每个样本只有一个标签，那么损失函数可能以进行简写</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cross_entropy</span>(<span class="params">y_hat,y</span>)</span><br><span class="line">	<span class="keyword">return</span> -nd.pick(y_hat,y).log()</span><br></pre></td></tr></table></figure>
<p>上面的损失函数等价于<script type="math/tex">-log_{y^{(i)}}y^{*(i)}</script>，其余部分后面实现</p>
<p><img src="/posts/b96b4738/image-20220409191415164.png" alt="image-20220409191415164"></p>
<p><img src="/posts/b96b4738/image-20220409191601846.png" alt="image-20220409191601846"></p>
<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义两个数之间的准确率</span></span><br><span class="line"><span class="comment">#相等条件判别式(y_hat.argmax(axis=1) == y)是⼀个值为0（相等为假）或1（相等</span></span><br><span class="line"><span class="comment">#为真）的NDArray。由于标签类型为整数，我们先将变量y变换为浮点数再进⾏相等条件判断。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">accuracy</span>(<span class="params">y_hat,y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y_hat.argmax(axis=<span class="number">1</span>)==y.astype(<span class="string">'float32'</span>)).mean().asscalar()</span><br></pre></td></tr></table></figure>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">def evaluate_accuracy(data_iter,net):</span><br><span class="line">    acc_sum,n=<span class="number">0.0</span>,<span class="number">0</span></span><br><span class="line">    for X,y in data_iter:</span><br><span class="line">        y=y.<span class="built_in">astype</span>(<span class="string">'float32'</span>)</span><br><span class="line">        acc_sum+=(<span class="built_in">net</span>(X).<span class="built_in">argmax</span>(axis=<span class="number">1</span>)==y).<span class="built_in">sum</span>().<span class="built_in">asscalar</span>()</span><br><span class="line">        n+=y.size</span><br><span class="line">    return acc_sum/n</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">num_epochs,<span class="attribute">lr</span>=5,0.1</span><br><span class="line"></span><br><span class="line">def train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,<span class="attribute">params</span>=None,lr=None,trainer=None):</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        train_l_sum,train_acc_sum,<span class="attribute">n</span>=0.0,0.0,0</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter: #train_iter 返回图像和标签</span><br><span class="line">            with autograd.record(): </span><br><span class="line">                <span class="attribute">y_hat</span>=net(X) #自己的y值</span><br><span class="line">                <span class="attribute">l</span>=loss(y_hat,y).sum()</span><br><span class="line">            l.backward() #求导</span><br><span class="line">            <span class="keyword">if</span> trainer is None:</span><br><span class="line">                d2l.sgd(params,lr,batch_size) #之前定义的小型梯度下降</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                trainer.<span class="keyword">step</span>(batch_size)</span><br><span class="line">            <span class="attribute">y</span>=y.astype('float32')</span><br><span class="line">            train_l_sum+=l.asscalar()</span><br><span class="line">            train_acc_sum+=(y_hat.argmax(<span class="attribute">axis</span>=1)==y).sum().asscalar()</span><br><span class="line">            n+=y.size</span><br><span class="line">        <span class="attribute">test_acc</span>=evaluate_accuracy(test_iter,net)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'epoch %d,loss %.4f,train acc %.3f,test acc %.3f'</span>%(epoch+1,train_l_sum/n,train_acc_sum/n,test_acc))</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">train<span class="constructor">_ch3(<span class="params">net</span>,<span class="params">train_iter</span>,<span class="params">test_iter</span>,<span class="params">cross_entropy</span>,<span class="params">num_epochs</span>,<span class="params">batch_size</span>,[<span class="params">w</span>,<span class="params">b</span>],<span class="params">lr</span>)</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409192303044.png" alt="image-20220409192303044"></p>
<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">#给定⼀系列图像（第三⾏图像输出），我们⽐较⼀下它们的真实标签（第⼀⾏⽂本输出）和模型预测结果（第⼆⾏⽂</span><br><span class="line">#本输出）</span><br><span class="line"><span class="keyword">for</span> X,y <span class="keyword">in</span> test_iter:</span><br><span class="line">    break</span><br><span class="line">true_labels=d2l<span class="selector-class">.get_fashion_mnist_labels</span>(y<span class="selector-class">.asnumpy</span>())</span><br><span class="line">pred_labels=d2l<span class="selector-class">.get_fashion_mnist_labels</span>(<span class="built_in">net</span>(X)<span class="selector-class">.argmax</span>(axis=<span class="number">1</span>)<span class="selector-class">.asnumpy</span>())</span><br><span class="line">titels=<span class="selector-attr">[true+<span class="string">'\n'</span>+pred for true,pred in zip(true_labels,pred_labels)]</span></span><br><span class="line">d2l<span class="selector-class">.show_fashion_mnist</span>(X<span class="selector-attr">[0:9]</span>,titels<span class="selector-attr">[0:9]</span>)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409192405732.png" alt="image-20220409192405732"></p>
<h2 id="softmax回归简洁实现"><a href="#softmax回归简洁实现" class="headerlink" title="softmax回归简洁实现"></a>softmax回归简洁实现</h2><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import d2lzh as d2l</span><br><span class="line"><span class="keyword">from</span> mxnet import autograd,nd</span><br><span class="line"><span class="keyword">from</span> mxnet import gluon,init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import loss as gloss,nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line"><span class="attribute">batch_size</span>=256</span><br><span class="line">train_iter,<span class="attribute">test_iter</span>=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义和初始化模型</span></span><br><span class="line"><span class="comment">#softmax回归的输出是一个全连接层，因此，我们添加一个输出个数为10的全连接层，我们使用均值0、标准差为#0.01的正态分布随机初始化模型的权重参数</span></span><br><span class="line"><span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(10))</span><br><span class="line">net.initialize(init.Normal(<span class="attribute">sigma</span>=0.01))</span><br><span class="line"></span><br><span class="line"><span class="comment">#softmax和交叉熵损失函数</span></span><br><span class="line"><span class="comment">#Gluon提供了⼀个包括softmax运算和交叉熵损失计算的函数。它的数值稳定性更好。</span></span><br><span class="line"><span class="attribute">loss</span>=gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义优化算法</span></span><br><span class="line"><span class="comment">#使用学习率为0.1的小批量随机梯度下降作为优化算法</span></span><br><span class="line"><span class="attribute">trainer</span>=gluon.Trainer(net.collect_params(),'sgd',{<span class="string">'learning_rate'</span>:0.1})</span><br><span class="line"></span><br><span class="line"><span class="comment">#训练模型</span></span><br><span class="line"><span class="attribute">num_epochs</span>=5</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer)</span><br></pre></td></tr></table></figure>
<h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><p><img src="/posts/b96b4738/image-20220409205054558.png" alt="image-20220409205054558"></p>
<h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p><img src="/posts/b96b4738/image-20220409205422168.png" alt="image-20220409205422168"></p>
<p><img src="/posts/b96b4738/image-20220409205503297.png" alt="image-20220409205503297"></p>
<h2 id="多层感知机的从零开始实现"><a href="#多层感知机的从零开始实现" class="headerlink" title="多层感知机的从零开始实现"></a>多层感知机的从零开始实现</h2><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import d2lzh as d2l</span><br><span class="line"><span class="keyword">from</span> mxnet import<span class="built_in"> nd</span></span><br><span class="line"><span class="built_in"></span><span class="keyword">from</span> mxnet.gluon import loss as gloss</span><br><span class="line"></span><br><span class="line"><span class="attribute">batch_size</span>=256</span><br><span class="line">train_iter,<span class="attribute">test_iter</span>=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Fashion-MNIST数据集中图像形状</span></span><br><span class="line"><span class="comment">#为28 × 28，类别数为10。本节中我们依然使⽤⻓度为28 × 28 = 784的向量表⽰每⼀张图像。因此，</span></span><br><span class="line"><span class="comment">#输⼊个数为784，输出个数为10。实验中，我们设超参数隐藏单元个数为256</span></span><br><span class="line">num_inputs,num_outputs,<span class="attribute">num_hiddens</span>=784,10,256</span><br><span class="line"><span class="attribute">W1</span>=nd.random.normal(scale=0.01,shape=(num_inputs,num_hiddens))</span><br><span class="line"><span class="attribute">b1</span>=nd.zeros(num_hiddens)</span><br><span class="line"><span class="attribute">W2</span>=nd.random.normal(scale=0.01,shape=(num_hiddens,num_outputs))</span><br><span class="line"><span class="attribute">b2</span>=nd.zeros(num_outputs)</span><br><span class="line">params=[W1,b1,W2,b2]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="多层感知机的简洁实现"><a href="#多层感知机的简洁实现" class="headerlink" title="多层感知机的简洁实现"></a>多层感知机的简洁实现</h2><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">import d2lzh as d2l</span><br><span class="line"><span class="keyword">from</span> mxnet import gluon,init</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import loss as gloss,nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#和softmax回归唯⼀的不同在于，我们多加了⼀个全连接层作为隐藏层。它的隐藏单元个数为256，</span></span><br><span class="line"><span class="comment">#并使⽤ReLU函数作为激活函数</span></span><br><span class="line"><span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(256,<span class="attribute">activation</span>=<span class="string">'relu'</span>),nn.Dense(10))</span><br><span class="line">net.initialize(init.Normal(<span class="attribute">sigma</span>=0.01))</span><br><span class="line"></span><br><span class="line"><span class="attribute">batch_size</span>=256</span><br><span class="line">train_iter,<span class="attribute">test_iter</span>=d2l.load_data_fashion_mnist(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="attribute">loss</span>=gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line"><span class="attribute">trainer</span>=gluon.Trainer(net.collect_params(),'sgd',{<span class="string">'learning_rate'</span>:0.5})</span><br><span class="line"><span class="attribute">num_epochs</span>=5</span><br><span class="line">d2l.train_ch3(net,train_iter,test_iter,loss,num_epochs,batch_size,None,None,trainer)</span><br></pre></td></tr></table></figure>
<h2 id="欠拟合与过拟合"><a href="#欠拟合与过拟合" class="headerlink" title="欠拟合与过拟合"></a>欠拟合与过拟合</h2><p>给定一个由标量数据特征x和对应的标题标签y组成的训练数据集，多项式函数拟合的目标是找一个K项多项式的函数</p>
<script type="math/tex; mode=display">
y^*=b+\sum_{k=1}^{K}x^kw_k</script><p>来近似，在上式中，<script type="math/tex">w_k</script>的模型的权重参数,b是偏差参数。</p>
<p>因为⾼阶多项式函数模型参数更多，模型函数的选择空间更⼤，所以⾼阶多项式函数⽐低阶多项式函数的复杂度更⾼。因此，⾼阶多项式函数⽐低阶多项式函数更容易在相同的训练数据集上得到更低的训练误差。给定训练数据集，模型复杂度和误差之间的关系通常如图3.4所⽰。给定训练数据集，如果模型的复杂度过低，很容易出现⽋拟合；如果模型复杂度过⾼，很容易出现过拟合。应对⽋拟合和过拟合的⼀个办法是针对数据集选择合适复杂度的模型。</p>
<p><img src="/posts/b96b4738/image-20220409220107017.png" alt="image-20220409220107017"></p>
<p>影响⽋拟合和过拟合的另⼀个重要因素是训练数据集的⼤小。⼀般来说，如果训练数据集中样本数过少，特别是⽐模型参数数量（按元素计）更少时，过拟合更容易发⽣。此外，泛化误差不会随训练数据集⾥样本数量增加而增⼤。因此，在计算资源允许的范围之内，我们通常希望训练数据集⼤⼀些，特别是在模型复杂度较⾼时，如层数较多的深度学习模型。  </p>
<figure class="highlight pgsql"><table><tr><td class="code"><pre><span class="line">%matplotlib <span class="keyword">inline</span></span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd,gluon,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata,loss <span class="keyword">as</span> gloss,nn</span><br><span class="line"></span><br><span class="line">n_train,n_test,true_w,true_b=<span class="number">100</span>,<span class="number">100</span>,[<span class="number">1.2</span>,<span class="number">-3.4</span>,<span class="number">5.6</span>],<span class="number">5</span></span><br><span class="line">features=nd.random.normal(shape=(n_train+n_test,<span class="number">1</span>))</span><br><span class="line">#拼接,ploy_featuers=x,x^<span class="number">2</span>,x^<span class="number">3</span></span><br><span class="line">ploy_features=nd.concat(features,nd.power(features,<span class="number">2</span>),nd.power(features,<span class="number">3</span>))</span><br><span class="line"><span class="meta">#y=1.2x-3.4x^2+5.6x^3+5</span></span><br><span class="line">labels=(true_w[<span class="number">0</span>]*ploy_features[:,<span class="number">0</span>]+true_w[<span class="number">1</span>]*ploy_features[:,<span class="number">1</span>]+true_w[<span class="number">2</span>]*ploy_features[:,<span class="number">2</span>]+true_b)</span><br><span class="line"><span class="meta">#y+=噪声项</span></span><br><span class="line">labels+=nd.random.normal(scale=<span class="number">0.1</span>,shape=labels.shape)</span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409225842985.png" alt="image-20220409225842985"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#作图函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">semilogy</span>(<span class="params">x_vals,y_vals,x_label,y_label,x2_vals=<span class="literal">None</span>,y2_vals=<span class="literal">None</span>,legend=<span class="literal">None</span>,figsize=(<span class="params"><span class="number">3.5</span>,<span class="number">2.5</span></span>)</span>):</span><br><span class="line">    d2l.set_figsize(figsize)</span><br><span class="line">    d2l.plt.xlabel(x_label)</span><br><span class="line">    d2l.plt.ylabel(y_label)</span><br><span class="line">    d2l.plt.semilogy(x_vals,y_vals)</span><br><span class="line">    <span class="keyword">if</span> x2_vals <span class="keyword">and</span> y2_vals:</span><br><span class="line">       d2l.plt.semilogy(x2_vals,y2_vals,linestyle=<span class="string">':'</span>)</span><br><span class="line">       d2l.plt.legend(legend)</span><br></pre></td></tr></table></figure>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">num_epochs,<span class="attribute">loss</span>=100,gloss.L2Loss()</span><br><span class="line">def fit_and_plot(train_features,test_features,train_labels,test_labels):</span><br><span class="line">    <span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(1)) #单层神经网络，一个输出单元</span><br><span class="line">    net.initialize() #初始化参数</span><br><span class="line">    <span class="attribute">batch_size</span>=min(10,train_labels.shape[0])</span><br><span class="line">    <span class="attribute">train_iter</span>=gdata.DataLoader(gdata.ArrayDataset(train_features,train_labels),batch_size,shuffle=True)</span><br><span class="line">    <span class="attribute">trainer</span>=gluon.Trainer(net.collect_params(),'sgd',{<span class="string">'learning_rate'</span>:0.01})</span><br><span class="line">    train_ls,test_ls=[], []</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            with autograd.record():</span><br><span class="line">                <span class="attribute">l</span>=loss(net(X),y)</span><br><span class="line">            l.backward()</span><br><span class="line">            trainer.<span class="keyword">step</span>(batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features),train_labels).mean().asscalar())</span><br><span class="line">        test_ls.append(loss(net(test_features),test_labels).mean().asscalar())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'final epoch:train loss'</span>,train_ls[-1],<span class="string">'test loss'</span>,test_ls[-1])</span><br><span class="line">    semilogy(range(1,num_epochs+1),train_ls,<span class="string">'epochs'</span>,<span class="string">'loss'</span>,range(1,num_epochs+1),test_ls,[<span class="string">'train'</span>,<span class="string">'test'</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'weight:'</span>,net[0].weight.data().asnumpy(),<span class="string">'\nbias:'</span>,net[0].bias.data().asnumpy())</span><br></pre></td></tr></table></figure>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">#三阶多项式函数拟合(正常)</span><br><span class="line">#我们先使⽤与数据⽣成函数同阶的三阶多项式函数拟合。实验表明，这个模型的训练误差和在测</span><br><span class="line">#试数据集的误差都较低。训练出的模型参数也接近真实值： w1 = <span class="number">1</span>:<span class="number">2</span>; w2 = −<span class="number">3</span>:<span class="number">4</span>; w3 = <span class="number">5</span>:<span class="number">6</span>; b = <span class="number">5</span>。</span><br><span class="line">#ploy_features<span class="literal">[:<span class="identifier">n_train</span>,:]</span>取前<span class="number">100</span>个元素，ploy_features<span class="literal">[<span class="identifier">n_train</span>:,:]</span>取后<span class="number">100</span>个元素</span><br><span class="line">fit<span class="constructor">_and_plot(<span class="params">ploy_features</span>[:<span class="params">n_train</span>,:],<span class="params">ploy_features</span>[<span class="params">n_train</span>:,:],<span class="params">labels</span>[:<span class="params">n_train</span>],<span class="params">labels</span>[<span class="params">n_train</span>:])</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220409232656580.png" alt="image-20220409232656580"></p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">#线性函数拟合(欠拟合)</span><br><span class="line">#这个地方，生成的函数是线性的，所以有欠拟合状态</span><br><span class="line">fit<span class="constructor">_and_plot(<span class="params">features</span>[:<span class="params">n_train</span>,:],<span class="params">features</span>[<span class="params">n_train</span>:,:],<span class="params">labels</span>[:<span class="params">n_train</span>],<span class="params">labels</span>[<span class="params">n_train</span>:])</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220410092456601.png" alt="image-20220410092456601"></p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">#训练样本不足(过拟合)</span><br><span class="line">fit<span class="constructor">_and_plot(<span class="params">ploy_features</span>[0:2,:],<span class="params">ploy_features</span>[<span class="params">n_train</span>:,:],<span class="params">labels</span>[0:2],<span class="params">labels</span>[<span class="params">n_train</span>:])</span></span><br></pre></td></tr></table></figure>
<p><img src="/posts/b96b4738/image-20220410092549529.png" alt="image-20220410092549529"></p>
<h2 id="权重衰减-原理暂不解释"><a href="#权重衰减-原理暂不解释" class="headerlink" title="权重衰减-原理暂不解释"></a>权重衰减-原理暂不解释</h2><p>​    <img src="/posts/b96b4738/image-20220410103609151.png" alt="image-20220410103609151"></p>
<p><img src="/posts/b96b4738/权重衰减1.png" alt="权重衰减1"></p>
<p>在上图中我们的更新公式为</p>
<script type="math/tex; mode=display">
\theta_j:=\theta_j(1-a\frac{\lambda}{m})-a\frac{1}{m}(\sum_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_j^{(i)})</script><script type="math/tex; mode=display">1-a\frac{\lambda}{m}<1$$不大于1，这样进行逐步进行迭代。

<figure class="highlight clean"><table><tr><td class="code"><pre><span class="line">%matplotlib <span class="keyword">inline</span></span><br><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd,gluon,init,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> data <span class="keyword">as</span> gdata,loss <span class="keyword">as</span> gloss,nn</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">n_train,n_test,<span class="attribute">num_inputs</span>=20,100,100</span><br><span class="line">true_w,<span class="attribute">true_b</span>=nd.ones((num_inputs,1))*0.01,0.05</span><br><span class="line"></span><br><span class="line"><span class="attribute">features</span>=nd.random.normal(shape=(n_train+n_test,num_inputs))</span><br><span class="line"><span class="attribute">labels</span>=nd.dot(features,true_w)+true_b</span><br><span class="line">labels+=nd.random.normal(<span class="attribute">scale</span>=0.01,shape=labels.shape)</span><br><span class="line"></span><br><span class="line">train_features,<span class="attribute">test_features</span>=features[:n_train,:],features[n_train:,:]</span><br><span class="line">train_labels,<span class="attribute">test_labels</span>=labels[:n_train],labels[n_train:]</span><br></pre></td></tr></table></figure>

![image-20220410104348188](动手学深度学习总结/image-20220410104348188.png)

<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">#初始化模型参数</span><br><span class="line">def init_params():</span><br><span class="line">    w=nd.random.<span class="built_in">normal</span>(scale=<span class="number">1</span>,shape=(num_inputs,<span class="number">1</span>))</span><br><span class="line">    b=nd.<span class="built_in">zeros</span>(shape=(<span class="number">1</span>,))</span><br><span class="line">    w.<span class="built_in">attach_grad</span>()</span><br><span class="line">    b.<span class="built_in">attach_grad</span>()</span><br><span class="line">    return [w,b]</span><br><span class="line"></span><br><span class="line">#定义l2范数惩罚项</span><br><span class="line">def <span class="built_in">l2_penalty</span>(w):</span><br><span class="line">    return (w**<span class="number">2</span>).<span class="built_in">sum</span>()/<span class="number">2</span></span><br><span class="line">    </span><br><span class="line">#定义训练与测试</span><br><span class="line">batch_size,num_epochs,lr=<span class="number">1</span>,<span class="number">100</span>,<span class="number">0.003</span></span><br><span class="line">net,loss=d2l.linreg,d2l.squared_loss</span><br><span class="line">train_iter=gdata.<span class="built_in">DataLoader</span>(gdata.<span class="built_in">ArrayDataset</span>(train_features,train_labels),batch_size,shuffle=True)</span><br><span class="line"></span><br><span class="line">def <span class="built_in">fit_and_plot</span>(lambd):</span><br><span class="line">    w,b=<span class="built_in">init_params</span>()</span><br><span class="line">    train_ls,test_ls=[],[]</span><br><span class="line">    for _ in <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        for X,y in train_iter:</span><br><span class="line">            with autograd.<span class="built_in">record</span>():</span><br><span class="line">                l=<span class="built_in">loss</span>(<span class="built_in">net</span>(X,w,b),y)+lambd*<span class="built_in">l2_penalty</span>(w)</span><br><span class="line">            l.<span class="built_in">backward</span>()</span><br><span class="line">            d2l.<span class="built_in">sgd</span>([w,b],lr,batch_size)</span><br><span class="line">        train_ls.<span class="built_in">append</span>(<span class="built_in">loss</span>(<span class="built_in">net</span>(train_features,w,b),train_labels).<span class="built_in">mean</span>().<span class="built_in">asscalar</span>())</span><br><span class="line">        test_ls.<span class="built_in">append</span>(<span class="built_in">loss</span>(<span class="built_in">net</span>(test_features,w,b),test_labels).<span class="built_in">mean</span>().<span class="built_in">asscalar</span>())</span><br><span class="line">    d2l.<span class="built_in">semilogy</span>(<span class="built_in">range</span>(<span class="number">1</span>,num_epochs+<span class="number">1</span>),train_ls,<span class="string">&#x27;epochs&#x27;</span>,<span class="string">&#x27;loss&#x27;</span>,<span class="built_in">range</span>(<span class="number">1</span>,num_epochs+<span class="number">1</span>),test_ls,[<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;L2 norm of w:&#x27;</span>,w.<span class="built_in">norm</span>().<span class="built_in">asscalar</span>())</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#观察过拟合</span></span><br><span class="line">fit_and_plot(<span class="attribute">lambd</span>=0)</span><br></pre></td></tr></table></figure>

![image-20220410104604559](动手学深度学习总结/image-20220410104604559.png)

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用权重衰减</span></span><br><span class="line">fit_and_plot(<span class="attribute">lambd</span>=3)</span><br></pre></td></tr></table></figure>

![image-20220410104636790](动手学深度学习总结/image-20220410104636790.png)

## 权重衰减简洁实现

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line">import d2lzh as d2l</span><br><span class="line"><span class="keyword">from</span> mxnet import autograd,gluon,init,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import data as gdata,loss as gloss,nn</span><br><span class="line"></span><br><span class="line">n_train,n_test,<span class="attribute">num_inputs</span>=20,100,100</span><br><span class="line">true_w,<span class="attribute">true_b</span>=nd.ones((num_inputs,1))*0.01,0.05</span><br><span class="line"></span><br><span class="line"><span class="attribute">features</span>=nd.random.normal(shape=(n_train+n_test,num_inputs))</span><br><span class="line"><span class="attribute">labels</span>=nd.dot(features,true_w)+true_b</span><br><span class="line">labels+=nd.random.normal(<span class="attribute">scale</span>=0.01,shape=labels.shape)</span><br><span class="line"></span><br><span class="line">train_features,<span class="attribute">test_features</span>=features[:n_train,:],features[n_train:,:]</span><br><span class="line">train_labels,<span class="attribute">test_labels</span>=labels[:n_train],labels[n_train:]</span><br><span class="line"></span><br><span class="line">batch_size,num_epochs,<span class="attribute">lr</span>=1,100,0.003</span><br><span class="line">net,<span class="attribute">loss</span>=d2l.linreg,d2l.squared_loss</span><br><span class="line"><span class="attribute">train_iter</span>=gdata.DataLoader(gdata.ArrayDataset(train_features,train_labels),batch_size,shuffle=True)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def fit_and_plot_gluon(wd):</span><br><span class="line">    <span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">    #nn.Dense</span><br><span class="line">    #Dense实现了如下操作:output = activation(dot(input, weight) + bias)，其中activation是	  #作为激活参数传递的元素激活函数，weight是由层创建的权重矩阵，bias是由层创建的偏差向量(仅在    	 #use_bias为<span class="literal">True</span>时适用)。  </span><br><span class="line">    net.<span class="built_in">add</span>(nn.Dense(1))</span><br><span class="line">    net.initialize(init.Normal(<span class="attribute">sigma</span>=1))</span><br><span class="line">    #nn.collect_params</span><br><span class="line">    #返回一个包含该Block及其所有子块Parameters的参数字典(默认)，也可以返回匹配某些给定正则表达式的	#select参数字典。</span><br><span class="line">    #wd也为函数的一个参数</span><br><span class="line">    #对权重参数衰减，权重名称一般以weight结尾</span><br><span class="line">    #不对偏差参数衰减，权重名称一般以bias结尾</span><br><span class="line">    <span class="attribute">trainer_w</span>=gluon.Trainer(net.collect_params(&#x27;.*weight&#x27;),&#x27;sgd&#x27;,&#123;<span class="string">&#x27;learning_rate&#x27;</span>:lr,<span class="string">&#x27;wd&#x27;</span>:wd&#125;)</span><br><span class="line">    <span class="attribute">trainer_b</span>=gluon.Trainer(net.collect_params(&#x27;.*bias&#x27;),&#x27;sgd&#x27;,&#123;<span class="string">&#x27;learning_rate&#x27;</span>:lr&#125;)</span><br><span class="line">    train_ls,test_ls=[],[]</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        <span class="keyword">for</span> X,y <span class="keyword">in</span> train_iter:</span><br><span class="line">            with autograd.record():</span><br><span class="line">                <span class="attribute">l</span>=loss(net(X),y)</span><br><span class="line">            l.backward()</span><br><span class="line">            #更新权重和偏差</span><br><span class="line">            trainer_w.<span class="keyword">step</span>(batch_size) #设置批量大小，设置梯度下降</span><br><span class="line">            trainer_b.<span class="keyword">step</span>(batch_size)</span><br><span class="line">        train_ls.append(loss(net(train_features),train_labels).mean().asscalar())</span><br><span class="line">        test_ls.append(loss(net(test_features),test_labels).mean().asscalar())</span><br><span class="line">    d2l.semilogy(range(1,num_epochs+1),train_ls,<span class="string">&#x27;epochs&#x27;</span>,<span class="string">&#x27;loss&#x27;</span>,range(1,num_epochs+1),test_ls,[<span class="string">&#x27;train&#x27;</span>,<span class="string">&#x27;test&#x27;</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;L2 norm of w:&#x27;</span>,net[0].weight.data().norm().asscalar())</span><br></pre></td></tr></table></figure>

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">fit_and_plot_gluon</span><span class="params">(<span class="number">0</span>)</span></span></span><br></pre></td></tr></table></figure>

![image-20220410115627382](动手学深度学习总结/image-20220410115627382.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">fit_and_plot_gluon</span><span class="params">(<span class="number">3</span>)</span></span></span><br></pre></td></tr></table></figure>

![image-20220410115644377](动手学深度学习总结/image-20220410115644377.png)

## 数值稳定性和模型初始化

![image-20220410123144883](动手学深度学习总结/image-20220410123144883.png)

# 深度学习计算

## 模型构造

****

![image-20220410173714617](动手学深度学习总结/image-20220410173714617.png)

<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">class FancyMLP(nn<span class="selector-class">.Block</span>):</span><br><span class="line">    def <span class="built_in">__init__</span>(self,**kwargs):</span><br><span class="line">        <span class="built_in">super</span>(FancyMLP,self).<span class="built_in">__init__</span>(**kwargs)</span><br><span class="line">        self.rand_weight=self.params.<span class="built_in">get_constant</span>(<span class="string">&#x27;rand_weight&#x27;</span>,nd.random.<span class="built_in">uniform</span>(shape=(<span class="number">20</span>,<span class="number">20</span>)))</span><br><span class="line">        self.dense=nn.<span class="built_in">Dense</span>(<span class="number">20</span>,activation=<span class="string">&#x27;relu&#x27;</span>)</span><br><span class="line">    def <span class="built_in">forward</span>(self,x):</span><br><span class="line">    # 使⽤创建的常数参数，以及NDArray的relu函数和dot函数</span><br><span class="line">        x=self.<span class="built_in">dense</span>(x)</span><br><span class="line">    # 复⽤全连接层。等价于两个全连接层共享参数</span><br><span class="line">        x=nd.<span class="built_in">relu</span>(nd.<span class="built_in">dot</span>(x,self.rand_weight.<span class="built_in">data</span>())+<span class="number">1</span>)</span><br><span class="line">    # 控制流，这⾥我们需要调⽤asscalar函数来返回标量进⾏⽐较</span><br><span class="line">        x=self.<span class="built_in">dense</span>(x)</span><br><span class="line">        x</span><br><span class="line">        while x.<span class="built_in">norm</span>().<span class="built_in">asscalar</span>()&gt;<span class="number">1</span>:</span><br><span class="line">            x/=<span class="number">2</span></span><br><span class="line">        if x.<span class="built_in">norm</span>().<span class="built_in">asscalar</span>()&lt;<span class="number">0.8</span>:</span><br><span class="line">            x*=<span class="number">10</span></span><br><span class="line">        return x.<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">net=<span class="built_in">FancyMLP</span>()</span><br><span class="line">net.<span class="built_in">initialize</span>()</span><br><span class="line"><span class="built_in">net</span>(X)</span><br></pre></td></tr></table></figure>

其中神经模型为以下过程

![构造复杂的模型](动手学深度学习总结/构造复杂的模型.png)

其中前后两个隐藏单元为同一层，共用同一参数ReLu(X)

## 模型参数的访问、初始化和共享

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet import init,nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line"><span class="attribute">net</span>=nn.Sequential()</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(256,<span class="attribute">activation</span>=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">net.<span class="built_in">add</span>(nn.Dense(10))</span><br><span class="line">net.initialize()</span><br><span class="line"></span><br><span class="line"><span class="attribute">X</span>=nd.random.uniform(shape=(2,20))</span><br><span class="line"><span class="attribute">Y</span>=net(X)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight crmsh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#访问第一层的参数</span></span><br><span class="line">net[<span class="number">0</span>].<span class="keyword">params</span>,<span class="keyword">type</span>(net[<span class="number">0</span>].<span class="keyword">params</span>)</span><br></pre></td></tr></table></figure>

![image-20220410203825121](动手学深度学习总结/image-20220410203825121.png)

<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#访问第一层权重的数据</span></span><br><span class="line"><span class="title">net</span>[<span class="number">0</span>].weight.<span class="class"><span class="keyword">data</span>()</span></span><br></pre></td></tr></table></figure>

![image-20220410203912239](动手学深度学习总结/image-20220410203912239.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">#第一层权重的梯度</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.grad</span>()</span><br></pre></td></tr></table></figure>

![image-20220410203953273](动手学深度学习总结/image-20220410203953273.png)

<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#第二层权重数据</span></span><br><span class="line"><span class="title">net</span>[<span class="number">1</span>].weight.<span class="class"><span class="keyword">data</span>()</span></span><br></pre></td></tr></table></figure>

![image-20220410204035268](动手学深度学习总结/image-20220410204035268.png)

<figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#神经网络中所有参数</span></span><br><span class="line">net.collect_params()</span><br></pre></td></tr></table></figure>

![image-20220410204113149](动手学深度学习总结/image-20220410204113149.png)

<figure class="highlight 1c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#神经网络中所有权重(不含偏差)</span></span><br><span class="line">net.collect_params(&#x27;.*weight&#x27;)</span><br></pre></td></tr></table></figure>

![image-20220410204153597](动手学深度学习总结/image-20220410204153597.png)

<figure class="highlight dos"><table><tr><td class="code"><pre><span class="line"><span class="built_in">net</span>.collect_params()</span><br></pre></td></tr></table></figure>

![image-20220410221835680](动手学深度学习总结/image-20220410221835680.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">#模型的默认初始化⽅法：权重参数元素为<span class="selector-attr">[-0.07, 0.07]</span>之间均匀分布的随机数，偏差参数则全为<span class="number">0</span>。但我们经常需#要使⽤其他⽅法来初始化权重。 MXNet的init模块⾥提供了多种预设的初始化⽅法。在下⾯的例⼦中，我们将权重</span><br><span class="line">#参数初始化成均值为<span class="number">0</span>、标准差为<span class="number">0.01</span>的正态分布随机数，并依然将偏差参数清零。</span><br><span class="line">net<span class="selector-class">.initialize</span>(init=init<span class="selector-class">.Normal</span>(sigma=<span class="number">0.01</span>),force_reinit=True)</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>()<span class="selector-attr">[0]</span></span><br></pre></td></tr></table></figure>

![image-20220410221400528](动手学深度学习总结/image-20220410221400528.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">net<span class="selector-class">.initialize</span>(init=init<span class="selector-class">.Normal</span>(sigma=<span class="number">0.01</span>),force_reinit=True)</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>()</span><br></pre></td></tr></table></figure>

![image-20220410221926023](动手学深度学习总结/image-20220410221926023.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">#使用常数来初始化权重参数</span><br><span class="line">net<span class="selector-class">.initialize</span>(init=init<span class="selector-class">.Constant</span>(<span class="number">1</span>),force_reinit=True)</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>()</span><br></pre></td></tr></table></figure>

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">#我们对隐藏层的权重使用Xavier随机初始化方法</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.initialize</span>(init=init<span class="selector-class">.Xavier</span>(),force_reinit=True)</span><br><span class="line">net<span class="selector-attr">[0]</span><span class="selector-class">.weight</span><span class="selector-class">.data</span>()<span class="selector-attr">[0]</span></span><br></pre></td></tr></table></figure>

![image-20220410222224319](动手学深度学习总结/image-20220410222224319.png)

![image-20220410222411304](动手学深度学习总结/image-20220410222411304.png)

## 模型参数的延后初始化

![image-20220410223649237](动手学深度学习总结/image-20220410223649237.png)

## 自定义层

![image-20220411113908180](动手学深度学习总结/image-20220411113908180.png)

# 卷积神经网络

## 二维卷积层

![image-20220411162009820](动手学深度学习总结/image-20220411162009820.png)

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">from</span> mxnet import autograd,nd</span><br><span class="line"><span class="attribute">from</span> mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入与核矩阵相乘</span></span><br><span class="line"><span class="attribute">def</span> corr2d(X,K):</span><br><span class="line">    <span class="attribute">h</span>,w=K.shape</span><br><span class="line">    <span class="attribute">Y</span>=nd.zeros((X.shape[<span class="number">0</span>]-h+<span class="number">1</span>,X.shape[<span class="number">1</span>]-w+<span class="number">1</span>))</span><br><span class="line">    <span class="attribute">for</span> i in range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="attribute">for</span> j in range(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="attribute">Y</span>[i,j]=(X[i:i+h,j:j+w]*K).sum()</span><br><span class="line">    <span class="attribute">return</span> Y</span><br><span class="line"></span><br><span class="line"><span class="attribute">X</span>=nd.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line"><span class="attribute">K</span>=nd.array([[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]])</span><br><span class="line"></span><br><span class="line"><span class="attribute">corr2d</span>(X,K)</span><br></pre></td></tr></table></figure>

![image-20220411162449392](动手学深度学习总结/image-20220411162449392.png)

<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="comment">#加上偏差</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2</span>D(nn.Block):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params"><span class="variable language_">self</span>,kernel_size,**kwargs</span>):</span><br><span class="line">        <span class="variable language_">super</span>(Conv2D,<span class="variable language_">self</span>).__init__(**kwargs)</span><br><span class="line">        <span class="variable language_">self</span>.weight=<span class="variable language_">self</span>.params.get(<span class="string">&#x27;weight&#x27;</span>,shape=kernel_size)</span><br><span class="line">        <span class="variable language_">self</span>.bias=<span class="variable language_">self</span>.params_get(<span class="string">&#x27;bias&#x27;</span>,shape=(<span class="number">1</span>,))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params"><span class="variable language_">self</span>,x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x,<span class="variable language_">self</span>.weight.data()+<span class="variable language_">self</span>.bias.data())</span><br></pre></td></tr></table></figure>

![image-20220411162714775](动手学深度学习总结/image-20220411162714775.png)

### 通过数据学习核数组

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 构造一个输出通道数为1，核数组形状是(1,2)的二维卷积层</span></span><br><span class="line"><span class="attribute">conv2d</span>=nn.Conv2D(<span class="number">1</span>,kernel_size=(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line"><span class="attribute">conv2d</span>.initialize()</span><br><span class="line"></span><br><span class="line"><span class="comment">#二维卷积层使用4维输入输出，格式为(样本，通道，高，宽)，这里批量大小(批量中的样本数)和通道数均为1</span></span><br><span class="line"><span class="attribute">X</span>=X.reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">6</span>,<span class="number">8</span>))</span><br><span class="line"><span class="attribute">Y</span>=Y.reshape((<span class="number">1</span>,<span class="number">1</span>,<span class="number">6</span>,<span class="number">7</span>))</span><br><span class="line"></span><br><span class="line"><span class="attribute">for</span> i in range(<span class="number">10</span>):</span><br><span class="line">    <span class="attribute">with</span> autograd.record():</span><br><span class="line">        <span class="attribute">Y_hat</span>=conv2d(X)</span><br><span class="line">        <span class="attribute">l</span>=(Y_hat-Y)**<span class="number">2</span></span><br><span class="line">    <span class="attribute">l</span>.backward()</span><br><span class="line">    <span class="attribute">conv2d</span>.weight.data()[:]-=<span class="number">3</span>e-<span class="number">2</span>*conv2d.weight.grad()</span><br><span class="line">    <span class="attribute">if</span> (i+<span class="number">1</span>)%<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        <span class="attribute">print</span>(&#x27;batch %d, loss %.<span class="number">3</span>f&#x27; %(i+<span class="number">1</span>,l.sum().asscalar()))</span><br></pre></td></tr></table></figure>

![image-20220411163718012](动手学深度学习总结/image-20220411163718012.png)

<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line"><span class="title">conv2d</span>.weight.<span class="class"><span class="keyword">data</span>().reshape((1,2))</span></span><br></pre></td></tr></table></figure>

![image-20220411163737479](动手学深度学习总结/image-20220411163737479.png)

可以看出参数与图像中物体边缘检测中我们设置的参数[1,-1]接近，所以我们对核进行了参数迭代。

![通过数据学习核数组](动手学深度学习总结/通过数据学习核数组.png)

## 填充与步幅

![image-20220411173136493](动手学深度学习总结/image-20220411173136493.png)

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet import<span class="built_in"> nd</span></span><br><span class="line"><span class="built_in"></span><span class="keyword">from</span> mxnet.gluon import nn</span><br><span class="line"><span class="comment">#定义一个函数来计算卷积层。它初始化卷积层权重，并对输入和输出做相应的升维与降维</span></span><br><span class="line">def comp_conv2d(conv2d,X):</span><br><span class="line">    conv2d.initialize()</span><br><span class="line">    #(1,1)代表批量大小和通道数均为1</span><br><span class="line">    <span class="attribute">X</span>=X.reshape((1,1)+X.shape)</span><br><span class="line">    <span class="attribute">Y</span>=conv2d(X)</span><br><span class="line">    return Y.reshape(Y.shape[2:])#排除不关心的前两维：批量与通道</span><br><span class="line"><span class="comment">#这里是两侧分别填充1行或列，所以在两侧一共填充2行与列</span></span><br><span class="line"><span class="attribute">conv2d</span>=nn.Conv2D(1,kernel_size=3,padding=1)</span><br><span class="line">conv2d</span><br><span class="line"></span><br><span class="line"><span class="attribute">X</span>=nd.random.uniform(shape=(8,8))</span><br><span class="line">comp_conv2d(conv2d,X).shape</span><br></pre></td></tr></table></figure>

![填充与步幅](动手学深度学习总结/填充与步幅.png)

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="comment">#使用高为5，宽为3的卷积核，在高和宽两侧的填充数分别为2和1</span></span><br><span class="line"><span class="attribute">conv2d</span>=nn.Conv2D(<span class="number">1</span>,kernel_size=(<span class="number">5</span>,<span class="number">3</span>),padding=(<span class="number">2</span>,<span class="number">1</span>))</span><br><span class="line"><span class="attribute">comp_conv2d</span>(conv2d,X).shape</span><br></pre></td></tr></table></figure>

![image-20220411174831149](动手学深度学习总结/image-20220411174831149.png)

![image-20220411174859742](动手学深度学习总结/image-20220411174859742.png)

## 多输入通道和多输出通道

### 多输入通道

![image-20220411180109484](动手学深度学习总结/image-20220411180109484.png)

![image-20220411180155152](动手学深度学习总结/image-20220411180155152.png)

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">import</span> d2lzh as d2l</span><br><span class="line"><span class="attribute">from</span> mxnet import nd </span><br><span class="line"></span><br><span class="line"><span class="attribute">def</span> corr2d_multi_in(X,K):</span><br><span class="line">    <span class="attribute">return</span> nd.add_n(*[d2l.corr2d(X,K) for X,K in zip(X,K)])</span><br><span class="line">    </span><br><span class="line"><span class="attribute">X</span>=nd.array([[[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]],[[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]]])</span><br><span class="line"><span class="attribute">K</span>=nd.array([[[<span class="number">0</span>,<span class="number">1</span>],[<span class="number">2</span>,<span class="number">3</span>]],[[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]]])</span><br><span class="line"></span><br><span class="line"><span class="attribute">corr2d_multi_in</span>(X,K)</span><br></pre></td></tr></table></figure>

通过在corr2d_multi_in函数里加*使得两个矩阵相加

![image-20220411180307603](动手学深度学习总结/image-20220411180307603.png)

### 多输出通道

![image-20220411180944244](动手学深度学习总结/image-20220411180944244.png)

<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d_multi_in_out</span>(<span class="params">X,K</span>):</span><br><span class="line">	<span class="comment">#对K的第0维遍历，每次同输入X做互相关计算，所有结果使用stack函数合并在一起</span></span><br><span class="line">    <span class="keyword">return</span> nd.stack(*[corr2d_multi_in(X,k) <span class="keyword">for</span> k <span class="keyword">in</span> K])</span><br></pre></td></tr></table></figure>

<figure class="highlight mathematica"><table><tr><td class="code"><pre><span class="line"><span class="type">#k</span><span class="operator">+</span><span class="number">1</span>等于<span class="built_in">K</span>中的每个元素加一</span><br><span class="line"><span class="built_in">K</span><span class="operator">=</span><span class="variable">nd</span><span class="operator">.</span><span class="variable">stack</span><span class="punctuation">(</span><span class="built_in">K</span><span class="operator">,</span><span class="built_in">K</span><span class="operator">+</span><span class="number">1</span><span class="operator">,</span><span class="built_in">K</span><span class="operator">+</span><span class="number">2</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">K</span><span class="operator">.</span><span class="variable">shape</span></span><br></pre></td></tr></table></figure>

![image-20220411181123216](动手学深度学习总结/image-20220411181123216.png)

3表示样本数，2表示通道数，2表示高，2表示宽

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">corr2d_multi_in_out</span><span class="params">(X,K)</span></span></span><br></pre></td></tr></table></figure>

 ![image-20220411181233717](动手学深度学习总结/image-20220411181233717.png)

## 1x1卷积层

![image-20220411203515169](动手学深度学习总结/image-20220411203515169.png)

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">def corr2d_multi_in_out_1x1(X,K):</span><br><span class="line">	#<span class="attribute">c_i</span>=3表三个输入,h高度,w宽度</span><br><span class="line">    c_i,h,<span class="attribute">w</span>=X.shape</span><br><span class="line">    #输出个数</span><br><span class="line">    <span class="attribute">c_o</span>=K.shape[0]</span><br><span class="line">    #3<span class="number">*9</span></span><br><span class="line">    <span class="attribute">X</span>=X.reshape((c_i,h*w))</span><br><span class="line">    #2<span class="number">*3</span></span><br><span class="line">    <span class="attribute">K</span>=K.reshape((c_o,c_i))</span><br><span class="line">    <span class="attribute">Y</span>=nd.dot(K,X)</span><br><span class="line">    return Y.reshape((c_o,h,w))</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="comment">#输入3*3*3</span></span><br><span class="line"><span class="attribute">X</span>=nd.random.uniform(shape=(<span class="number">3</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment">#核通道2，个数3，高和宽1*1</span></span><br><span class="line"><span class="attribute">K</span>=nd.random.uniform(shape=(<span class="number">2</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br></pre></td></tr></table></figure>

![image-20220411203933318](动手学深度学习总结/image-20220411203933318.png)

<figure class="highlight tp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Y</span><span class="number">1</span>=corr2d_multi_in_out_1x<span class="number">1</span>(<span class="keyword">X</span>,K)</span><br><span class="line"><span class="keyword">Y</span><span class="number">2</span>=corr2d_multi_in_out(<span class="keyword">X</span>,K)</span><br><span class="line">(<span class="keyword">Y</span><span class="number">1</span>-<span class="keyword">Y</span><span class="number">2</span>).norm().asscalar()&lt;<span class="number">1e-6</span></span><br></pre></td></tr></table></figure>

![image-20220411204101977](动手学深度学习总结/image-20220411204101977.png)

## 池化层

![image-20220411213253633](动手学深度学习总结/image-20220411213253633.png)

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">from</span> mxnet import nd</span><br><span class="line"><span class="attribute">from</span> mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line"><span class="attribute">def</span> pool2d(X,pool_size,mode=&#x27;max&#x27;):</span><br><span class="line">    <span class="attribute">p_h</span>,p_w=pool_size</span><br><span class="line">    <span class="attribute">Y</span>=nd.zeros((X.shape[<span class="number">0</span>]-p_h+<span class="number">1</span>,X.shape[<span class="number">1</span>]-p_w+<span class="number">1</span>))</span><br><span class="line">    <span class="attribute">for</span> i in range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="attribute">for</span> j in range(Y.shape[<span class="number">0</span>]):</span><br><span class="line">            <span class="attribute">if</span> mode==&#x27;max&#x27;:</span><br><span class="line">                <span class="attribute">Y</span>[i,j]=X[i:i+p_h,j:j+p_w].max()</span><br><span class="line">            <span class="attribute">elif</span> mode==&#x27;avg&#x27;:</span><br><span class="line">                <span class="attribute">Y</span>[i,j]=X[i:i+p_h,j:j+p_w].mean()</span><br><span class="line">    <span class="attribute">return</span> Y</span><br><span class="line">    </span><br><span class="line"><span class="attribute">X</span>=nd.array([[<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],[<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>]])</span><br><span class="line"><span class="attribute">pool2d</span>(X,(<span class="number">2</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

![image-20220411211219896](动手学深度学习总结/image-20220411211219896.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">pool2d</span><span class="params">(X,(<span class="number">2</span>,<span class="number">2</span>)</span></span>,<span class="string">&#x27;avg&#x27;</span>)</span><br></pre></td></tr></table></figure>

![image-20220411211238878](动手学深度学习总结/image-20220411211238878.png)

![image-20220411213329939](动手学深度学习总结/image-20220411213329939.png)

## 卷积神经网络(LeNet)

<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">import d2lzh <span class="keyword">as</span> d2l</span><br><span class="line">import mxnet <span class="keyword">as</span> mx</span><br><span class="line">from mxnet import autograd,gluon,init,nd</span><br><span class="line">from mxnet.gluon import loss <span class="keyword">as</span> gloss,nn</span><br><span class="line">import time</span><br><span class="line"></span><br><span class="line">net=nn.<span class="constructor">Sequential()</span></span><br><span class="line"># channels表过滤器的数量</span><br><span class="line">net.add(nn.<span class="constructor">Conv2D(<span class="params">channels</span>=6,<span class="params">kernel_size</span>=5,<span class="params">activation</span>=&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Conv2D(<span class="params">channels</span>=16,<span class="params">kernel_size</span>=5,<span class="params">activation</span>=&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">        # Dense会默认将(批量⼤⼩, 通道, ⾼, 宽)形状的输⼊转换成</span><br><span class="line">		# (批量⼤⼩, 通道<span class="operator"> * </span>⾼<span class="operator"> * </span>宽)形状的输⼊</span><br><span class="line">        nn.<span class="constructor">Dense(120,<span class="params">activation</span>=&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(84,<span class="params">activation</span>=&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(10)</span>)</span><br><span class="line"></span><br><span class="line">X=nd.random.uniform(shape=(<span class="number">1</span>,<span class="number">1</span>,<span class="number">28</span>,<span class="number">28</span>))</span><br><span class="line">net.initialize<span class="literal">()</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net:</span><br><span class="line">    X=layer(X)</span><br><span class="line">    print(layer.name,&#x27;output shape:\t&#x27;,<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>shape)</span><br></pre></td></tr></table></figure>

![image-20220412105434810](动手学深度学习总结/image-20220412105434810.png)

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="attribute">batch_size</span>=256</span><br><span class="line">train_iter,<span class="attribute">test_iter</span>=d2l.load_data_fashion_mnist(batch_size=batch_size)</span><br><span class="line"><span class="comment">#尝试使用gpu</span></span><br><span class="line">def try_gpu():</span><br><span class="line">    try:</span><br><span class="line">        <span class="attribute">ctx</span>=mx.gpu()</span><br><span class="line">        <span class="attribute">_</span>=nd.zeros((1,),ctx=ctx)</span><br><span class="line">    except mx.base.MXNetError:</span><br><span class="line">        <span class="attribute">ctx</span>=mx.cpu()</span><br><span class="line">    return ctx</span><br><span class="line"><span class="attribute">ctx</span>=try_gpu()</span><br><span class="line">ctx</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#这个函数是通过样本X，经过卷积神经网络来得到生成的y_hat是否与y相同</span></span><br><span class="line">def evaluate_accuracy(data_iter,net,ctx):</span><br><span class="line">    acc_sum,<span class="attribute">n</span>=nd.array([0],<span class="attribute">ctx</span>=ctx),0</span><br><span class="line">    <span class="keyword">for</span> X,y <span class="keyword">in</span> data_iter:</span><br><span class="line">        X,<span class="attribute">y</span>=X.as_in_context(ctx),y.as_in_context(ctx).astype(&#x27;float32&#x27;)</span><br><span class="line">        #net(X).argmax(<span class="attribute">axis</span>=1)表示返回矩阵中每行最大元素的索引</span><br><span class="line">        #相等差别式(y_hat.argmax(<span class="attribute">axis</span>=1)==y)是一个值为0(相等为假)或1(相等为真)的NDArray</span><br><span class="line">        acc_sum+=(net(X).argmax(<span class="attribute">axis</span>=1)==y).sum()</span><br><span class="line">        n+=y.size</span><br><span class="line">    return acc_sum.asscalar()/n</span><br></pre></td></tr></table></figure>

<figure class="highlight css"><table><tr><td class="code"><pre><span class="line">def train_ch5(net,train_iter,test_iter,batch_size,trainer,ctx,num_epochs):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;training on&#x27;</span>,ctx)</span><br><span class="line">    loss=gloss.<span class="built_in">SoftmaxCrossEntropyLoss</span>()</span><br><span class="line">    for epoch in <span class="built_in">range</span>(num_epochs):</span><br><span class="line">        train_l_sum,train_acc_sum,n,start=<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0</span>,time.<span class="built_in">time</span>()</span><br><span class="line">        for X,y in train_iter:</span><br><span class="line">            X,y=X.<span class="built_in">as_in_context</span>(ctx),y.<span class="built_in">as_in_context</span>(ctx)</span><br><span class="line">            with autograd.<span class="built_in">record</span>():</span><br><span class="line">                y_hat=<span class="built_in">net</span>(X)</span><br><span class="line">                l=<span class="built_in">loss</span>(y_hat,y).<span class="built_in">sum</span>()</span><br><span class="line">            l.<span class="built_in">backward</span>()</span><br><span class="line">            trainer.<span class="built_in">step</span>(batch_size)</span><br><span class="line">            y=y.<span class="built_in">astype</span>(<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">            train_l_sum+=l.<span class="built_in">asscalar</span>()</span><br><span class="line">            train_acc_sum+=(y_hat.<span class="built_in">argmax</span>(axis=<span class="number">1</span>)==y).<span class="built_in">sum</span>().<span class="built_in">asscalar</span>()</span><br><span class="line">            n+=y.size</span><br><span class="line">        test_acc=<span class="built_in">evaluate_accuracy</span>(test_iter,net,ctx)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;epoch %d,loss %.4f, train acc %.3f,test acc %.3f,time %.1f sec&#x27;</span> %(epoch+<span class="number">1</span>,train_l_sum/n,train_acc_sum/n,test_acc,time.<span class="built_in">time</span>()-start))</span><br></pre></td></tr></table></figure>

<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">lr,num_epochs=<span class="number">0.9</span>,<span class="number">5</span></span><br><span class="line">net.initialize(force_reinit=True,ctx=ctx,init=init.<span class="constructor">Xavier()</span>)</span><br><span class="line">trainer=gluon.<span class="constructor">Trainer(<span class="params">net</span>.<span class="params">collect_params</span>()</span>,&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr&#125;)</span><br><span class="line">train<span class="constructor">_ch5(<span class="params">net</span>,<span class="params">train_iter</span>,<span class="params">test_iter</span>,<span class="params">batch_size</span>,<span class="params">trainer</span>,<span class="params">ctx</span>,<span class="params">num_epochs</span>)</span></span><br></pre></td></tr></table></figure>

![image-20220412110156595](动手学深度学习总结/image-20220412110156595.png)

## 批量归一化

<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">import d2lzh <span class="keyword">as</span> d2l</span><br><span class="line">from mxnet import autograd,gluon,init,nd</span><br><span class="line">from mxnet.gluon import nn</span><br><span class="line"></span><br><span class="line">def batch<span class="constructor">_norm(X,<span class="params">gamma</span>,<span class="params">beta</span>,<span class="params">moving_mean</span>,<span class="params">moving_var</span>,<span class="params">eps</span>,<span class="params">momentum</span>)</span>:</span><br><span class="line">		#通过autograd来判断当前模式是训练模式还是预测模式</span><br><span class="line">    <span class="keyword">if</span> not autograd.is<span class="constructor">_training()</span>:</span><br><span class="line">    	#如果是在预测模式下，直接使用传入的移动平均所得的均值与方差</span><br><span class="line">        X_hat=(X-moving_mean)/nd.sqrt(moving_var+eps)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">assert</span> len(<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>shape) <span class="keyword">in</span> (<span class="number">2</span>,<span class="number">4</span>)</span><br><span class="line">        #使用全连接层的情况，计算特征维上的均值与方差</span><br><span class="line">        <span class="keyword">if</span> len(<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>shape)==<span class="number">2</span>:</span><br><span class="line">            mean=<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>mean(axis=<span class="number">0</span>)</span><br><span class="line">            var=((X-mean)**<span class="number">2</span>).mean(axis=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">        #使用二维卷积层的情况，计算通道维上(axis=<span class="number">1</span>)的均值与方差，这里我们需要保持</span><br><span class="line">        #X的开关以便后面可以做广播运算</span><br><span class="line">            mean=<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdims=True)</span><br><span class="line">            var=((X-mean)**<span class="number">2</span>).mean(axis=(<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>),keepdims=True)</span><br><span class="line">        #训练模式下用当前的均值和方差做标准化</span><br><span class="line">        X_hat=(X-mean)/nd.sqrt(var+eps)</span><br><span class="line">        #更新移动平均的均值与方差</span><br><span class="line">        moving_mean=momentum*moving_mean+(<span class="number">1.0</span>-momentum)*mean</span><br><span class="line">        moving_var=momentum*moving_var+(<span class="number">1.0</span>-momentum)*var</span><br><span class="line">    Y=gamma*X_hat+beta</span><br><span class="line">    return Y,moving_mean,moving_var</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="constructor">BatchNorm(<span class="params">nn</span>.Block)</span>:</span><br><span class="line">    def <span class="constructor">__init__(<span class="params">self</span>,<span class="params">num_features</span>,<span class="params">num_dims</span>,<span class="operator">**</span><span class="params">kwargs</span>)</span>:</span><br><span class="line">        super(BatchNorm,self).<span class="constructor">__init__(<span class="operator">**</span><span class="params">kwargs</span>)</span></span><br><span class="line">        <span class="keyword">if</span> num_dims==<span class="number">2</span>:</span><br><span class="line">            shape=(<span class="number">1</span>,num_features)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            shape=(<span class="number">1</span>,num_features,<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        #参与求梯度和迭代的拉伸和偏移参数，分别初始化为<span class="number">0</span>和<span class="number">1</span></span><br><span class="line">        self.gamma=self.params.get(&#x27;gamma&#x27;,shape=shape,init=init.<span class="constructor">One()</span>)</span><br><span class="line">        self.beta=self.params.get(&#x27;beta&#x27;,shape=shape,init=init.<span class="constructor">Zero()</span>)</span><br><span class="line">        #不参与求梯度和迭代的变量，全在内存上初始化成<span class="number">0</span></span><br><span class="line">        self.moving_mean=nd.zeros(shape)</span><br><span class="line">        self.moving_var=nd.zeros(shape)</span><br><span class="line">        </span><br><span class="line">    def forward(self,X):</span><br><span class="line">    	#如果X不在内存上，将moving_mean和moving_var复制到X所在的显存上</span><br><span class="line">        <span class="keyword">if</span> self.moving_mean.context!=<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>context:</span><br><span class="line">            self.moving_mean=self.moving_mean.copyto(<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>context)</span><br><span class="line">            self.moving_var=self.moving_var.copyto(<span class="module-access"><span class="module"><span class="identifier">X</span>.</span></span>context)</span><br><span class="line">        #保存更新过的moving_mean和moving_var</span><br><span class="line">        Y,self.moving_mean,self.moving_var=batch<span class="constructor">_norm(X,<span class="params">self</span>.<span class="params">gamma</span>.<span class="params">data</span>()</span>,</span><br><span class="line">        self.beta.data<span class="literal">()</span>,self.moving_mean,self.moving_var,eps=<span class="number">1e-5</span>,momentum=<span class="number">0.9</span>)</span><br><span class="line">        return Y</span><br><span class="line">        </span><br><span class="line"></span><br><span class="line">net=nn.<span class="constructor">Sequential()</span></span><br><span class="line">net.add(nn.<span class="constructor">Conv2D(6,<span class="params">kernel_size</span>=5)</span>,</span><br><span class="line">        <span class="constructor">BatchNorm(6,<span class="params">num_dims</span>=4)</span>,</span><br><span class="line">        nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Conv2D(16,<span class="params">kernel_size</span>=4)</span>,</span><br><span class="line">        <span class="constructor">BatchNorm(16,<span class="params">num_dims</span>=4)</span>,</span><br><span class="line">        nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(120)</span>,</span><br><span class="line">        <span class="constructor">BatchNorm(120,<span class="params">num_dims</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(84)</span>,</span><br><span class="line">        <span class="constructor">BatchNorm(84,<span class="params">num_dims</span>=2)</span>,</span><br><span class="line">        nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">        nn.<span class="constructor">Dense(10)</span>)</span><br><span class="line"> </span><br><span class="line">lr,num_epochs,batch_size,ctx=<span class="number">1.0</span>,<span class="number">5</span>,<span class="number">256</span>,d2l.<span class="keyword">try</span><span class="constructor">_gpu()</span></span><br><span class="line">net.initialize(ctx=ctx,init=init.<span class="constructor">Xavier()</span>)</span><br><span class="line">trainer=gluon.<span class="constructor">Trainer(<span class="params">net</span>.<span class="params">collect_params</span>()</span>,&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr&#125;)</span><br><span class="line">train_iter,test_iter=d2l.load<span class="constructor">_data_fashion_mnist(<span class="params">batch_size</span>)</span></span><br><span class="line">d2l.train<span class="constructor">_ch5(<span class="params">net</span>,<span class="params">train_iter</span>,<span class="params">test_iter</span>,<span class="params">batch_size</span>,<span class="params">trainer</span>,<span class="params">ctx</span>,<span class="params">num_epochs</span>)</span></span><br></pre></td></tr></table></figure>

![image-20220412162432934](动手学深度学习总结/image-20220412162432934.png)

<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#查看第一批量归化一层学习到的拉伸参数gamma和偏移参数beta</span></span><br><span class="line"><span class="title">net</span>[<span class="number">1</span>].gamma.<span class="class"><span class="keyword">data</span>().reshape((-1,)),net[1].beta.<span class="keyword">data</span>().reshape((-1,))</span></span><br></pre></td></tr></table></figure>

![image-20220412162614282](动手学深度学习总结/image-20220412162614282.png)

## 批量归一化的简洁实现

<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">    #使用gluon的批量归一化</span><br><span class="line">    net=nn.<span class="constructor">Sequential()</span></span><br><span class="line">    net.add(nn.<span class="constructor">Conv2D(6,<span class="params">kernel_size</span>=5)</span>,</span><br><span class="line">    nn.<span class="constructor">BatchNorm()</span>,</span><br><span class="line">    nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">    nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">    nn.<span class="constructor">Conv2D(16,<span class="params">kernel_size</span>=5)</span>,</span><br><span class="line">    nn.<span class="constructor">BatchNorm()</span>,</span><br><span class="line">     nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">    nn.<span class="constructor">MaxPool2D(<span class="params">pool_size</span>=2,<span class="params">strides</span>=2)</span>,</span><br><span class="line">    nn.<span class="constructor">Dense(120)</span>,</span><br><span class="line">    nn.<span class="constructor">BatchNorm()</span>,</span><br><span class="line">     nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">    nn.<span class="constructor">Dense(84)</span>,</span><br><span class="line">    nn.<span class="constructor">BatchNorm()</span>,</span><br><span class="line">      nn.<span class="constructor">Activation(&#x27;<span class="params">sigmoid</span>&#x27;)</span>,</span><br><span class="line">    nn.<span class="constructor">Dense(10)</span></span><br><span class="line">    )</span><br><span class="line"> net.initialize(ctx=ctx,init=init.<span class="constructor">Xavier()</span>)</span><br><span class="line">trainer=gluon.<span class="constructor">Trainer(<span class="params">net</span>.<span class="params">collect_params</span>()</span>,&#x27;sgd&#x27;,&#123;&#x27;learning_rate&#x27;:lr&#125;)</span><br><span class="line">d2l.train<span class="constructor">_ch5(<span class="params">net</span>,<span class="params">train_iter</span>,<span class="params">test_iter</span>,<span class="params">batch_size</span>,<span class="params">trainer</span>,<span class="params">ctx</span>,<span class="params">num_epochs</span>)</span></span><br></pre></td></tr></table></figure>

![image-20220412165145598](动手学深度学习总结/image-20220412165145598.png)

## 残差网络

![image-20220412180722522](动手学深度学习总结/image-20220412180722522.png)

<figure class="highlight haskell"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="title">from</span> mxnet <span class="keyword">import</span> gluon,init,nd</span><br><span class="line"><span class="title">from</span> mxnet.gluon <span class="keyword">import</span> nn</span><br><span class="line"><span class="class"></span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="type">Residual</span>(<span class="title">nn</span>.<span class="type">Block</span>):</span></span><br><span class="line"><span class="class">    def __init__(<span class="title">self</span>,<span class="title">num_channels</span>,<span class="title">use_1x1conv</span>=<span class="type">False</span>,<span class="title">strides</span>=1,**<span class="title">kwargs</span>):</span></span><br><span class="line"><span class="class">        super(<span class="type">Residual</span>,<span class="title">self</span>).__init__(**<span class="title">kwargs</span>)</span></span><br><span class="line"><span class="class">        self.conv1=nn.<span class="type">Conv2D</span>(<span class="title">num_channels</span>,<span class="title">kernel_size</span>=3,<span class="title">padding</span>=1,<span class="title">strides</span>=<span class="title">strides</span>)</span></span><br><span class="line"><span class="class">        self.conv2=nn.<span class="type">Conv2D</span>(<span class="title">num_channels</span>,<span class="title">kernel_size</span>=3,<span class="title">padding</span>=1)</span></span><br><span class="line"><span class="class">        </span></span><br><span class="line"><span class="class">        if use_1x1conv:</span></span><br><span class="line"><span class="class">            self.conv3=nn.<span class="type">Conv2D</span>(<span class="title">num_channels</span>,<span class="title">kernel_size</span>=1,<span class="title">strides</span>=<span class="title">strides</span>)</span></span><br><span class="line"><span class="class">        else:</span></span><br><span class="line"><span class="class">            self.conv3=<span class="type">None</span></span></span><br><span class="line"><span class="class">        self.bn1=nn.<span class="type">BatchNorm</span>()</span></span><br><span class="line"><span class="class">        self.bn2=nn.<span class="type">BatchNorm</span>()</span></span><br><span class="line"><span class="class">    def forward(<span class="title">self</span>,<span class="type">X</span>):</span></span><br><span class="line"><span class="class">        <span class="type">Y</span>=nd.relu(<span class="title">self</span>.<span class="title">bn1</span>(<span class="title">self</span>.<span class="title">conv1</span>(<span class="type">X</span>)))</span></span><br><span class="line"><span class="class">        <span class="type">Y</span>=self.bn2(<span class="title">self</span>.<span class="title">conv2</span>(<span class="type">Y</span>))</span></span><br><span class="line"><span class="class">        if self.conv3:</span></span><br><span class="line"><span class="class">            <span class="type">X</span>=self.conv3(<span class="type">X</span>)</span></span><br><span class="line"><span class="class">        return nd.relu(<span class="type">Y</span>+<span class="type">X</span>)</span></span><br></pre></td></tr></table></figure>

可以通过调用跳过第三层卷积层。

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">blk=<span class="built_in">Residual</span>(<span class="number">3</span>)</span><br><span class="line">blk<span class="selector-class">.initialize</span>()</span><br><span class="line">X=nd<span class="selector-class">.random</span><span class="selector-class">.uniform</span>(shape=(<span class="number">4</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">6</span>))</span><br><span class="line"><span class="function"><span class="title">blk</span><span class="params">(X)</span></span>.shape</span><br></pre></td></tr></table></figure>

![image-20220412180853217](动手学深度学习总结/image-20220412180853217.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">blk=<span class="built_in">Residual</span>(<span class="number">6</span>,use_1x1conv=True,strides=<span class="number">2</span>)</span><br><span class="line">blk<span class="selector-class">.initialize</span>()</span><br><span class="line"><span class="function"><span class="title">blk</span><span class="params">(X)</span></span>.shape</span><br></pre></td></tr></table></figure>

![image-20220412180949437](动手学深度学习总结/image-20220412180949437.png)

### ResNet模型

![image-20220412181057419](动手学深度学习总结/image-20220412181057419.png)

![image-20220412190440555](动手学深度学习总结/image-20220412190440555.png)

# 循环神经网络

假设有一段长度为T的文本中的词依次为$$w_1,w_2,...,w_t$$，那么在离散的时间序列中，$$w_t(1\leq t\leq T)$$可看作在时间步T的输出或标签，即有语言模型</script><p>P(w_1,w_2,…,w_t)</p>
<script type="math/tex; mode=display">
假设序列$$w_1,w_2,...,w_t$$中的每个词依次生成的，我们有</script><p>P(w_1,w_2,…,w_t)=\prod_{t=1}^Tp(w_t|w_1,…,w_{t-1})</p>
<script type="math/tex; mode=display">
其中$$p(w_2|w_1)$$可以计算为$$w_1$$和$$w_2$$两词相邻的频率与$$w_1$$词频的比值，因为该比值即$$p(w_1,w_2)$$与$$p(w_1)$$之比

![image-20220415124314823](动手学深度学习总结/image-20220415124314823.png)

对这部分我们有公式

![image-20220415124353842](动手学深度学习总结/image-20220415124353842.png)

$$H_{t-1}$$为上一层的隐藏状态，所以这表示该层的隐藏状态与上一层的隐藏状态有关，所以会导致上一层的隐藏状态会影响这一层的隐藏状态，而输出层又为

![image-20220415124442889](动手学深度学习总结/image-20220415124442889.png)



## 语言模型数据集

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line"><span class="keyword">import</span> random </span><br><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取数据集</span></span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">&#x27;data/jaychou_lyrics.txt.zip&#x27;</span>) <span class="keyword">as</span> zin:</span><br><span class="line">    <span class="keyword">with</span> zin.<span class="built_in">open</span>(<span class="string">&#x27;jaychou_lyrics.txt&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        corpus_chars=f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">corpus_chars[:<span class="number">40</span>] <span class="comment">#读取前40个字符 </span></span><br></pre></td></tr></table></figure>

![image-20220414214551904](动手学深度学习总结/image-20220414214551904.png)

<figure class="highlight ini"><table><tr><td class="code"><pre><span class="line"><span class="comment">#把换行符替换成空格，使用前1万个字符来训练模型</span></span><br><span class="line"><span class="attr">corpus_chars</span>=corpus_chars.replace(<span class="string">&#x27;\n&#x27;</span>,<span class="string">&#x27; &#x27;</span>).replace(<span class="string">&#x27;\r&#x27;</span>,<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"><span class="attr">corpus_chars</span>=corpus_chars[<span class="number">0</span>:<span class="number">10000</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#建立字符索引，将字符映射成数字</span></span><br><span class="line"><span class="attribute">idx_to_char</span>=list(set(corpus_chars))</span><br><span class="line"><span class="comment">#字符组映射成数字组</span></span><br><span class="line"><span class="attribute">char_to_idx</span>=dict([(char,i)<span class="keyword">for</span> i,char <span class="keyword">in</span> enumerate(idx_to_char)])</span><br><span class="line"><span class="attribute">vocab_size</span>=len(char_to_idx)</span><br><span class="line">vocab_size</span><br><span class="line"></span><br><span class="line"><span class="comment">#分别打印出字符组与所对应的数字组</span></span><br><span class="line">corpus_indices=[char_to_idx[char] <span class="keyword">for</span> char <span class="keyword">in</span> corpus_chars]</span><br><span class="line"><span class="attribute">sample</span>=corpus_indices[:20]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;chars:&#x27;</span>,<span class="string">&#x27;&#x27;</span>.join([idx_to_char[idx] <span class="keyword">for</span> idx <span class="keyword">in</span> sample]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;indices:&#x27;</span>,sample)</span><br></pre></td></tr></table></figure>

![image-20220414215208400](动手学深度学习总结/image-20220414215208400.png)

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#随机采样 </span></span><br><span class="line"><span class="comment">#batch_size指每个小批量的样本数，num_steps为每个样本包含的时间步数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">data_iter_random</span>(<span class="params">corpus_indices,batch_size,num_steps,ctx=<span class="literal">None</span></span>):</span><br><span class="line">	<span class="comment">#减1是因为输出的索引是相应输入的索引加1</span></span><br><span class="line">    num_examples=(<span class="built_in">len</span>(corpus_indices)-<span class="number">1</span>)//num_steps</span><br><span class="line">    <span class="comment">#迭代数=样本数/批量大小</span></span><br><span class="line">    epoch_size=num_examples//batch_size</span><br><span class="line">    example_indices=<span class="built_in">list</span>(<span class="built_in">range</span>(num_examples))</span><br><span class="line">    random.shuffle(example_indices)</span><br><span class="line">    <span class="comment">#返回从pos开始的长为num_steps的序列</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_data</span>(<span class="params">pos</span>):</span><br><span class="line">        <span class="keyword">return</span> corpus_indices[pos:pos+num_steps]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch_size):</span><br><span class="line">        i=i*batch_size</span><br><span class="line">        batch_indices=example_indices[i:i+batch_size]</span><br><span class="line">        X=[_data(j*num_steps) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        Y=[_data(j*num_steps+<span class="number">1</span>) <span class="keyword">for</span> j <span class="keyword">in</span> batch_indices]</span><br><span class="line">        <span class="keyword">yield</span> nd.array(X,ctx),nd.array(Y,ctx)</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#my_seq为人工序列，batch_size为批量大小，num_steps为时间步数</span></span><br><span class="line"><span class="attribute">my_seq</span>=list(range(30))</span><br><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> data_iter_random(my_seq,<span class="attribute">batch_size</span>=2,num_steps=6):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>,X,<span class="string">&#x27;\nY:&#x27;</span>,Y,<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

![image-20220414215323211](动手学深度学习总结/image-20220414215323211.png)

<figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line"><span class="comment">#相邻采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">data_iter_consecutive</span></span>(corpus_indices,batch_size,num_steps,ctx=<span class="title class_">None</span>):</span><br><span class="line">    corpus_indices=nd.array(corpus_indices,ctx=ctx)</span><br><span class="line">    data_len=len(corpus_indices)</span><br><span class="line">    batch_len=data_len//batch_size</span><br><span class="line">    indices=corpus_indices[<span class="number">0</span><span class="symbol">:batch_size*batch_len</span>].reshape((batch_size,batch_len))</span><br><span class="line">    epoch_size=(batch_len<span class="number">-1</span>)//num_steps</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(epoch_size):</span><br><span class="line">        i=i*num_steps</span><br><span class="line">        X=indices[<span class="symbol">:</span>,<span class="symbol">i:</span>i+num_steps]</span><br><span class="line">        Y=indices[<span class="symbol">:</span>,i+<span class="number">1</span><span class="symbol">:i+num_steps+</span><span class="number">1</span>]</span><br><span class="line">        yield X,Y</span><br></pre></td></tr></table></figure>

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> X,Y <span class="keyword">in</span> data_iter_consecutive(my_seq,<span class="attribute">batch_size</span>=2,num_steps=6):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;X: &#x27;</span>,X,<span class="string">&#x27;\nY:&#x27;</span>,Y,<span class="string">&#x27;\n&#x27;</span>)</span><br></pre></td></tr></table></figure>

![image-20220414215520491](动手学深度学习总结/image-20220414215520491.png)

## 循环神经网络的从零开始实现

<figure class="highlight elm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> d2lzh <span class="keyword">as</span> d2l</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="title">from</span> mxnet <span class="keyword">import</span> autograd,nd</span><br><span class="line"><span class="title">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">(corpus_indices,char_to_idx,idx_to_char,vocab_size)=d2l.load_data_jay_lyrics()</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="comment">#这是one-hot向量</span></span><br><span class="line"><span class="attribute">nd</span>.one_hot(nd.array([<span class="number">0</span>,<span class="number">2</span>]),vocab_size)</span><br></pre></td></tr></table></figure>

![image-20220414215644013](动手学深度学习总结/image-20220414215644013.png)

<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="comment">#one-hot向量定义</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">to_onehot</span>(<span class="params">X,size</span>):</span><br><span class="line">    <span class="keyword">return</span> [nd.one_hot(x,size) <span class="keyword">for</span> x <span class="keyword">in</span> X.T]</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">X</span>=nd.arange(<span class="number">10</span>).reshape((<span class="number">2</span>,<span class="number">5</span>))</span><br><span class="line"><span class="attribute">X</span></span><br></pre></td></tr></table></figure>

![image-20220414215810664](动手学深度学习总结/image-20220414215810664.png)

<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">inputs=<span class="keyword">to</span><span class="constructor">_onehot(X,<span class="params">vocab_size</span>)</span></span><br><span class="line">inputs</span><br></pre></td></tr></table></figure>

![image-20220414215828076](动手学深度学习总结/image-20220414215828076.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">len</span><span class="params">(inputs)</span></span>,inputs<span class="selector-attr">[0]</span>.shape</span><br></pre></td></tr></table></figure>

![image-20220414215844220](动手学深度学习总结/image-20220414215844220.png)

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">num_inputs,num_hiddens,<span class="attribute">num_outputs</span>=vocab_size,256,vocab_size</span><br><span class="line"><span class="attribute">ctx</span>=d2l.try_gpu()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;will use&#x27;</span>,ctx)</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化模型参数</span></span><br><span class="line">def get_params():</span><br><span class="line">    def _one(shape):</span><br><span class="line">        return nd.random.normal(<span class="attribute">scale</span>=0.01,shape=shape,ctx=ctx)</span><br><span class="line">    #隐藏层参数</span><br><span class="line">    <span class="attribute">W_xh</span>=_one((num_inputs,num_hiddens))</span><br><span class="line">    <span class="attribute">W_hh</span>=_one((num_hiddens,num_hiddens))</span><br><span class="line">    <span class="attribute">b_h</span>=nd.zeros(num_hiddens,ctx=ctx)</span><br><span class="line">    #输出层参数</span><br><span class="line">    <span class="attribute">W_hq</span>=_one((num_hiddens,num_outputs))</span><br><span class="line">    <span class="attribute">b_q</span>=nd.zeros(num_outputs,ctx=ctx)</span><br><span class="line">    #附上模型</span><br><span class="line">    params=[W_xh,W_hh,b_h,W_hq,b_q]</span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> params:</span><br><span class="line">        param.attach_grad()</span><br><span class="line">    return params</span><br></pre></td></tr></table></figure>

<figure class="highlight ruby"><table><tr><td class="code"><pre><span class="line"><span class="comment">#初始化rnn模型状态</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_rnn_state</span>(<span class="params">batch_size,num_hiddens,ctx</span>):</span><br><span class="line">    <span class="keyword">return</span> (nd.zeros(shape=(batch_size,num_hiddens),ctx=ctx),)</span><br></pre></td></tr></table></figure>

<figure class="highlight pf"><table><tr><td class="code"><pre><span class="line">def rnn(inputs,<span class="keyword">state</span>,params):</span><br><span class="line">    W_xh,W_hh,b_h,W_hq,b_q=params</span><br><span class="line">    H,=<span class="keyword">state</span></span><br><span class="line">    outputs=[]</span><br><span class="line">    <span class="keyword">for</span> X <span class="keyword">in</span> inputs:</span><br><span class="line">        H=nd.tanh(nd.dot(X,W_xh)+nd.dot(H,W_hh)+b_h)</span><br><span class="line">        Y=nd.dot(H,W_hq)+b_q</span><br><span class="line">        outputs.append(Y)</span><br><span class="line">    return outputs,(H,)</span><br><span class="line">    </span><br></pre></td></tr></table></figure>

![循环神经网络rnn公式](动手学深度学习总结/循环神经网络rnn公式.png)

<figure class="highlight stylus"><table><tr><td class="code"><pre><span class="line">state=<span class="built_in">init_rnn_state</span>(X<span class="selector-class">.shape</span><span class="selector-attr">[0]</span>,num_hiddens,ctx)</span><br><span class="line">inputs=<span class="built_in">to_onehot</span>(X<span class="selector-class">.as_in_context</span>(ctx),vocab_size)</span><br><span class="line">params=<span class="built_in">get_params</span>()</span><br><span class="line">outputs,state_new=<span class="built_in">rnn</span>(inputs,state,params)</span><br><span class="line"><span class="function"><span class="title">len</span><span class="params">(outputs)</span></span>,outputs<span class="selector-attr">[0]</span><span class="selector-class">.shape</span>,state_new<span class="selector-attr">[0]</span>.shape</span><br></pre></td></tr></table></figure>

![image-20220414220338235](动手学深度学习总结/image-20220414220338235.png)

<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">#本函数基于前缀prefix(含有数个字符的字符串)来预测接下来的num_chars个字符。</span><br><span class="line">def predict<span class="constructor">_rnn(<span class="params">prefix</span>,<span class="params">num_chars</span>,<span class="params">rnn</span>,<span class="params">params</span>,<span class="params">init_run_state</span>,<span class="params">num_hiddens</span>,<span class="params">vocab_size</span>,<span class="params">ctx</span>,<span class="params">idx_to_char</span>,<span class="params">char_to_idx</span>)</span>:</span><br><span class="line">    state=init<span class="constructor">_run_state(1,<span class="params">num_hiddens</span>,<span class="params">ctx</span>)</span></span><br><span class="line">    output=<span class="literal">[<span class="identifier">char_to_idx</span>[<span class="identifier">prefix</span>[<span class="number">0</span>]</span>]]</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(num_chars+len(prefix)-<span class="number">1</span>):</span><br><span class="line">    	#将上一时间步的输出当作当前时间步的输入，第一次输入为output<span class="literal">[-<span class="number">1</span>]</span>为output<span class="literal">[<span class="number">0</span>]</span>，第二次为上次         #时间步</span><br><span class="line">        X=<span class="keyword">to</span><span class="constructor">_onehot(<span class="params">nd</span>.<span class="params">array</span>([<span class="params">output</span>[-1]],<span class="params">ctx</span>=<span class="params">ctx</span>)</span>,vocab_size)</span><br><span class="line">        #计算输出和更新隐藏状态</span><br><span class="line">        (Y,state)=rnn(X,state,params)</span><br><span class="line">        #下一个时间步的输入是prefix里的字符或者当前的最佳预测字符 </span><br><span class="line">        <span class="keyword">if</span> t&lt;len(prefix)-<span class="number">1</span>:</span><br><span class="line">            output.append(char_to_idx<span class="literal">[<span class="identifier">prefix</span>[<span class="identifier">t</span>+<span class="number">1</span>]</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            output.append(<span class="built_in">int</span>(Y<span class="literal">[<span class="number">0</span>]</span>.argmax(axis=<span class="number">1</span>).asscalar<span class="literal">()</span>))#取概率为最大的那一个</span><br><span class="line">    return &#x27;&#x27;.join(<span class="literal">[<span class="identifier">idx_to_char</span>[<span class="identifier">i</span>]</span> <span class="keyword">for</span> i <span class="keyword">in</span> output])</span><br></pre></td></tr></table></figure>

<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">predict<span class="constructor">_rnn(&#x27;分开&#x27;,10,<span class="params">rnn</span>,<span class="params">params</span>,<span class="params">init_rnn_state</span>,<span class="params">num_hiddens</span>,<span class="params">vocab_size</span>,<span class="params">ctx</span>,<span class="params">idx_to_char</span>,<span class="params">char_to_idx</span>)</span></span><br></pre></td></tr></table></figure>

![image-20220414222335439](动手学深度学习总结/image-20220414222335439.png)

<figure class="highlight livecodeserver"><table><tr><td class="code"><pre><span class="line"><span class="comment">#裁剪梯度</span></span><br><span class="line">def grad_clipping(<span class="built_in">params</span>,theta,ctx):</span><br><span class="line">    norm=nd.array([<span class="number">0</span>],ctx)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">param</span> <span class="keyword">in</span> <span class="built_in">params</span>:</span><br><span class="line">        norm+=(<span class="built_in">param</span>.grad**<span class="number">2</span>).<span class="built_in">sum</span>()</span><br><span class="line">    norm=norm.<span class="built_in">sqrt</span>().asscalar()</span><br><span class="line">    <span class="keyword">if</span> norm&gt;theta:</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">param</span> <span class="keyword">in</span> <span class="built_in">params</span>:</span><br><span class="line">            <span class="built_in">param</span>.grad[:]*=theta/norm</span><br></pre></td></tr></table></figure>

![image-20220414222559582](动手学深度学习总结/image-20220414222559582.png)

<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line"><span class="comment">#定义模型训练函数</span></span><br><span class="line">def train_and_predict_rnn(rnn,get_params,init_rnn_state,num_hiddens,vocab_size,ctx,corpus_indices,idx_to_char,char_to_idx,</span><br><span class="line">                          is_random_iter,num_epochs,num_steps,lr,clipping_theta,batch_size,pred_period,pred_len,prefixes):</span><br><span class="line">    <span class="keyword">if</span> is_random_iter:</span><br><span class="line">        <span class="attribute">data_iter_fn</span>=d2l.data_iter_random</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="attribute">data_iter_fn</span>=d2l.data_iter_consecutive</span><br><span class="line">    <span class="attribute">params</span>=get_params()</span><br><span class="line">    <span class="attribute">loss</span>=gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">   # <span class="built_in">print</span>(num_epochs)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">       # <span class="built_in">print</span>(epoch)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> is_random_iter:</span><br><span class="line">            <span class="attribute">state</span>=init_rnn_state(batch_size,num_hiddens,ctx)</span><br><span class="line">        l_sum,n,<span class="attribute">start</span>=0.0,0,time.time()</span><br><span class="line">        <span class="attribute">data_iter</span>=data_iter_fn(corpus_indices,batch_size,num_steps,ctx)</span><br><span class="line">        <span class="keyword">for</span> X,Y <span class="keyword">in</span> data_iter:</span><br><span class="line">            <span class="keyword">if</span> is_random_iter:</span><br><span class="line">                <span class="attribute">state</span>=init_rnn_state(batch_size,num_hiddens,ctx)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> s <span class="keyword">in</span> state:</span><br><span class="line">                    s.detach()</span><br><span class="line">            with autograd.record():</span><br><span class="line">                <span class="attribute">inputs</span>=to_onehot(X,vocab_size)</span><br><span class="line">                (outputs,state)=rnn(inputs,state,params)</span><br><span class="line">                <span class="attribute">outputs</span>=nd.concat(*outputs,dim=0)</span><br><span class="line">                <span class="attribute">y</span>=Y.T.reshape((-1,))</span><br><span class="line">                <span class="attribute">l</span>=loss(outputs,y).mean()</span><br><span class="line">            l.backward()</span><br><span class="line">            grad_clipping(params,clipping_theta,ctx)</span><br><span class="line">            d2l.sgd(params,lr,1)</span><br><span class="line">            l_sum+=l.asscalar()*y.size</span><br><span class="line">            n+=y.size</span><br><span class="line">            <span class="keyword">if</span> (epoch+1)%<span class="attribute">pred_period</span>==0:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&#x27;epoch %d,perplexity %f,time %.2f sec&#x27;</span>%(epoch+1,math.exp(l_sum/n),time.time()-start))</span><br><span class="line">                <span class="keyword">for</span><span class="built_in"> prefix </span><span class="keyword">in</span> prefixes:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&#x27;-&#x27;</span>,predict_rnn(prefix,pred_len,rnn,params,init_rnn_state,num_hiddens,vocab_size,ctx,</span><br><span class="line">                          idx_to_char,char_to_idx))</span><br></pre></td></tr></table></figure>

<figure class="highlight apache"><table><tr><td class="code"><pre><span class="line"><span class="attribute">num_epochs</span>,num_steps,batch_size,lr,clipping_theta=<span class="number">250</span>,<span class="number">35</span>,<span class="number">32</span>,<span class="number">1</span>e2,<span class="number">1</span>e-<span class="number">2</span></span><br><span class="line"><span class="attribute">pred_period</span>,pred_len,prefixes=<span class="number">50</span>,<span class="number">50</span>,[&#x27;分开&#x27;,&#x27;不分开&#x27;]</span><br><span class="line"></span><br><span class="line"><span class="attribute">train_and_predict_rnn</span>(rnn,get_params,init_rnn_state,num_hiddens,vocab_size,ctx,corpus_indices,idx_to_char,</span><br><span class="line">                       <span class="attribute">char_to_idx</span>,True,num_epochs,num_steps,lr,clipping_theta,batch_size,pred_period,pred_len,</span><br><span class="line">                         <span class="attribute">prefixes</span>)</span><br></pre></td></tr></table></figure>

![image-20220414222728131](动手学深度学习总结/image-20220414222728131.png)

## 门控循环单元(GRU)

门控循环单元结构 

![image-20220415130545022](动手学深度学习总结/image-20220415130545022.png)

候选隐藏单元

![image-20220415130715939](动手学深度学习总结/image-20220415130715939.png)

可以看出这个地方是由重置门来控制$$H_{t-1}$$，当重置门为0时，表示上一时间步的隐藏状态$$H_{t-1}$$不加入该时间步中，即上一时间步不影响这一时间步隐藏单元$$\tilde{H_t}$$的生成，当重置门为1时，表示上一时间步的隐藏状态$$H_{t-1}$$会影响该时间步隐藏单元$$\tilde{H_t}$$的的生成。

![image-20220415131115611](动手学深度学习总结/image-20220415131115611.png)

可以看到我们有t时刻的隐藏状态公式

![image-20220415131637947](动手学深度学习总结/image-20220415131637947.png)

这表示若更新门当从t-1到t时之间近似为1，表示在时间步t-1到t之间的输入信息几乎没有流入时间步t的隐藏状态$$H_t$$，这表示我从所状态$$H_{t-1}$$隐藏状态，经过t时间步的单元时没有怎么改变，并且可以以$$H_{t}$$的状态输入到时刻t+1处，这可以保证时间序列之间的长依赖。

**重置门有助力捕捉时间序列里短期的依赖关系，从t-1到t时刻**

**更新门有助于捕捉时间序列里长期的依赖关系，可以通过更新门把状态从更加长时间的依赖**

![GRUpng](动手学深度学习总结/GRUpng.png)</script><p>(Y^{<em>})^{<t>}=H_t</t></em>W_{hq}+b</p>
<p>$$</p>
<h2 id="长短期记忆"><a href="#长短期记忆" class="headerlink" title="长短期记忆"></a>长短期记忆</h2><p><img src="/posts/b96b4738/image-20220415182710558.png" alt="image-20220415182710558"></p>
<p><img src="/posts/b96b4738/image-20220415182738771.png" alt="image-20220415182738771"></p>
<p><img src="/posts/b96b4738/image-20220415183135068.png" alt="image-20220415183135068"></p>
<p><img src="/posts/b96b4738/image-20220415183156489.png" alt="image-20220415183156489"></p>
<p></p>]]></content>
      <categories>
        <category>计算机</category>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
        <tag>MXNet</tag>
        <tag>动手学深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>高性能mysql</title>
    <url>/posts/5b906d18/</url>
    <content><![CDATA[<p> 本文为高性能mysql知识点整理。</p>
<span id="more"></span>
<h1 id="mysql架构与历史"><a href="#mysql架构与历史" class="headerlink" title="mysql架构与历史"></a>mysql架构与历史</h1><h2 id="mysql逻辑架构"><a href="#mysql逻辑架构" class="headerlink" title="mysql逻辑架构"></a>mysql逻辑架构</h2><p>mysql逻辑架构，如下图所示。</p>
<p><img src="/posts/5b906d18/image-20220330165956330.png" alt="image-20220330165956330"></p>
<p>最上层为连接处理、授权认证、安全等。</p>
<p>第二层架构包括查询解析、分析、优化、缓存以及所有内置函数、存储过程、视图、触发器等。</p>
<p>第三层为存储引擎，存储引擎负责MYSQL中数据的存储与提取。</p>
<h3 id="连接管理与安全性"><a href="#连接管理与安全性" class="headerlink" title="连接管理与安全性"></a>连接管理与安全性</h3><p>每个客户端连接都在服务器进程中拥有一个线程，这个连接的查询中会在这个单独的线程中执行，该线程只能轮流在某个CPU核心或者CPU中运行，服务器会缓存线程，因此不需要为每一个新建的连接创建或销毁线程，</p>
<p><strong>注：</strong>与用户级线程类似</p>
<p>当客户端连接到MySQL服务器时，服务器需要对其进行认证，认证基于用户名密码等。</p>
<h3 id="优化与执行"><a href="#优化与执行" class="headerlink" title="优化与执行"></a>优化与执行</h3><p>mysql会解析查询，并创建内部数据结构(解析树)，然后对其进行各种优化，包括重写查询、决定表的读取顺序，以及选择合适的索引等。</p>
<h2 id="并发控制"><a href="#并发控制" class="headerlink" title="并发控制"></a>并发控制</h2><h3 id="读写锁"><a href="#读写锁" class="headerlink" title="读写锁"></a>读写锁</h3><p>当我们出现一个线程要删除A表，另一个线程要读取A表，如若不进行并发控制，会出现结果不确定性。</p>
<p>为了解决这种问题，我们有<strong>共享锁</strong>与<strong>排它锁</strong>，也叫<strong>读锁</strong>与<strong>写锁</strong>。</p>
<ol>
<li>读锁：读锁是共享的，即相互不阻塞。多个客户在同一时刻可以同时读取同一资源，而不互相干扰。</li>
<li>写锁：写锁是排他的，即一个写锁会阻塞其它的写锁与读锁。</li>
</ol>
<h3 id="锁粒度"><a href="#锁粒度" class="headerlink" title="锁粒度"></a>锁粒度</h3><p>锁粒度即锁住数据的颗粒度，如锁表，锁行等。锁定数据的级别不一样。</p>
<p><strong>表锁</strong>：会锁定整张表，一个用户在表进行写操作(插入、删除、修改等)前，需要先获得写锁，这会阻塞其他用户对该表的所有读写操作。只有在没有写锁时，其他读取的用户才获得读锁，读锁之间是不相互阻塞的。</p>
<p><strong>行级锁</strong>：可以最大程度地支持并发处理(同时也带来锁开销)，其特性与表锁一致。</p>
<p><strong>注</strong>：实现表锁与行级锁的必要条件是操作是原子级别的，即不可分割。</p>
<h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">start transaction;</span><br><span class="line">select balance <span class="keyword">from</span> checking where <span class="attribute">customer_id</span>=10233276;</span><br><span class="line">update checking <span class="built_in">set</span> <span class="attribute">balance</span>=balance-200 where <span class="attribute">customer_id</span>=10233276;</span><br><span class="line">update savings <span class="built_in">set</span> <span class="attribute">balance</span>=balance+200 where <span class="attribute">customer_id</span>=10233276;</span><br><span class="line">commit;</span><br></pre></td></tr></table></figure>
<p>若语句在执行到第四个语句崩溃了，用户可能会损失200元。</p>
<p>所以我们需要系统通过ACID测试，ACID具有</p>
<ol>
<li>原子性</li>
<li>一致性</li>
<li>隔离性</li>
<li>持久性</li>
</ol>
<p><strong>原子性</strong>：一个事务必须被视为一个不可分割的最小单元，整个事务要么全部提交成功，要么全部失败回滚，对于一事务来说，不可能只执行其中的一部分操作，就是事务的原子性。</p>
<p><strong>一致性</strong>：数据库总是从一个一致性的状态转换到另外一个一致性的状态。在前面的例子中，一致性确保了，即使在执行第三、第四条语句之间时系统崩溃，支票账户也不会损失200美元，因为事务最终没有提交，所有事务中所做的修改也不会保存到数据库中。</p>
<p><strong>隔离性</strong>：一个事务所做的修改在最终提交以前，对其他事务不可见的。在前面的例子中，当执行完第三条语句、第四要语句还未开始时，此时有另一个账户汇总程序开始运行，则其看到的支票账户的余额并没有被减去200美元。</p>
<p><strong>持久性</strong>：一旦事务提交，则其所做的修改应付永久保存到数据库中。此时即使系统崩溃，修改的数据也不会丢失。</p>
<p>我们将采用一个由几个账户和一个访问和更新账户的事务集合构成的简单的银行应用来阐明事务的概念。事务运用以下两个操作访问数据。</p>
<p>read(X)：从数据库把数据项X传送到执行read操作的事务的主存缓冲区的一个也称为X的变量中。</p>
<p>write(X)：从执行write的事务的主存缓冲区的变量X中把数据项X传回数据库中。write操作不一定立即更新磁盘上的数据；write操作的结果可以临时存储在某处，以后再写到磁盘上。目前假设write操作立即更新数据库。</p>
<p>设$T_i$是从账户A过户$50到账户B的事务。这个事务的代码有：</p>
<figure class="highlight scss"><table><tr><td class="code"><pre><span class="line"><span class="built_in">read</span>(X);</span><br><span class="line"><span class="selector-tag">A</span>:=A-<span class="number">50</span>;</span><br><span class="line"><span class="built_in">write</span>(A);</span><br><span class="line"><span class="built_in">read</span>(B);</span><br><span class="line"><span class="selector-tag">B</span>:=B+<span class="number">50</span>;</span><br><span class="line"><span class="built_in">write</span>(B);</span><br></pre></td></tr></table></figure>
<p>在这个例子中</p>
<p>一致性：一致性要求事务的执行不改变A、B之和。如果没有一致性要求，金额可以能被事务凭空创造或销毁，如果数据库在事务执行前是一致的，那么事务执行数据库仍将保持一致。</p>
<p>总而言之，一致性即在事务执行前与执行后数据的一致与逻辑的自洽。</p>
<h3 id="隔离级别"><a href="#隔离级别" class="headerlink" title="隔离级别"></a>隔离级别</h3><p>SQL标准定义了四个隔离级别，每一种级别都规定了一个事务中所做的修改，哪些在事务内和事务间是可见的，哪些是不可见的。</p>
<p>四种隔离级别。</p>
<p><strong>READ UNCOMMITTED(未提交读)</strong></p>
<p>​    在READ UNCOMMITTED级别，事务中的修改，即使没有提交，对其它事务也都是可见的。事务可以读取未提交的数据，这称之为<strong>脏读</strong>。</p>
<p><strong>READ COMMITTED(提交读)</strong></p>
<p>​    READ COMMITTED满足前面的隔离性的简单定义：一个事务开始时，只能”看见”已经提交的事务所做的修改。即一个事务从开始直到提交之前，所做的任何修改对其他事务都是不可见的。这个级别也叫做<strong>不可重复读</strong>，因为两次执行同样的查询，可能会得到不一样的结果。</p>
<p><strong>REPEATABLE READ(可重复读)</strong></p>
<p>​    REPEATABLE READ解决了脏读的问题。该级别保证了在同一事务中多次读取同样记录的结果是一致的。但是可重复读隔离级别无法解决另一个<strong>幻读</strong>问题。幻读是指当某个事务在读取某个范围内的记录时，另一个事务又在该范围内插入了新的记录，当之前的事务再次读取该范围的记录时，会产生<strong>幻行</strong>。</p>
<p><strong>可重复读是MYSQL的默认事务隔离级别</strong>。</p>
<p><strong>SERIALIZABLE(可串行化)</strong></p>
<p>​    SERIALIZABLE是最高的隔离级别，通过强制事务串行执行，避免了前面的幻读问题。即SERIALIZABLE会在读取的每一行数据上都加锁，所以可能会导致大量的超时与锁争用问题。实际应用中很少用到这个隔离级别，只有在非常需要确保数据的一致性而且可以接受没有并发的情况下，才考虑采用该级别。</p>
<center>ANSI SQL隔离级别</center>

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">隔离级别</th>
<th>脏读可能性</th>
<th>不可重复读可能性</th>
<th>幻读可能性</th>
<th>加锁读</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">READ UNCOMMITTED</td>
<td>yes</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td style="text-align:left">READ COMMITTED</td>
<td>no</td>
<td>yes</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td style="text-align:left">REPEATABLE READ</td>
<td>no</td>
<td>no</td>
<td>yes</td>
<td>no</td>
</tr>
<tr>
<td style="text-align:left">SERIALIZABLE</td>
<td>no</td>
<td>no</td>
<td>no</td>
<td>yes</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>计算机</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>高性能mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>数据库三大范式</title>
    <url>/posts/82be6141/</url>
    <content><![CDATA[<p>本文为数据库三大范式的笔记整理。</p>
<span id="more"></span>
<h1 id="数据库-基本概念整理-三大范式"><a href="#数据库-基本概念整理-三大范式" class="headerlink" title="数据库-基本概念整理-三大范式"></a>数据库-基本概念整理-三大范式</h1><h2 id="候选码"><a href="#候选码" class="headerlink" title="候选码"></a>候选码</h2><p>候选码：若关系中的某一属性的值能唯一地标识一个元组，而其子集不能，则称该属性组为候选码，若一个关系中有多个候选码，则选定其中一个为主码。</p>
<h2 id="主属性"><a href="#主属性" class="headerlink" title="主属性"></a>主属性</h2><p>主属性：一个属性只要在任何一个候选码中出现过，这个属性就是主属性。</p>
<h2 id="第一范式"><a href="#第一范式" class="headerlink" title="第一范式"></a>第一范式</h2><p>定义：表示属于第一范式的所有属性不可再分，即数据项不可再分。</p>
<h2 id="第二范式"><a href="#第二范式" class="headerlink" title="第二范式"></a>第二范式</h2><p>定义：若某关系R属于第一范式，且每一个非主属性完全函数完全依赖于任何一个候选码，则关系R属于第二范式。</p>
<h3 id="部分函数依赖"><a href="#部分函数依赖" class="headerlink" title="部分函数依赖"></a>部分函数依赖</h3><p>部分函数依赖是指多个属性决定另一个属性，但事实上，这多个属性是有冗余的。例如，(学号，班级)-&gt;姓名，事实上只需要学号就能决定姓名，因此班级是冗余的。</p>
<h3 id="完全函数依赖"><a href="#完全函数依赖" class="headerlink" title="完全函数依赖"></a>完全函数依赖</h3><p>完全函数依赖是消除了部分函数依赖，关系中的非主属性完全能主属性或者组合属性推导而出。如有选课关系表(学号，课程号，成绩，学分)，其中关键字为组合关键字(学号，课程号)，由于非主属性学分仅依赖于课程号，对关系字(学号，课程号)只是部分依赖，而不是完全依赖，因些这种方式会导致数据冗余以及更新异常等问题，解决办法是将其分为两个关键模式：学生表(学号，课程号，分数)和课程表(课程号，学分)，新关系通过学生表中的外关键字课程号联系，在需要时进行连接。</p>
<p>我们有选课关系表(学号，课程号，成绩，学分)，我们对其进行学分修改时，我们需要找到所有选过这个课程号的学生，并对其进行学分的修改，这样会导致数据冗余，而若我们拆分学生表与课程表就不会发生这种情况，而若我们需要删除该课程时，也会导致此类情况。</p>
<h3 id="判断"><a href="#判断" class="headerlink" title="判断"></a>判断</h3><p>判断一个关系是否属于第二范式：</p>
<p>1、找出数据表中的所有码；</p>
<p>2、找出所有主属性和非主属性；</p>
<p>3、判断所有非主属性对码的部分函数依赖；</p>
<h2 id="第三范式"><a href="#第三范式" class="headerlink" title="第三范式"></a>第三范式</h2><p>定义：第三范式是在第二范式的基础上消除了传递依赖，即非主属性既不传递依赖于码，也不部分依赖于码。</p>
<p>若我们有表学生上课表 (姓名，课程，老师，老师职称，教室，上课时间)，这个表上具有传递依赖。即</p>
<p>(姓名，课程)-&gt;老师-&gt;老师职称</p>
<p>该表具有如下问题</p>
<p>1、若老师改变了职称，则需要修改数据库，若有N条记录，都需要修改N条(修改异常)</p>
<p>2、若没人选这个老师的课，那么老师的职称也没了记录(删除异常)</p>
<p>3、若新来一个老师，暂时还未分配教授课程，那么没有表记录他的职称(插入异常)</p>
<p>我们需要对于学生上课表进行分解，有</p>
<p>(姓名，课程，老师，教室，上课时间)</p>
<p>(老师，老师职称)</p>
<h2 id="BC范式-BCNF"><a href="#BC范式-BCNF" class="headerlink" title="BC范式(BCNF)"></a>BC范式(BCNF)</h2><p>BC范式符合3NF，并且主属性不依赖于主属性。</p>
<p>BC范式既检查非主属性，又检查主属性。当只检查非主属性时，就成了第三范式。满足BC范式的关系都必然满足第三范式。还可以这么说：若一个关系达到了第三范式，并且它只有一个候选码，或者它的每个候选码都是单属性，则该关系自然达到BC范式。</p>
<p>假设仓库管理关系表(仓库ID，存储物品ID，管理员ID，数量)，且只有一个管理员只在一个仓库工作，一个仓库可以存储多种物品。</p>
<p>这个数据库表存在如下决定关系：</p>
<p>(仓库ID，存储物品ID)-&gt;(管理员ID，数量)</p>
<p>(管理员ID，存储物品ID)-&gt;(仓库ID，数量)</p>
<p>所以，(仓库ID, 存储物品ID)和(管理员ID, 存储物品ID)都是StorehouseManage的候选关键字，表中的唯一非关键字段为数量，它是符合第三范式的。但是，由于存在如下决定关系：</p>
<p>(仓库ID) → (管理员ID)</p>
<p>(管理员ID) → (仓库ID)</p>
<p>这种关键字决定关键字段的情况，其不符合BCNF范式。</p>
<p>以上关系表会存在如下问题</p>
<p>1、删除异常</p>
<p>当仓库被清空后，所有”存储物品ID”和”数量”信息被删除的同时，”仓库ID“与”管理ID“也被删除了。</p>
<p>2、插入异常</p>
<p>当仓库没有存储任何物品时，无法给仓库分配管理员。</p>
<p>3、更新异常</p>
<p>如果仓库更换了管理员，则表中所有行的管理员ID都要修改。</p>
<p>所以需要把上述关系分解为</p>
<p>仓库管理：(仓库ID，管理员ID)</p>
<p>仓库：(仓库ID，存储物品ID，数量)</p>
<p>这样的数据库表符合BCNF范式，消除了删除异常，插入异常与更新异常</p>
<p>参考链接：<a href="https://www.cnblogs.com/lca1826/p/6601395.html">https://www.cnblogs.com/lca1826/p/6601395.html</a></p>
]]></content>
      <categories>
        <category>计算机</category>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库基础知识</tag>
      </tags>
  </entry>
</search>
